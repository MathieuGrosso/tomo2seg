{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JEHjvuBBIab"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1602255916978,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "KTMgQv07JkgY",
    "outputId": "69cd78fc-f0f1-46f6-f1d8-63b99d55eaae"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "import functools\n",
    "import operator\n",
    "import logging\n",
    "from typing import *\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import humanize\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymicro.file import file_utils\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks as keras_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tomo2seg import datasets as t2s_datasets\n",
    "from tomo2seg import slack\n",
    "from tomo2seg import modular_unet\n",
    "from tomo2seg.logger import logger, dict2str, add_file_handler\n",
    "from tomo2seg import viz\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.volume_sequence import (\n",
    "    MetaCrop3DGenerator, VolumeCropSequence,\n",
    "    SequentialGridPosition\n",
    ")\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import callbacks as tomo2seg_callbacks\n",
    "from tomo2seg import losses as tomo2seg_losses\n",
    "from tomo2seg import schedule as tomo2seg_schedule\n",
    "from tomo2seg import utils as tomo2seg_utils\n",
    "from tomo2seg import slackme\n",
    "from tomo2seg.args import TrainArgs as Args\n",
    "from tomo2seg import hosts as t2s_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.warning(Args.versions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from argv\n",
    "\n",
    "partition_train = \"lc_train_10000\"\n",
    "model_fold = 0\n",
    "\n",
    "logger.warning(f\"{partition_train=}\")\n",
    "logger.warning(f\"{model_fold=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "\n",
    "args = Args(\n",
    "    script_name = \"train-08-lc-akela.ipynb\",\n",
    "    \n",
    "    host = t2s_hosts.get_hosts()[socket.gethostname()],\n",
    "    \n",
    "    early_stop_mode = Args.EarlyStopMode.no_early_stop,\n",
    "    batch_size_mode = Args.BatchSizeMode.try_max_and_reduce,\n",
    "    train_mode = Args.TrainMode.from_scratch,\n",
    "\n",
    "    volume_name = t2s_datasets.VOLUME_COMPOSITE_V1[0],\n",
    "    volume_version = t2s_datasets.VOLUME_COMPOSITE_V1[1],\n",
    "    labels_version = t2s_datasets.VOLUME_COMPOSITE_V1_LABELS_REFINED3,\n",
    "    \n",
    "    partition_train = partition_train,\n",
    "    partition_val = \"lc_val\",\n",
    "    model_fold = model_fold,\n",
    "    \n",
    "    batch_size_per_gpu = 5,\n",
    "\n",
    "    # None == auto\n",
    "    random_state_seed = model_fold,\n",
    "    runid = None,\n",
    ")\n",
    "\n",
    "logger.info(f\"args={dict2str(asdict(args))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "except NameError:\n",
    "    print(\"already deleted (:\")\n",
    "else:\n",
    "    del tomo2seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_proportion = partition_train.split(\"_\")[-1]\n",
    "\n",
    "logger.debug(f\"{lc_proportion=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "\n",
    "crop_shape = (384, 384, 1)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "model_nclasses = 3\n",
    "\n",
    "model_master_name = \"unet2d-vanilla03\"\n",
    "model_version = f\"lc-{lc_proportion}\"\n",
    "\n",
    "model_is_2halfd = False\n",
    "model_is_2d = True\n",
    "\n",
    "model_factory_function = modular_unet.u_net\n",
    "model_factory_kwargs = {\n",
    "    **modular_unet.kwargs_vanilla03,\n",
    "    **dict(\n",
    "        convlayer=modular_unet.ConvLayer.conv2d,\n",
    "        \n",
    "        input_shape=crop_shape,\n",
    "        output_channels=model_nclasses,\n",
    "        nb_filters_0 = 16,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "try:\n",
    "    tomo2seg_model\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    logger.info(\"Creating a Tomo2SegModel.\")\n",
    "\n",
    "    tomo2seg_model = Tomo2SegModel(\n",
    "        model_master_name,\n",
    "        model_version,\n",
    "        runid=args.runid,\n",
    "        factory_function=model_factory_function,\n",
    "        factory_kwargs=model_factory_kwargs,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    logger.warning(\n",
    "        \"The model is already defined. To create a new one: `del tomo2seg_model`\")\n",
    "\n",
    "finally:\n",
    "    logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")\n",
    "    logger.info(f\"{tomo2seg_model.name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "random_state = np.random.RandomState(args.random_state_seed)\n",
    "\n",
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "tf_version = tf.__version__\n",
    "logger.info(f\"{tf_version=}\")\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "logger.info(\n",
    "    f\"Hostname: {hostname}\\nNum GPUs Available: {n_gpus}\\nThis should be:\\n\\t\"\n",
    "    + '\\n\\t'.join(['2 on R790-TOMO', '1 on akela', '1 on hathi', '1 on krilin'])\n",
    ")\n",
    "\n",
    "logger.debug(\n",
    "    \"physical GPU devices:\\n\\t\"\n",
    "    + \"\\n\\t\".join(map(str, tf.config.list_physical_devices('GPU'))) + \"\\n\"\n",
    "    + \"logical GPU devices:\\n\\t\"\n",
    "    + \"\\n\\t\".join(map(str, tf.config.list_logical_devices('GPU')))\n",
    ")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "gpu_strategy = tf.distribute.MirroredStrategy()\n",
    "logger.debug(f\"{gpu_strategy=}\")\n",
    "\n",
    "logger.info(f\"{dict2str(asdict(args.host))}\")\n",
    "\n",
    "MAX_INTERNAL_NVOXELS = int(\n",
    "    args.host.gpu_max_memory_factor * t2s_hosts.MAX_INTERNAL_NVOXELS\n",
    ")\n",
    "\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Creating the Keras model.\")\n",
    "\n",
    "if args.train_mode.is_continuation:\n",
    "    logger.warning(\"Training continuation: a model will be loaded.\")\n",
    "\n",
    "    if args.train_mode == Args.TrainMode.continuation_from_latest_model:\n",
    "        logger.info(\"Using the LATEST model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved_model:\n",
    "        logger.info(\"Using the AUTOSAVED model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved_model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved2_best_model:\n",
    "        logger.info(\"Using the (best) AUTOSAVED2 model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved2_best_model_path\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"{args.train_mode=}\")\n",
    "\n",
    "elif (\n",
    "    tomo2seg_model.model_path.exists()\n",
    "    or tomo2seg_model.autosaved_model_path.exists()\n",
    "    # todo uncomment me when implemented\n",
    "    #             or tomo2seg_model.autosaved2_best_model_path.exists()\n",
    "):\n",
    "    logger.error(\n",
    "        f\"The model seems to already exist but this is not a continuation. Please, make sure the arguments are correct.\")\n",
    "    raise ValueError(\n",
    "        f\"{args.train_mode=} ==> {args.train_mode.is_continuation=} {tomo2seg_model.name=}\")\n",
    "\n",
    "elif args.train_mode == Args.TrainMode.from_scratch:\n",
    "    logger.info(f\"A new model will be instantiated!\")\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.train_mode=}\")\n",
    "\n",
    "\n",
    "with gpu_strategy.scope():\n",
    "\n",
    "    if args.train_mode.is_continuation:\n",
    "\n",
    "        assert load_model_path.exists(\n",
    "        ), f\"Inconsistent arguments {args.train_mode.is_continuation=} {load_model_path=} {load_model_path.exists()=}.\"\n",
    "\n",
    "        logger.info(f\"Loading model {load_model_path.name}\")\n",
    "\n",
    "        model = keras.models.load_model(str(load_model_path), compile=False)\n",
    "\n",
    "        assert model.name == tomo2seg_model.name, f\"{model.name=} {tomo2seg_model.name=}\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        logger.info(\n",
    "            f\"Instantiating a new model with model_factory_function={model_factory_function.__name__}.\")\n",
    "\n",
    "        model = model_factory_function(\n",
    "            name=tomo2seg_model.name,\n",
    "            **model_factory_kwargs\n",
    "        )\n",
    "\n",
    "    logger.info(\"Compiling the model.\")\n",
    "\n",
    "    # [manual-input]\n",
    "    # using the avg jaccard is dangerous if one of the classes is too\n",
    "    # underrepresented because it's jaccard will be unstable\n",
    "    # to be verified!\n",
    "    loss = tomo2seg_losses.jaccard2_flat\n",
    "    optimizer = optimizers.Adam(lr=.003)\n",
    "    metrics = []\n",
    "\n",
    "    logger.debug(f\"{loss=}\")\n",
    "    logger.debug(f\"{optimizer=}\")\n",
    "    logger.debug(f\"{metrics=}\")\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not args.train_mode.is_continuation:\n",
    "\n",
    "    logger.info(f\"Saving the model at {tomo2seg_model.model_path=}.\")\\\n",
    "\n",
    "    model.save(tomo2seg_model.model_path)\n",
    "\n",
    "    logger.info(f\"Writing the model summary at {tomo2seg_model.summary_path=}.\")\n",
    "\n",
    "    with tomo2seg_model.summary_path.open(\"w\") as f:\n",
    "        def print_to_txt(line):\n",
    "            f.writelines([line + \"\\n\"])\n",
    "        model.summary(print_fn=print_to_txt, line_length=140)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Printing an image of the architecture at {tomo2seg_model.architecture_plot_path=}.\")\n",
    "\n",
    "    utils.plot_model(model, show_shapes=True,\n",
    "                     to_file=tomo2seg_model.architecture_plot_path)\n",
    "\n",
    "# it is here because before this cell the folder doesnt exist yet\n",
    "add_file_handler(logger, tomo2seg_model.train_log_path)\n",
    "    \n",
    "# repeat it so that the log file saves this\n",
    "logger.info(f\"args\\n{dict2str(asdict(args))}\")\n",
    "logger.info(f\"{tomo2seg_model.name=}\")\n",
    "logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8e5FhmUaKND"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1602255917946,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "4CfP7usu2VKr",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Metadata/paths objects\n",
    "\n",
    "# Volume\n",
    "volume = Volume.with_check(\n",
    "    name=args.volume_name, version=args.volume_version\n",
    ")\n",
    "\n",
    "logger.info(f\"volume\\n{dict2str(asdict(volume))}\")\n",
    "\n",
    "assert volume.nclasses == model_nclasses, f\"{model_nclasses=} {volume.nclasses=}\"\n",
    "\n",
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "# Data\n",
    "voldata = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=False,\n",
    "\n",
    ") / volume.normalization_factor\n",
    "\n",
    "logger.debug(f\"{voldata.shape=}\")\n",
    "\n",
    "voldata_train = volume[args.partition_train].get_volume_partition(voldata)\n",
    "voldata_val = volume[args.partition_val].get_volume_partition(voldata)\n",
    "\n",
    "logger.debug(f\"{voldata_train.shape=}\")\n",
    "logger.debug(f\"{voldata_val.shape=}\")\n",
    "\n",
    "del voldata\n",
    "\n",
    "# Labels\n",
    "\n",
    "vollabels = file_utils.HST_read(\n",
    "    str(volume.versioned_labels_path(args.labels_version)),\n",
    "\n",
    "    autoparse_filename=False,\n",
    "    data_type=\"uint8\",\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vollabels.shape=}\")\n",
    "\n",
    "vollabels_train = volume[args.partition_train].get_volume_partition(vollabels)\n",
    "vollabels_val = volume[args.partition_val].get_volume_partition(vollabels)\n",
    "\n",
    "logger.debug(f\"{vollabels_train.shape=}\")\n",
    "logger.debug(f\"{vollabels_val.shape=}\")\n",
    "\n",
    "del vollabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnsQ7lX0bVRh"
   },
   "source": [
    "# Data crop sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_internal_nvoxel_factor = tomo2seg_utils.get_model_internal_nvoxel_factor(model)\n",
    "\n",
    "logger.debug(f\"{model_internal_nvoxel_factor=}\")\n",
    "\n",
    "max_batch_nvoxels = int(np.floor(MAX_INTERNAL_NVOXELS / model_internal_nvoxel_factor))\n",
    "\n",
    "logger.debug(f\"{max_batch_nvoxels=} ({humanize.intcomma(max_batch_nvoxels)})\")\n",
    "\n",
    "crop_nvoxels = functools.reduce(operator.mul, crop_shape)\n",
    "\n",
    "logger.debug(f\"{crop_shape=} ==> {crop_nvoxels=}\")\n",
    "\n",
    "max_batch_size_per_gpu = batch_size_per_gpu = max(\n",
    "    1, int(np.floor(max_batch_nvoxels / crop_nvoxels)))\n",
    "\n",
    "logger.info(f\"{batch_size_per_gpu=}\")\n",
    "\n",
    "if args.batch_size_per_gpu is not None:\n",
    "    logger.warning(\n",
    "        f\"{args.batch_size_per_gpu=} given ==> replacing {batch_size_per_gpu=}\")\n",
    "    batch_size_per_gpu = args.batch_size_per_gpu\n",
    "\n",
    "logger.info(f\"{n_gpus=}\")\n",
    "\n",
    "batch_size = batch_size_per_gpu * max(1, n_gpus)\n",
    "\n",
    "logger.info(f\"{batch_size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "gtype = volume_sequence.GT2D if model_is_2d or model_is_2halfd else volume_sequence.GT3D\n",
    "\n",
    "metacrop_gen_common_kwargs = dict(\n",
    "    crop_shape=crop_shape,\n",
    "    common_random_state_seed=args.random_state_seed,\n",
    "    is_2halfd=model_is_2halfd,\n",
    "    gt_type=gtype,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{metacrop_gen_common_kwargs=}\")\n",
    "\n",
    "vol_crop_seq_common_kwargs = dict(\n",
    "    \n",
    "    output_as_2d=model_is_2d,\n",
    "    output_as_2halfd=model_is_2halfd,\n",
    "    labels=volume.metadata.labels,\n",
    "\n",
    "    # [manual-input]\n",
    "    debug__no_data_check=True,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vol_crop_seq_common_kwargs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = voldata_train\n",
    "labels = vollabels_train\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "crop_seq_train = VolumeCropSequence(\n",
    "    \n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "\n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_train01(\n",
    "        \n",
    "        volume_shape=volume_shape,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "        \n",
    "        data_original_dtype=volume.metadata.dtype,\n",
    "\n",
    "        # [manual-input]\n",
    "#         gt_no_transpose_rot=False,\n",
    "    ),\n",
    "\n",
    "    # this volume cropper only returns random crops,\n",
    "    # so the number of crops per epoch/batch is w/e i want\n",
    "    epoch_size=5,\n",
    "\n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = voldata_val\n",
    "labels = vollabels_val\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "grid_pos_gen = SequentialGridPosition.build_min_overlap(\n",
    "    volume_shape=volume_shape,\n",
    "    crop_shape=crop_shape,\n",
    "    \n",
    "    # [manual-input]\n",
    "    # reduce the total number of crops\n",
    "#         n_steps_x=11,\n",
    "#         n_steps_y=11,\n",
    "#     n_steps_z=30,\n",
    ")\n",
    "\n",
    "crop_seq_val = VolumeCropSequence(\n",
    "    \n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "\n",
    "    # go through all the crops in validation\n",
    "    epoch_size=len(grid_pos_gen),\n",
    "\n",
    "    # data augmentation\n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_val00(\n",
    "        volume_shape=volume_shape,\n",
    "        grid_pos_gen=grid_pos_gen,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "    ),\n",
    "\n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsccmAxOX7v"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "autosave_cb = keras_callbacks.ModelCheckpoint(\n",
    "    tomo2seg_model.autosaved2_model_path_str,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "logger.debug(f\"{autosave_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this is important because sometimes i update things in the notebook\n",
    "# so i need to make sure that the objects in the history cb are updated\n",
    "try:\n",
    "    history_cb\n",
    "\n",
    "except NameError:\n",
    "    \n",
    "    logger.info(\"Creating a new history callback.\")\n",
    "\n",
    "    history_cb = tomo2seg_callbacks.History(\n",
    "        optimizer=model.optimizer,\n",
    "        crop_seq_train=crop_seq_train,\n",
    "        crop_seq_val=crop_seq_val,\n",
    "        backup=1,\n",
    "        csv_path=tomo2seg_model.history_path,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    logger.warning(\"The history callback already exists!\")\n",
    "\n",
    "    history_df = history_cb.dataframe\n",
    "\n",
    "    try:\n",
    "        history_df_temp = pd.read_csv(tomo2seg_model.history_path)\n",
    "        # keep the longest one\n",
    "        history_df = history_df if history_df.shape[0] >= history_df_temp.shape[0] else history_df_temp\n",
    "        del history_df_temp\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "\n",
    "finally:\n",
    "    # make sure the correct objects are linked\n",
    "    history_cb.optimizer = model.optimizer\n",
    "    history_cb.crop_seq_train = crop_seq_train\n",
    "    history_cb.crop_seq_val = crop_seq_val\n",
    "\n",
    "logger.debug(f\"{history_cb=}\")\n",
    "logger.debug(f\"{history_cb.dataframe.index.size=}\")\n",
    "logger.debug(f\"{history_cb.last_epoch=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "history_plot_cb = tomo2seg_callbacks.HistoryPlot(\n",
    "    history_callback=history_cb,\n",
    "    save_path=tomo2seg_model.train_history_plot_wip_path\n",
    ")\n",
    "logger.debug(f\"{history_plot_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Setting up early stop with {args.early_stop_mode=}\")\n",
    "\n",
    "if args.early_stop_mode == Args.EarlyStopMode.no_early_stop:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.early_stop_mode=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYnLzlFdDeY"
   },
   "source": [
    "# Summary before training\n",
    "\n",
    "stuff that i use after the training but i want it to appear in the\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode## Metadata\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Volume slices\n",
    "\n",
    "todo do this in a notebook\n",
    "\n",
    "## Generator samples\n",
    "\n",
    "todo do this in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuEmT2AZODXi"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "lr_schedule_cb = keras_callbacks.LearningRateScheduler(\n",
    "    schedule=(\n",
    "        schedule := tomo2seg_schedule.get_schedule00()\n",
    "        #         schedule := tomo2seg_schedule.LogSpaceSchedule(\n",
    "        #             offset_epoch=0, wait=0, start=-3, stop=-5, n_between_scales=100\n",
    "        #         )\n",
    "    ),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"{lr_schedule_cb.schedule.range=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras_callbacks.TerminateOnNaN(),\n",
    "    autosave_cb,\n",
    "    history_cb,\n",
    "    history_plot_cb,\n",
    "    lr_schedule_cb,\n",
    "]\n",
    "\n",
    "try:\n",
    "    early_stop_cb\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    callbacks.append(early_stop_cb)\n",
    "\n",
    "for cb in callbacks:\n",
    "    logger.debug(f\"using callback {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "n_epochs = 300\n",
    "\n",
    "\n",
    "class TrainingFinished(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class FailedToFindBatchSize(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def fit():\n",
    "\n",
    "#     raise NotImplementedError(\"I have to automate the logic of the initial epoch...\")\n",
    "\n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # [manual-input]\n",
    "        # epochs\n",
    "        initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "        \n",
    "        #     initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "        #         epochs=history_cb.last_epoch + 1 + n_epochs,\n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,\n",
    "        verbose=2,\n",
    "\n",
    "        # todo change the volume sequence to dinamically load the volume\n",
    "        # because it would allow me to pass just a path string therefore\n",
    "        # making it serializible ==> i will be able to multithread (:\n",
    "        use_multiprocessing=False,\n",
    "    )\n",
    "    raise TrainingFinished()\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        fit()\n",
    "\n",
    "    except TrainingFinished:\n",
    "        slack.notify_finished()\n",
    "\n",
    "    except Exception as ex:\n",
    "\n",
    "        logger.exception(ex)\n",
    "\n",
    "        if args.batch_size_mode == Args.BatchSizeMode.try_max_and_fail:\n",
    "            raise ex\n",
    "\n",
    "        batch_size -= n_gpus\n",
    "        logger.warning(f\"reduced {batch_size=}\")\n",
    "\n",
    "        if batch_size < n_gpus:\n",
    "            raise FailedToFindBatchSize\n",
    "\n",
    "        crop_seq_train.batch_size = batch_size\n",
    "        crop_seq_val.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows := 2, ncols := 1, figsize=(\n",
    "    2.5 * (sz := 5), nrows * sz), dpi=100)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "hist_display = viz.TrainingHistoryDisplay(\n",
    "    history_cb.history,\n",
    "    model_name=tomo2seg_model.name,\n",
    "    loss_name=model.loss.__name__,\n",
    "    x_axis_mode=(\n",
    "        \"epoch\", \"batch\", \"crop\", \"voxel\", \"time\",\n",
    "    ),\n",
    ").plot(\n",
    "    axs,\n",
    "    with_lr=True,\n",
    "    metrics=(\n",
    "        \"loss\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[-1].set_yscale(\"log\")\n",
    "\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"loss\"][0])\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"val_loss\"][0], txt_kwargs=dict(rotation=0))\n",
    "\n",
    "hist_display.fig_.savefig(\n",
    "    tomo2seg_model.model_path / (hist_display.title + \".png\"),\n",
    "    format='png',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "aborted",
     "timestamp": 1602255923919,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "d-EnhRhrrEGQ"
   },
   "outputs": [],
   "source": [
    "history_cb.dataframe.to_csv(history_cb.csv_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "aborted",
     "timestamp": 1602255923920,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "LQz6HBJss1o4"
   },
   "outputs": [],
   "source": [
    "model.save(tomo2seg_model.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack.notify(f\"script `{args.script_name}` on `{args.host.hostname}` finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
