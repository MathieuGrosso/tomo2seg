{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JEHjvuBBIab"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1602255916978,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "KTMgQv07JkgY",
    "outputId": "69cd78fc-f0f1-46f6-f1d8-63b99d55eaae"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from enum import Enum\n",
    "import functools\n",
    "import operator\n",
    "from functools import partial\n",
    "import logging\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from typing import *\n",
    "import time\n",
    "import yaml\n",
    "from yaml import YAMLObject\n",
    "import socket\n",
    "\n",
    "import humanize\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymicro.file import file_utils\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks as keras_callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics as keras_metrics\n",
    "\n",
    "from tomo2seg import slack\n",
    "from tomo2seg import modular_unet\n",
    "from tomo2seg.logger import logger, dict2str, add_file_handler\n",
    "from tomo2seg import data as tomo2seg_data\n",
    "from tomo2seg import viz\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.metadata import Metadata\n",
    "from tomo2seg.volume_sequence import (\n",
    "    MetaCrop3DGenerator, VolumeCropSequence,\n",
    "    UniformGridPosition, SequentialGridPosition,\n",
    "    ET3DUniformCuboidAlmostEverywhere, ET3DConstantEverywhere, \n",
    "    GTUniformEverywhere, GTConstantEverywhere, \n",
    "    VSConstantEverywhere, VSUniformEverywhere\n",
    ")\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import callbacks as tomo2seg_callbacks\n",
    "from tomo2seg import losses as tomo2seg_losses\n",
    "from tomo2seg import schedule as tomo2seg_schedule\n",
    "from tomo2seg import utils as tomo2seg_utils\n",
    "from tomo2seg import slackme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are estimates based on things i've seen fit in the GPU\n",
    "MAX_INTERNAL_NVOXELS = max(\n",
    "    # seen cases\n",
    "    # batch_size * internal_multiplier_factor * (crop_nvoxels) * gpu_factor=1\n",
    "    4 * (8 * 6) * (96**3),\n",
    "    8 * (16 * 6) * (320**2),  \n",
    "    3 * (16 * 6) * (800 * 928),\n",
    "    15 * 23 * (208**2 * 5) * (8 / 5),\n",
    ")\n",
    "\n",
    "MAX_INTERNAL_NVOXELS *= 5/8  # a smaller gpu on other pcs...\n",
    "MAX_INTERNAL_NVOXELS = int(MAX_INTERNAL_NVOXELS)\n",
    "\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "\n",
    "    class EarlyStopMode(Enum):\n",
    "        no_early_stop = 0\n",
    "        \n",
    "    class BatchSizeMode(Enum):\n",
    "        try_max_and_fail = 0\n",
    "        try_max_and_reduce = 1\n",
    "        \n",
    "    # None: continue from the latest model\n",
    "    # 1: continue from model.autosaved_model_path\n",
    "    # 2: continue from model.autosaved2_model_path\n",
    "    # continue_from_autosave: Optional[int] = None \n",
    "    class TrainMode(Enum):\n",
    "        from_scratch = 0\n",
    "        continuation_from_autosaved_model = 1\n",
    "        continuation_from_autosaved2_best_model = 2\n",
    "        continuation_from_latest_model = 3\n",
    "\n",
    "        @property\n",
    "        def is_continuation(self) -> bool:\n",
    "            return self in (\n",
    "                Args.TrainMode.continuation_from_autosaved_model,\n",
    "                Args.TrainMode.continuation_from_autosaved2_best_model,\n",
    "                Args.TrainMode.continuation_from_latest_model,\n",
    "            )\n",
    "\n",
    "    early_stop_mode: EarlyStopMode\n",
    "    batch_size_mode: BatchSizeMode\n",
    "    train_mode: TrainMode\n",
    "        \n",
    "    volume_name: str\n",
    "    volume_version: str\n",
    "    labels_version: str\n",
    "    \n",
    "    # override the auto-sized value \n",
    "    # this allows to reproduce reproduce the same conditions across experiments\n",
    "    batch_size_per_gpu: Optional[int] = None  \n",
    "    \n",
    "    random_state_seed: int = 42\n",
    "        \n",
    "    runid: int = None\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        if self.train_mode.is_continuation:\n",
    "            assert self.runid is not None, f\"Incompatible args {self.runid=} {self.self.train_mode=}\"\n",
    "        \n",
    "        if self.runid is None:\n",
    "            self.runid = int(time.time())\n",
    "            \n",
    "        if self.batch_size_per_gpu is not None:\n",
    "            assert self.batch_size_per_gpu > 0, f\"{self.batch_size_per_gpu=}\"\n",
    "\n",
    "            ngpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "            if ngpus > 0: \n",
    "                assert self.batch_size_per_gpu % ngpus == 0, f\"{self.batch_size_per_gpu=} {ngpus=}\"\n",
    "            \n",
    "            try:\n",
    "                MAX_INTERNAL_NVOXELS\n",
    "            \n",
    "            except NameError as ex:\n",
    "                ValueError(f\"Please define the variable `{ex.args[0]}`\")\n",
    "\n",
    "                \n",
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_V1_REDUCED as VOLUME_NAME_VERSION,\n",
    "    VOLUME_COMPOSITE_V1_LABELS_REFINED3 as LABELS_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00_LABELS_REFINED3 as LABELS_VERSION,\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    early_stop_mode = Args.EarlyStopMode.no_early_stop,\n",
    "    batch_size_mode = Args.BatchSizeMode.try_max_and_reduce,\n",
    "    train_mode=Args.TrainMode.from_scratch,\n",
    "    \n",
    "    volume_name=VOLUME_NAME_VERSION[0],\n",
    "    volume_version=VOLUME_NAME_VERSION[1],\n",
    "    labels_version=LABELS_VERSION,\n",
    "    \n",
    "#     random_state_seed=30,  # I'll change it so we don't repeat the same crops from the begining\n",
    "#     runid = 1607698009,\n",
    ")\n",
    "\n",
    "logger.info(f\"args={dict2str(asdict(args))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnVqPFS9BNCg"
   },
   "source": [
    "\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "random_state = np.random.RandomState(args.random_state_seed)\n",
    "\n",
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "tf_version = tf.__version__\n",
    "logger.info(f\"{tf_version=}\")\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "logger.info(\n",
    "    f\"Hostname: {hostname}\\nNum GPUs Available: {n_gpus}\\nThis should be:\\n\\t\" + '\\n\\t'.join(['2 on R790-TOMO', '1 on akela', '1 on hathi', '1 on krilin'])\n",
    ")\n",
    "\n",
    "logger.debug(\n",
    "    \"physical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_physical_devices('GPU'))) + \"\\n\" +\n",
    "    \"logical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_logical_devices('GPU'))) \n",
    ")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "gpu_strategy = tf.distribute.MirroredStrategy()  \n",
    "logger.debug(f\"{gpu_strategy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "except NameError:\n",
    "    print(\"already deleted (:\")\n",
    "else:\n",
    "    del tomo2seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "crop_shape = (208, 208, 5)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "model_nclasses = 3\n",
    "\n",
    "model_master_name = \"unet2halfd\"\n",
    "model_version = \"II-enc-c208-f08\"\n",
    "\n",
    "model_is_2halfd = True\n",
    "model_is_2d = False\n",
    "\n",
    "model_factory_function = modular_unet.u_net2halfd_IIenc\n",
    "model_factory_kwargs = {\n",
    "    **modular_unet.kwargs_IIenc03,\n",
    "    **dict(\n",
    "        convlayer=modular_unet.ConvLayer.conv2d,\n",
    "        input_shape = crop_shape,\n",
    "        output_channels = model_nclasses,\n",
    "#         nb_filters_0 = 2,\n",
    "#         nb_filters_0 = 4,\n",
    "        nb_filters_0 = 8,\n",
    "#         nb_filters_0 = 12,\n",
    "#         nb_filters_0 = 16,common_random_state\n",
    "#         nb_filters_0 = 32,\n",
    "    ),\n",
    "}\n",
    "\n",
    "try:\n",
    "    tomo2seg_model\n",
    "    \n",
    "except NameError:\n",
    "    \n",
    "    logger.info(\"Creating a Tomo2SegModel.\")\n",
    "    \n",
    "    tomo2seg_model = Tomo2SegModel(\n",
    "        model_master_name, \n",
    "        model_version, \n",
    "        runid=args.runid,\n",
    "        factory_function=model_factory_function,\n",
    "        factory_kwargs=model_factory_kwargs,\n",
    "    )\n",
    "                    \n",
    "else:\n",
    "    logger.warning(\"The model is already defined. To create a new one: `del tomo2seg_model`\")\n",
    "\n",
    "finally:\n",
    "    logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")    \n",
    "    logger.info(f\"{tomo2seg_model.name=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Creating the Keras model.\")\n",
    "\n",
    "if args.train_mode.is_continuation:\n",
    "    logger.warning(\"Training continuation: a model will be loaded.\")\n",
    "\n",
    "    if args.train_mode == Args.TrainMode.continuation_from_latest_model:\n",
    "        logger.info(\"Using the LATEST model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved_model:\n",
    "        logger.info(\"Using the AUTOSAVED model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved_model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved2_best_model:\n",
    "        logger.info(\"Using the (best) AUTOSAVED2 model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved2_best_model_path\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"{args.train_mode=}\")\n",
    "\n",
    "elif (\n",
    "    tomo2seg_model.model_path.exists() or\n",
    "    tomo2seg_model.autosaved_model_path.exists()\n",
    "    # todo uncomment me when implemented\n",
    "#             or tomo2seg_model.autosaved2_best_model_path.exists()\n",
    "):\n",
    "    logger.error(f\"The model seems to already exist but this is not a continuation. Please, make sure the arguments are correct.\")\n",
    "    raise ValueError(f\"{args.train_mode=} ==> {args.train_mode.is_continuation=} {tomo2seg_model.name=}\")\n",
    "\n",
    "elif args.train_mode == Args.TrainMode.from_scratch:\n",
    "    logger.info(f\"A new model will be instantiated!\")\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.train_mode=}\")\n",
    "\n",
    "    \n",
    "with gpu_strategy.scope():\n",
    "    \n",
    "    if args.train_mode.is_continuation:\n",
    "        \n",
    "        assert load_model_path.exists(), f\"Inconsistent arguments {args.train_mode.is_continuation=} {load_model_path=} {load_model_path.exists()=}.\"\n",
    "        \n",
    "        logger.info(f\"Loading model {load_model_path.name}\")\n",
    "        \n",
    "        model = keras.models.load_model(str(load_model_path), compile=False)\n",
    "\n",
    "        assert model.name == tomo2seg_model.name, f\"{model.name=} {tomo2seg_model.name=}\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        logger.info(f\"Instantiating a new model with model_factory_function={model_factory_function.__name__}.\")\n",
    "      \n",
    "        model = model_factory_function(\n",
    "            name=tomo2seg_model.name,\n",
    "            **model_factory_kwargs\n",
    "        )\n",
    "\n",
    "    logger.info(\"Compiling the model.\")\n",
    "\n",
    "    # [manual-input]\n",
    "    # using the avg jaccard is dangerous if one of the classes is too\n",
    "    # underrepresented because it's jaccard will be unstable\n",
    "    # to be verified!\n",
    "    loss = tomo2seg_losses.jaccard2_flat\n",
    "    optimizer = optimizers.Adam(lr=.003)\n",
    "    metrics = []\n",
    "    \n",
    "    logger.debug(f\"{loss=}\")\n",
    "    logger.debug(f\"{optimizer=}\")\n",
    "    logger.debug(f\"{metrics=}\")\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [],
   "source": [
    "if not args.train_mode.is_continuation:\n",
    "    \n",
    "    logger.info(f\"Saving the model at {tomo2seg_model.model_path=}.\")\\\n",
    "    \n",
    "    model.save(tomo2seg_model.model_path)\n",
    "\n",
    "    logger.info(f\"Writing the model summary at {tomo2seg_model.summary_path=}.\")\n",
    "    \n",
    "    with tomo2seg_model.summary_path.open(\"w\") as f:\n",
    "        def print_to_txt(line):\n",
    "            f.writelines([line + \"\\n\"])\n",
    "        model.summary(print_fn=print_to_txt, line_length=140)\n",
    "\n",
    "    logger.info(f\"Printing an image of the architecture at {tomo2seg_model.architecture_plot_path=}.\")\n",
    "    \n",
    "    utils.plot_model(model, show_shapes=True, to_file=tomo2seg_model.architecture_plot_path);\n",
    "    \n",
    "add_file_handler(logger, tomo2seg_model.train_log_path)\n",
    "\n",
    "# repeat it so that the log file saves this\n",
    "logger.info(f\"args\\n{dict2str(asdict(args))}\")    \n",
    "logger.info(f\"{tomo2seg_model.name=}\")\n",
    "logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8e5FhmUaKND"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1602255917946,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "4CfP7usu2VKr"
   },
   "outputs": [],
   "source": [
    "# Metadata/paths objects\n",
    "\n",
    "## Volume\n",
    "volume = Volume.with_check(\n",
    "    name=args.volume_name, version=args.volume_version\n",
    ")\n",
    "\n",
    "logger.info(f\"volume\\n{dict2str(asdict(volume))}\")\n",
    "\n",
    "assert volume.nclasses\n",
    "\n",
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "## Data\n",
    "voldata = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "    \n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    "    \n",
    ") / volume.normalization_factor\n",
    "\n",
    "logger.debug(f\"{voldata.shape=}\")\n",
    "\n",
    "voldata_train = volume.train_partition.get_volume_partition(voldata)\n",
    "voldata_val = volume.val_partition.get_volume_partition(voldata)\n",
    "\n",
    "logger.debug(f\"{voldata_train.shape=}\")\n",
    "logger.debug(f\"{voldata_val.shape=}\")\n",
    "\n",
    "del voldata\n",
    "\n",
    "## Labels\n",
    "\n",
    "vollabels = file_utils.HST_read(\n",
    "    str(volume.versioned_labels_path(args.labels_version)),\n",
    "    \n",
    "    autoparse_filename=False,\n",
    "    data_type=\"uint8\",\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vollabels.shape=}\")\n",
    "\n",
    "vollabels_train = volume.train_partition.get_volume_partition(vollabels)\n",
    "vollabels_val = volume.val_partition.get_volume_partition(vollabels)\n",
    "\n",
    "logger.debug(f\"{vollabels_train.shape=}\")\n",
    "logger.debug(f\"{vollabels_val.shape=}\")\n",
    "\n",
    "del vollabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnsQ7lX0bVRh"
   },
   "source": [
    "# Data crop sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_internal_nvoxel_factor = tomo2seg_utils.get_model_internal_nvoxel_factor(model)\n",
    "\n",
    "logger.debug(f\"{model_internal_nvoxel_factor=}\")\n",
    "\n",
    "max_batch_nvoxels = int(np.floor(MAX_INTERNAL_NVOXELS / model_internal_nvoxel_factor))\n",
    "\n",
    "logger.debug(f\"{max_batch_nvoxels=} ({humanize.intcomma(max_batch_nvoxels)})\")\n",
    "\n",
    "crop_nvoxels = functools.reduce(operator.mul, crop_shape)\n",
    "\n",
    "logger.debug(f\"{crop_shape=} ==> {crop_nvoxels=}\")\n",
    "\n",
    "max_batch_size_per_gpu = batch_size_per_gpu = max(1, int(np.floor(max_batch_nvoxels / crop_nvoxels)))\n",
    "\n",
    "logger.info(f\"{batch_size_per_gpu=}\")\n",
    "\n",
    "if args.batch_size_per_gpu is not None:\n",
    "    logger.warning(f\"{args.batch_size_per_gpu=} given ==> replacing {batch_size_per_gpu=}\")\n",
    "    batch_size_per_gpu = args.batch_size_per_gpu\n",
    "\n",
    "logger.info(f\"{n_gpus=}\")\n",
    "\n",
    "batch_size = batch_size_per_gpu * max(1, n_gpus)\n",
    "\n",
    "logger.info(f\"{batch_size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "metacrop_gen_common_kwargs = dict(\n",
    "    crop_shape=crop_shape,\n",
    "    common_random_state_seed=args.random_state_seed,\n",
    "    is_2halfd=model_is_2halfd,\n",
    "    gt_type=volume_sequence.GT2D if model_is_2d or model_is_2halfd else volume_sequence.GT3D,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{metacrop_gen_common_kwargs=}\")\n",
    "\n",
    "vol_crop_seq_common_kwargs = dict(\n",
    "    output_as_2d = model_is_2d,\n",
    "    output_as_2halfd = model_is_2halfd,\n",
    "    labels = volume.metadata.labels,\n",
    "\n",
    "    # [manual-input]\n",
    "    debug__no_data_check=True,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vol_crop_seq_common_kwargs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = voldata_train\n",
    "labels = vollabels_train\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "crop_seq_train = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_train01(\n",
    "        volume_shape=volume_shape,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "        data_original_dtype=volume.metadata.dtype,\n",
    "        \n",
    "        # [manual-input]\n",
    "        gt_no_transpose_rot = False,\n",
    "    ),\n",
    "    \n",
    "    # this volume cropper only returns random crops, \n",
    "    # so the number of crops per epoch/batch is w/e i want\n",
    "    epoch_size=10,\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = voldata_val\n",
    "labels = vollabels_val\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "# the validation has no reproducibility issues\n",
    "# so let's push the GPUs (:\n",
    "val_batch_size = max_batch_size_per_gpu * n_gpus\n",
    "\n",
    "logger.debug(f\"{val_batch_size=}\")\n",
    "\n",
    "grid_pos_gen = SequentialGridPosition.build_min_overlap(\n",
    "    volume_shape=volume_shape, \n",
    "    crop_shape=crop_shape,\n",
    "    # [manual-input]\n",
    "    # reduce the total number of crops\n",
    "#         n_steps_x=11,\n",
    "#         n_steps_y=11,\n",
    "        n_steps_z=30,\n",
    ")\n",
    "\n",
    "crop_seq_val = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=val_batch_size,\n",
    "    \n",
    "    # go through all the crops in validation\n",
    "    epoch_size=len(grid_pos_gen),      \n",
    "    \n",
    "    # data augmentation\n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_val00(\n",
    "        volume_shape=volume_shape,\n",
    "        grid_pos_gen=grid_pos_gen,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "    ),\n",
    "#     debug__no_data_check = True,\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsccmAxOX7v"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "autosave_cb = keras_callbacks.ModelCheckpoint(\n",
    "    tomo2seg_model.autosaved2_model_path_str, \n",
    "    monitor=\"val_loss\", \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "logger.debug(f\"{autosave_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "# this is important because sometimes i update things in the notebook\n",
    "# so i need to make sure that the objects in the history cb are updated\n",
    "try:\n",
    "    history_cb\n",
    "    \n",
    "except NameError:\n",
    "    logger.info(\"Creating a new history callback.\")\n",
    "    \n",
    "    history_cb = tomo2seg_callbacks.History(\n",
    "        optimizer=model.optimizer,\n",
    "        crop_seq_train=crop_seq_train,\n",
    "        crop_seq_val=crop_seq_val,\n",
    "        backup=1,\n",
    "        csv_path=tomo2seg_model.history_path,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"The history callback already exists!\")\n",
    "    \n",
    "    history_df = history_cb.dataframe\n",
    "\n",
    "    try:\n",
    "        history_df_temp = pd.read_csv(tomo2seg_model.history_path)\n",
    "        # keep the longest one\n",
    "        history_df = history_df if history_df.shape[0] >= history_df_temp.shape[0] else history_df_temp\n",
    "        del history_df_temp\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "finally:\n",
    "    # make sure the correct objects are linked \n",
    "    history_cb.optimizer = model.optimizer\n",
    "    history_cb.crop_seq_train = crop_seq_train\n",
    "    history_cb.crop_seq_val = crop_seq_val\n",
    "\n",
    "logger.debug(f\"{history_cb=}\")\n",
    "logger.debug(f\"{history_cb.dataframe.index.size=}\")\n",
    "logger.debug(f\"{history_cb.last_epoch=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "history_plot_cb = tomo2seg_callbacks.HistoryPlot(\n",
    "    history_callback=history_cb,\n",
    "    save_path=tomo2seg_model.train_history_plot_wip_path\n",
    ")\n",
    "logger.debug(f\"{history_plot_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Setting up early stop with {args.early_stop_mode=}\")\n",
    "\n",
    "if args.early_stop_mode == Args.EarlyStopMode.no_early_stop:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.early_stop_mode=}\")\n",
    "#     # todo modify the early stopping to take more conditions (don't stop too early before it doesnt break the jaccard2=.32)\n",
    "#     early_stop_cb = keras_callbacks.EarlyStopping(  \n",
    "#         monitor='val_loss', \n",
    "#         min_delta=.1 / 100, \n",
    "#         patience=50,\n",
    "#         verbose=2, \n",
    "#         mode='auto',\n",
    "#         baseline=.71,  # 0th-order classifier\n",
    "#         restore_best_weights=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYnLzlFdDeY"
   },
   "source": [
    "# Summary before training\n",
    "\n",
    "stuff that i use after the training but i want it to appear in the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode## Metadata\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Volume slices\n",
    "\n",
    "todo do this in a notebook\n",
    "\n",
    "## Generator samples\n",
    "\n",
    "todo do this in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuEmT2AZODXi"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teeth log lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "lr_schedule_cb = keras_callbacks.LearningRateScheduler(\n",
    "    schedule=(\n",
    "        schedule := tomo2seg_schedule.get_schedule00()\n",
    "#         schedule := tomo2seg_schedule.LogSpaceSchedule(\n",
    "#             offset_epoch=0, wait=0, start=-3, stop=-5, n_between_scales=100\n",
    "#         )\n",
    "    ),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"{lr_schedule_cb.schedule.range=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras_callbacks.TerminateOnNaN(),\n",
    "    autosave_cb,\n",
    "    history_cb,\n",
    "    history_plot_cb,\n",
    "    lr_schedule_cb,\n",
    "]\n",
    "\n",
    "try:\n",
    "    early_stop_cb\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    callbacks.append(early_stop_cb)\n",
    "\n",
    "for cb in callbacks:\n",
    "    logger.debug(f\"using callback {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "n_epochs = 400\n",
    "\n",
    "\n",
    "class TrainingFinished(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class FailedToFindBatchSize(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def fit():\n",
    "    \n",
    "    raise NotImplementedError(\"I have to automate the logic of the initial epoch...\")\n",
    "    \n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # [manual-input]\n",
    "        # epochs\n",
    "        initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "    #     initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "    #         epochs=history_cb.last_epoch + 1 + n_epochs,  \n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,  \n",
    "        verbose=2,\n",
    "\n",
    "        # todo change the volume sequence to dinamically load the volume\n",
    "        # because it would allow me to pass just a path string therefore\n",
    "        # making it serializible ==> i will be able to multithread (:\n",
    "        use_multiprocessing=False,   \n",
    "    );\n",
    "    raise TrainingFinished()\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        fit()\n",
    "        \n",
    "    except TrainingFinished:\n",
    "        slack.notify_finished()\n",
    "        \n",
    "    except Exception as ex:\n",
    "                \n",
    "        logger.exception(ex)\n",
    "        \n",
    "        if args.batch_size_mode == Args.BatchSizeMode.try_max_and_fail:\n",
    "            raise ex\n",
    "        \n",
    "        batch_size -= n_gpus\n",
    "        logger.warning(f\"reduced {batch_size=}\")\n",
    "        \n",
    "        if batch_size < n_gpus:\n",
    "            raise FailedToFindBatchSize\n",
    "        \n",
    "        crop_seq_train.batch_size = batch_size\n",
    "        crop_seq_val.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows := 2, ncols := 1, figsize=(2.5 * (sz := 5), nrows * sz), dpi=100)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "hist_display = viz.TrainingHistoryDisplay(\n",
    "    history_cb.history, \n",
    "    model_name=tomo2seg_model.name,\n",
    "    loss_name=model.loss.__name__,\n",
    "    x_axis_mode=(\n",
    "        \"epoch\", \"batch\", \"crop\", \"voxel\", \"time\",\n",
    "    ),\n",
    ").plot(\n",
    "    axs, \n",
    "    with_lr=True,\n",
    "    metrics=(\n",
    "        \"loss\", \n",
    "    ),\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[-1].set_yscale(\"log\")\n",
    "\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"loss\"][0])\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"val_loss\"][0], txt_kwargs=dict(rotation=0))\n",
    "\n",
    "hist_display.fig_.savefig(\n",
    "    tomo2seg_model.model_path / (hist_display.title + \".png\"),\n",
    "    format='png',\n",
    ")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "aborted",
     "timestamp": 1602255923919,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "d-EnhRhrrEGQ"
   },
   "outputs": [],
   "source": [
    "history_cb.dataframe.to_csv(history_cb.csv_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "aborted",
     "timestamp": 1602255923920,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "LQz6HBJss1o4"
   },
   "outputs": [],
   "source": [
    "model.save(tomo2seg_model.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_name = \"train-06-akela90.ipynb\"\n",
    "import os\n",
    "this_dir = os.getcwd()\n",
    "logger.warning(f\"{this_nb_name=} {this_dir=}\")\n",
    "\n",
    "os.system(f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {str(tomo2seg_model.model_path)} --to html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2FW3h3DkQ4XcY6OgH7u/r",
   "collapsed_sections": [
    "EnVqPFS9BNCg",
    "j8e5FhmUaKND",
    "nJtppItnKn5G"
   ],
   "mount_file_id": "1LuEITv9j0lLf8Z418J3a94SjEZ8GvKvI",
   "name": "dryrun-02.ipynb",
   "provenance": [
    {
     "file_id": "1NiX28EcC_FVOYCJL4usp7n5iQ2x3aXIm",
     "timestamp": 1602152789440
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
