{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JEHjvuBBIab"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1602255916978,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "KTMgQv07JkgY",
    "outputId": "69cd78fc-f0f1-46f6-f1d8-63b99d55eaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import functools\n",
    "import operator\n",
    "from functools import partial\n",
    "import logging\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from typing import *\n",
    "import time\n",
    "import yaml\n",
    "from yaml import YAMLObject\n",
    "\n",
    "import humanize\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymicro.file import file_utils\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks as keras_callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics as keras_metrics\n",
    "\n",
    "from tomo2seg import slack\n",
    "from tomo2seg import modular_unet\n",
    "from tomo2seg.logger import logger\n",
    "from tomo2seg import data, viz\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.metadata import Metadata\n",
    "from tomo2seg.volume_sequence import (\n",
    "    MetaCrop3DGenerator, VolumeCropSequence,\n",
    "    UniformGridPosition, SequentialGridPosition,\n",
    "    ET3DUniformCuboidAlmostEverywhere, ET3DConstantEverywhere, \n",
    "    GTUniformEverywhere, GTConstantEverywhere, \n",
    "    VSConstantEverywhere, VSUniformEverywhere\n",
    ")\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import callbacks as tomo2seg_callbacks\n",
    "from tomo2seg import losses as tomo2seg_losses\n",
    "from tomo2seg import schedule as tomo2seg_schedule\n",
    "from tomo2seg import utils as tomo2seg_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnVqPFS9BNCg"
   },
   "source": [
    "\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-7-f5dd942bd525>:<module>:005}::[2020-12-09::17:16:20.160]\n",
      "runid=1607530580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "random_state = np.random.RandomState(random_state)\n",
    "runid = int(time.time())\n",
    "# runid = 1606238421\n",
    "logger.info(f\"{runid=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-8-3220786131f1>:<module>:003}::[2020-12-09::17:16:20.259]\n",
      "tf.__version__='2.2.0'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-8-3220786131f1>:<module>:004}::[2020-12-09::17:16:20.260]\n",
      "Num GPUs Available: 1\n",
      "This should be 2 on R790-TOMO.\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-8-3220786131f1>:<module>:005}::[2020-12-09::17:16:20.261]\n",
      "Should return 2 devices...\n",
      "tf.config.list_physical_devices('GPU')=[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-8-3220786131f1>:<module>:006}::[2020-12-09::17:16:20.350]\n",
      "Should return 2 devices...\n",
      "tf.config.list_logical_devices('GPU')=[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "logger.debug(f\"{tf.__version__=}\")\n",
    "logger.info(f\"Num GPUs Available: {n_gpus}\\nThis should be 2 on R790-TOMO.\")\n",
    "logger.debug(f\"Should return 2 devices...\\n{tf.config.list_physical_devices('GPU')=}\")\n",
    "logger.debug(f\"Should return 2 devices...\\n{tf.config.list_logical_devices('GPU')=}\")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "strategy = tf.distribute.MirroredStrategy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-9-47a20a733e5b>:<module>:002}::[2020-12-09::17:16:20.395]\n",
      "MULTIPLE_REQUIREMENT=16\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-9-47a20a733e5b>:<module>:014}::[2020-12-09::17:16:20.396]\n",
      "MAX_INTERNAL_NVOXELS=133632000.0 (133,632,000.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MULTIPLE_REQUIREMENT = 16\n",
    "logger.info(f\"{MULTIPLE_REQUIREMENT=}\")\n",
    "\n",
    "# these are estimates based on things i've seen fit in the GPU\n",
    "MAX_INTERNAL_NVOXELS = max(\n",
    "    # seen cases\n",
    "    4 * (8 * 6) * (96**3),\n",
    "    8 * (16 * 6) * (320**2),  \n",
    "    3 * (16 * 6) * (800 * 928),\n",
    ")\n",
    "\n",
    "MAX_INTERNAL_NVOXELS *= 5/8  # a smaller gpu on other pcs...\n",
    "\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")\n",
    "\n",
    "# override_batch_size = None\n",
    "# doing this to reproduce the same conditions...\n",
    "override_batch_size_per_gpu = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8e5FhmUaKND"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-10-cd76321aa4c8>:<module>:010}::[2020-12-09::17:16:20.449]\n",
      "volume_name='PA66GF30'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-cd76321aa4c8>:<module>:011}::[2020-12-09::17:16:20.450]\n",
      "volume_version='v1'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-cd76321aa4c8>:<module>:012}::[2020-12-09::17:16:20.450]\n",
      "labels_version='refined3'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_V1_REDUCED as VOLUME_NAME_VERSION,\n",
    "    VOLUME_COMPOSITE_V1_LABELS_REFINED3 as LABELS_VERSION\n",
    ")\n",
    "\n",
    "volume_name, volume_version = VOLUME_NAME_VERSION\n",
    "labels_version = LABELS_VERSION\n",
    "\n",
    "logger.info(f\"{volume_name=}\")\n",
    "logger.info(f\"{volume_version=}\")\n",
    "logger.info(f\"{labels_version=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1602255917946,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "4CfP7usu2VKr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{data.py:with_check:258}::[2020-12-09::17:16:20.499]\n",
      "vol=Volume(name='PA66GF30', version='v1', _metadata=None)\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:metadata:195}::[2020-12-09::17:16:20.500]\n",
      "Loading metadata from `/home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.metadata.yml`.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:007}::[2020-12-09::17:16:20.505]\n",
      "volume=Volume(name='PA66GF30', version='v1', _metadata=Volume.Metadata(dimensions=[1300, 1040, 1900], dtype='uint8', labels=[0, 1, 2], labels_names={0: 'matrix', 1: 'fiber', 2: 'porosity'}, set_partitions={'train': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [0, 1300], 'alias': 'train'}, 'val': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [1600, 1900], 'alias': 'val'}, 'test': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [1300, 1600], 'alias': 'test'}}))\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:024}::[2020-12-09::17:16:20.505]\n",
      "Loading data from disk.\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:028}::[2020-12-09::17:16:34.041]\n",
      "voldata.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:033}::[2020-12-09::17:16:34.042]\n",
      "voldata_train.shape=(1300, 1040, 1300)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:034}::[2020-12-09::17:16:34.043]\n",
      "voldata_val.shape=(1300, 1040, 300)\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:040}::[2020-12-09::17:16:38.375]\n",
      "vollabels.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:045}::[2020-12-09::17:16:38.376]\n",
      "vollabels_train.shape=(1300, 1040, 1300)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-6e0649e4ec97>:<module>:046}::[2020-12-09::17:16:38.376]\n",
      "vollabels_val.shape=(1300, 1040, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metadata/paths objects\n",
    "\n",
    "## Volume\n",
    "volume = Volume.with_check(\n",
    "    name=volume_name, version=volume_version\n",
    ")\n",
    "logger.info(f\"{volume=}\")\n",
    "\n",
    "n_classes = len(volume.metadata.labels)\n",
    "\n",
    "def _read_raw(path_: Path, volume_: Volume): \n",
    "    # from pymicro\n",
    "    return file_utils.HST_read(\n",
    "        str(path_),  # it doesn't accept paths...\n",
    "        # pre-loaded kwargs\n",
    "        autoparse_filename=False,  # the file names are not properly formatted\n",
    "        data_type=volume.metadata.dtype,\n",
    "        dims=volume.metadata.dimensions,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "read_raw = partial(_read_raw, volume_=volume)\n",
    "\n",
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "## Data\n",
    "voldata = read_raw(volume.data_path) / 255  # normalize\n",
    "logger.debug(f\"{voldata.shape=}\")\n",
    "\n",
    "voldata_train = volume.train_partition.get_volume_partition(voldata)\n",
    "voldata_val = volume.val_partition.get_volume_partition(voldata)\n",
    "\n",
    "logger.debug(f\"{voldata_train.shape=}\")\n",
    "logger.debug(f\"{voldata_val.shape=}\")\n",
    "\n",
    "del voldata\n",
    "\n",
    "## Labels\n",
    "vollabels = read_raw(volume.versioned_labels_path(labels_version))\n",
    "logger.debug(f\"{vollabels.shape=}\")\n",
    "\n",
    "vollabels_train = volume.train_partition.get_volume_partition(vollabels)\n",
    "vollabels_val = volume.val_partition.get_volume_partition(vollabels)\n",
    "\n",
    "logger.debug(f\"{vollabels_train.shape=}\")\n",
    "logger.debug(f\"{vollabels_val.shape=}\")\n",
    "\n",
    "del vollabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already deleted (:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "except NameError:\n",
    "    print(\"already deleted (:\")\n",
    "else:\n",
    "    del tomo2seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [],
   "source": [
    "# crop_shape = (256, 256, 1)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "# bigger crops will have less border effects (?)\n",
    "crop_shape = (48, 48, 1)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "\n",
    "model_master_name = \"unet2d\"\n",
    "model_version = \"crop48-f16\"\n",
    "model_factory_function = modular_unet.u_net\n",
    "\n",
    "model_is_2halfd = False\n",
    "model_is_2d = True\n",
    "\n",
    "model_factory_kwargs = {\n",
    "    **modular_unet.kwargs_vanilla03,\n",
    "    **dict(\n",
    "        convlayer=modular_unet.ConvLayer.conv2d,\n",
    "        input_shape = crop_shape,\n",
    "        output_channels=n_classes,\n",
    "#         nb_filters_0 = 2,\n",
    "#         nb_filters_0 = 4,\n",
    "#         nb_filters_0 = 8,\n",
    "#         nb_filters_0 = 12,\n",
    "        nb_filters_0 = 16,\n",
    "#         nb_filters_0 = 32,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:005}::[2020-12-09::17:16:38.532]\n",
      "Creating a Tomo2SegModel.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:020}::[2020-12-09::17:16:38.533]\n",
      "tomo2seg_model=Model(master_name='unet2d', version='crop48-f16', fold=0, runid=1607530580, factory_function='tomo2seg.modular_unet.u_net', factory_kwargs={'depth': 4, 'sigma_noise': 0, 'updown_conv_sampling': True, 'unet_block_kwargs': {'kernel_size': 3, 'res': True, 'batch_norm': True, 'dropout': 0}, 'unet_down_kwargs': {'batchnorm': True}, 'unet_up_kwargs': {'batchnorm': True}, 'convlayer': <ConvLayer.conv2d: 0>, 'input_shape': (48, 48, 1), 'output_channels': 3, 'nb_filters_0': 16})\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:022}::[2020-12-09::17:16:38.534]\n",
      "Creating the Keras model.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:026}::[2020-12-09::17:16:38.535]\n",
      "Instantiating a new model with model_factory_function=u_net\n",
      "\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:036}::[2020-12-09::17:16:40.223]\n",
      "Compiling the model.\n",
      "\n",
      "WARNING:tensorflow:From /home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/assets\n",
      "INFO::tomo2seg::{<ipython-input-14-2291d74903ed>:<module>:067}::[2020-12-09::17:16:54.461]\n",
      "Check the summary and the figure of the model in the following locations:\n",
      "/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/summary.txt\n",
      "/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/architecture.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "    \n",
    "except NameError:\n",
    "    logger.info(\"Creating a Tomo2SegModel.\")\n",
    "    \n",
    "    tomo2seg_model = Tomo2SegModel(\n",
    "        model_master_name, \n",
    "        model_version, \n",
    "        runid=runid,\n",
    "        factory_function=model_factory_function,\n",
    "        factory_kwargs=model_factory_kwargs,\n",
    "    )\n",
    "                \n",
    "else:\n",
    "    logger.warning(\"The model is already defined. To create a new one: `del tomo2seg_model`\")\n",
    "\n",
    "finally:\n",
    "    \n",
    "    logger.info(f\"{tomo2seg_model=}\")\n",
    "    \n",
    "logger.info(\"Creating the Keras model.\")\n",
    "\n",
    "with strategy.scope():\n",
    "    if not tomo2seg_model.autosaved_model_path.exists():\n",
    "        logger.info(f\"Instantiating a new model with model_factory_function={model_factory_function.__name__}\")\n",
    "      \n",
    "        model = model_factory_function(\n",
    "            name=tomo2seg_model.name,\n",
    "            **model_factory_kwargs\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"An autosaved model already exists, loading it instead of creating a new one!\")\n",
    "        model = keras.models.load_model(tomo2seg_model.autosaved_model_path_str, compile=False)\n",
    "\n",
    "    logger.info(\"Compiling the model.\")\n",
    "\n",
    "    # using the avg jaccard is dangerous if one of the classes is too\n",
    "    # underrepresented because it's jaccard will be unstable\n",
    "    loss = tomo2seg_losses.jaccard2_flat\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=.003)\n",
    "    metrics = [\n",
    "#         tomo2seg_losses.jaccard2_macro_avg,\n",
    "#         keras_metrics.Accuracy(),\n",
    "#     ] + [\n",
    "#         tomo2seg_losses.Jaccard2(class_idx)\n",
    "#         for class_idx in range(n_classes)\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss, \n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    model.save(tomo2seg_model.model_path)\n",
    "\n",
    "    # write the model summary in a file\n",
    "    with tomo2seg_model.summary_path.open(\"w\") as f:\n",
    "        def print_to_txt(line):\n",
    "            f.writelines([line + \"\\n\"])\n",
    "        model.summary(print_fn=print_to_txt, line_length=140)\n",
    "\n",
    "    # same for the architecture\n",
    "    utils.plot_model(model, show_shapes=True, to_file=tomo2seg_model.architecture_plot_path);\n",
    "\n",
    "    logger.info(f\"Check the summary and the figure of the model in the following locations:\\n{tomo2seg_model.summary_path}\\n{tomo2seg_model.architecture_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnsQ7lX0bVRh"
   },
   "source": [
    "# Data crop sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:017}::[2020-12-09::17:16:54.522]\n",
      "input_layer=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7fcb02c82220>\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:023}::[2020-12-09::17:16:54.523]\n",
      "input_nvoxels=2304\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:037}::[2020-12-09::17:16:54.524]\n",
      "max_internal_nvoxels=221184 (221,184)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:005}::[2020-12-09::17:16:54.525]\n",
      "model_internal_nvoxel_factor=96\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:009}::[2020-12-09::17:16:54.526]\n",
      "max_batch_nvoxels=1392000 (1,392,000)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:013}::[2020-12-09::17:16:54.526]\n",
      "crop_shape=(48, 48, 1) ==> crop_nvoxels=2304\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:017}::[2020-12-09::17:16:54.527]\n",
      "batch_size_per_gpu=604\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:025}::[2020-12-09::17:16:54.527]\n",
      "override_batch_size_per_gpu=8 given ==> replacing batch_size_per_gpu=8\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:027}::[2020-12-09::17:16:54.528]\n",
      "n_gpus=1\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-15-72fa96fad2a5>:<module>:031}::[2020-12-09::17:16:54.528]\n",
      "batch_size=8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size_per_replica = 1\n",
    "\n",
    "model_internal_nvoxel_factor = tomo2seg_utils.get_model_internal_nvoxel_factor(model)\n",
    "\n",
    "logger.debug(f\"{model_internal_nvoxel_factor=}\")\n",
    "\n",
    "max_batch_nvoxels = int(np.floor(MAX_INTERNAL_NVOXELS / model_internal_nvoxel_factor))\n",
    "\n",
    "logger.debug(f\"{max_batch_nvoxels=} ({humanize.intcomma(max_batch_nvoxels)})\")\n",
    "\n",
    "crop_nvoxels = functools.reduce(operator.mul, crop_shape)\n",
    "\n",
    "logger.debug(f\"{crop_shape=} ==> {crop_nvoxels=}\")\n",
    "\n",
    "max_batch_size_per_gpu = batch_size_per_gpu = int(np.floor(max_batch_nvoxels / crop_nvoxels))\n",
    "\n",
    "logger.info(f\"{batch_size_per_gpu=}\")\n",
    "\n",
    "if override_batch_size_per_gpu is not None:\n",
    "    \n",
    "    assert override_batch_size_per_gpu > 0, f\"{override_batch_size_per_gpu=}\"\n",
    "    \n",
    "    batch_size_per_gpu = override_batch_size_per_gpu\n",
    "    \n",
    "    logger.warning(f\"{override_batch_size_per_gpu=} given ==> replacing {batch_size_per_gpu=}\")\n",
    "\n",
    "logger.info(f\"{n_gpus=}\")\n",
    "\n",
    "batch_size = batch_size_per_gpu * max(1, n_gpus)\n",
    "\n",
    "logger.info(f\"{batch_size=}\")\n",
    "\n",
    "common_random_state = 143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomo2seg import volume_sequence\n",
    "\n",
    "metacrop_gen_common_kwargs = dict(\n",
    "    crop_shape=crop_shape,\n",
    "    common_random_state_seed=common_random_state,\n",
    "    is_2halfd=model_is_2halfd,\n",
    "    gt_type=volume_sequence.GT2D if model_is_2d else volume_sequence.GT3D,\n",
    ")\n",
    "\n",
    "vol_crop_seq_common_kwargs = dict(\n",
    "    output_as_2d=model_is_2d,\n",
    "    output_as_2halfd=model_is_2halfd,\n",
    "    labels = volume.metadata.labels,\n",
    "\n",
    "    # not automated...\n",
    "    debug__no_data_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:438}::[2020-12-09::17:18:35.465]\n",
      "Built UniformGridPosition from volume_shape=(1300, 1040, 1300) and crop_shape=(48, 48, 1) ==> {'x_range': (0, 1253), 'y_range': (0, 993), 'z_range': (0, 1300)}\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:400}::[2020-12-09::17:18:35.467]\n",
      "UniformGridPosition ==> npositions=1617497700 (1,617,497,700)\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-09::17:18:35.467]\n",
      "Initializing ET3DConstantEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-09::17:18:35.470]\n",
      "Initializing GTUniformEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-09::17:18:35.472]\n",
      "Initializing VSUniformEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:1353}::[2020-12-09::17:18:35.473]\n",
      "Initializing VolumeCropSequence.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1385}::[2020-12-09::17:18:35.474]\n",
      "No meta crops history file path given. The randomly generated crops will not be saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = voldata_train\n",
    "labels = vollabels_train\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "crop_seq_train = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_train00(\n",
    "        volume_shape=volume_shape,\n",
    "        **metacrop_gen_common_kwargs\n",
    "    ),\n",
    "    \n",
    "    # this volume cropper only returns random crops, \n",
    "    # so the number of crops per epoch/batch is w/e i want\n",
    "    epoch_size=10,\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{volume_sequence.py:build_min_overlap:506}::[2020-12-10::13:51:56.905]\n",
      "Building SequentialGridPosition with minimal overlap (smallest n_steps in each directions) n_steps={'n_steps_x': 28, 'n_steps_y': 22, 'n_steps_z': 300}.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:build_min_overlap:509}::[2020-12-10::13:51:56.907]\n",
      "n_steps_kwargs={'n_steps_x': 10, 'n_steps_y': 10, 'n_steps_z': 10} was given --> effective n_steps={'n_steps_x': 10, 'n_steps_y': 10, 'n_steps_z': 10}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:438}::[2020-12-10::13:51:56.908]\n",
      "Built SequentialGridPosition from volume_shape=(1300, 1040, 300) and crop_shape=(48, 48, 1) ==> {'x_range': (0, 1253), 'y_range': (0, 993), 'z_range': (0, 300)}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:__post_init__:486}::[2020-12-10::13:51:56.913]\n",
      "The SequentialGridPosition has len(self.positions)=1000 different positions (therefore crops).\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-10::13:51:56.914]\n",
      "Initializing ET3DConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-10::13:51:56.915]\n",
      "Initializing GTConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:695}::[2020-12-10::13:51:56.916]\n",
      "Initializing VSConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1385}::[2020-12-10::13:51:56.916]\n",
      "No meta crops history file path given. The randomly generated crops will not be saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = voldata_val\n",
    "labels = vollabels_val\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "# the validation has no reproducibility issues\n",
    "# so let's push the GPUs (:\n",
    "val_batch_size = max_batch_size_per_gpu * n_gpus\n",
    "\n",
    "logger.debug(f\"{val_batch_size=}\")\n",
    "\n",
    "grid_pos_gen = SequentialGridPosition.build_min_overlap(\n",
    "    volume_shape=volume_shape, \n",
    "    crop_shape=crop_shape,\n",
    "    # reduce the total number of crops\n",
    "    \n",
    "    # this was way too long\n",
    "#         n_steps_x=15,\n",
    "#         n_steps_y=15,\n",
    "#         n_steps_z=15,\n",
    "    \n",
    "    # i changed to this at epoch 36\n",
    "        n_steps_x=10,\n",
    "        n_steps_y=10,\n",
    "        n_steps_z=10,\n",
    ")\n",
    "\n",
    "crop_seq_val = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=val_batch_size,\n",
    "    \n",
    "    # go through all the crops in validation\n",
    "    epoch_size=len(grid_pos_gen),      \n",
    "    \n",
    "    # data augmentation\n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_val00(\n",
    "        volume_shape=volume_shape,\n",
    "        grid_pos_gen=grid_pos_gen,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "    ),\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsccmAxOX7v"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::tomo2seg::{<ipython-input-38-51b55b964faa>:<module>:023}::[2020-12-10::13:52:06.164]\n",
      "The history callback already exists!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autosave_cb = keras_callbacks.ModelCheckpoint(\n",
    "    tomo2seg_model.autosaved_model_path_str, \n",
    "    monitor=\"val_loss\", \n",
    "    verbose=2, \n",
    "    save_best_only=True, \n",
    "    mode=\"auto\",\n",
    ")\n",
    "\n",
    "# todo load if it already exists\n",
    "try:\n",
    "    history_cb\n",
    "    \n",
    "except NameError:\n",
    "    history_cb = tomo2seg_callbacks.History(\n",
    "        optimizer=model.optimizer,\n",
    "        crop_seq_train=crop_seq_train,\n",
    "        crop_seq_val=crop_seq_val,\n",
    "        backup=1,\n",
    "        csv_path=tomo2seg_model.history_path,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"The history callback already exists!\")\n",
    "    \n",
    "    history_df = history_cb.dataframe\n",
    "\n",
    "    try:\n",
    "        history_df_temp = pd.read_csv(tomo2seg_model.history_path)\n",
    "        # keep the longest one\n",
    "        history_df = history_df if history_df.shape[0] >= history_df_temp.shape[0] else history_df_temp\n",
    "        del history_df_temp\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "finally:\n",
    "    # make sure the correct objects are linked \n",
    "    history_cb.model = model\n",
    "    history_cb.crop_seq_train = crop_seq_train\n",
    "    history_cb.crop_seq_val = crop_seq_val\n",
    "    # todo do the same with other objs in history_cb\n",
    "    \n",
    "history_plot_cb = tomo2seg_callbacks.HistoryPlot(\n",
    "    history_callback=history_cb,\n",
    "    save_path=tomo2seg_model.train_history_plot_wip_path\n",
    ")\n",
    "\n",
    "early_stop_cb = keras_callbacks.EarlyStopping(  # todo modify the early stopping to take more conditions (don't stop too early before it doesnt break the jaccard2=.32)\n",
    "    monitor='val_loss', \n",
    "    min_delta=.1 / 100, \n",
    "    patience=50,\n",
    "    verbose=2, \n",
    "    mode='auto',\n",
    "    baseline=.71,  # 0th-order classifier\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYnLzlFdDeY"
   },
   "source": [
    "# Summary before training\n",
    "\n",
    "stuff that i use after the training but i want it to appear in the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Volume slices\n",
    "\n",
    "todo do this in a notebook\n",
    "\n",
    "## Generator samples\n",
    "\n",
    "todo do this in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuEmT2AZODXi"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teeth log lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.596]\n",
      "LogSpaceSchedule ==> self.n=10\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.597]\n",
      "LogSpaceSchedule ==> self.n=30\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.598]\n",
      "LogSpaceSchedule ==> self.n=20\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.599]\n",
      "LogSpaceSchedule ==> self.n=40\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.599]\n",
      "LogSpaceSchedule ==> self.n=20\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.600]\n",
      "LogSpaceSchedule ==> self.n=40\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-09::17:31:22.600]\n",
      "LogSpaceSchedule ==> self.n=100\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:107}::[2020-12-09::17:31:22.601]\n",
      "ComposedSchedule ==> self.n=260\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:006}::[2020-12-09::17:31:22.602]\n",
      "(0, 260)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:018}::[2020-12-09::17:31:22.602]\n",
      "using callback TerminateOnNaN\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:018}::[2020-12-09::17:31:22.603]\n",
      "using callback ModelCheckpoint\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:018}::[2020-12-09::17:31:22.603]\n",
      "using callback History\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:018}::[2020-12-09::17:31:22.604]\n",
      "using callback HistoryPlot\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-35-7223301c97cd>:<module>:018}::[2020-12-09::17:31:22.604]\n",
      "using callback LearningRateScheduler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_schedule_cb = keras_callbacks.LearningRateScheduler(\n",
    "    schedule= tomo2seg_schedule.get_schedule00(),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"{schedule.range}\")\n",
    "\n",
    "callbacks = [\n",
    "    keras_callbacks.TerminateOnNaN(),\n",
    "    autosave_cb,\n",
    "    history_cb,\n",
    "    history_plot_cb,\n",
    "    lr_schedule_cb,\n",
    "#     early_stop_cb,\n",
    "]\n",
    "\n",
    "for cb in callbacks:\n",
    "    logger.debug(f\"using callback {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67003, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::18:18:55.461]\n",
      "Saving backup of the training history epoch=0 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:103}::[2020-12-09::18:18:55.652]\n",
      "epoch=0 is too early to plot something.\n",
      "\n",
      "ERROR::tomo2seg::{callbacks.py:on_epoch_end:144}::[2020-12-09::18:18:55.723]\n",
      "AssertionError occurred while trying to plot the history.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/callbacks.py\", line 115, in on_epoch_end\n",
      "    def on_epoch_end(self, epoch, logs=None):\n",
      "  File \"<string>\", line 9, in __init__\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/viz.py\", line 254, in __post_init__\n",
      "    assert len(x_axis) > 1, \"You don't have enough epochs to plot. Go to the gym and call me later.\"\n",
      "AssertionError: You don't have enough epochs to plot. Go to the gym and call me later.\n",
      "10/10 - 2176s - loss: 0.7805 - val_loss: 0.6700 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001291549665014884.\n",
      "Epoch 2/400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.67003 to 0.64897, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::18:50:48.520]\n",
      "Saving backup of the training history epoch=1 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:103}::[2020-12-09::18:50:48.543]\n",
      "epoch=1 is too early to plot something.\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::18:50:48.583]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::18:50:48.604]\n",
      "train: argmin=1 --> min=0.536\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::18:50:48.608]\n",
      "val: argmin=1 --> min=0.649\n",
      "\n",
      "10/10 - 1913s - loss: 0.5361 - val_loss: 0.6490 - lr: 1.2915e-04\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001668100537200059.\n",
      "Epoch 3/400\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64897 to 0.60799, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::19:22:40.514]\n",
      "Saving backup of the training history epoch=2 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::19:22:40.570]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::19:22:40.592]\n",
      "train: argmin=2 --> min=0.358\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::19:22:40.594]\n",
      "val: argmin=2 --> min=0.608\n",
      "\n",
      "10/10 - 1912s - loss: 0.3575 - val_loss: 0.6080 - lr: 1.6681e-04\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00021544346900318845.\n",
      "Epoch 4/400\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60799 to 0.56517, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::19:54:31.678]\n",
      "Saving backup of the training history epoch=3 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::19:54:31.729]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::19:54:31.750]\n",
      "train: argmin=3 --> min=0.247\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::19:54:31.751]\n",
      "val: argmin=3 --> min=0.565\n",
      "\n",
      "10/10 - 1911s - loss: 0.2472 - val_loss: 0.5652 - lr: 2.1544e-04\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002782559402207126.\n",
      "Epoch 5/400\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.56517 to 0.52344, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::20:26:23.025]\n",
      "Saving backup of the training history epoch=4 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::20:26:23.108]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::20:26:23.129]\n",
      "train: argmin=4 --> min=0.187\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::20:26:23.131]\n",
      "val: argmin=4 --> min=0.523\n",
      "\n",
      "10/10 - 1911s - loss: 0.1869 - val_loss: 0.5234 - lr: 2.7826e-04\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00035938136638046257.\n",
      "Epoch 6/400\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.52344 to 0.49516, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::20:58:13.065]\n",
      "Saving backup of the training history epoch=5 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::20:58:13.139]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::20:58:13.160]\n",
      "train: argmin=5 --> min=0.14\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::20:58:13.162]\n",
      "val: argmin=5 --> min=0.495\n",
      "\n",
      "10/10 - 1910s - loss: 0.1402 - val_loss: 0.4952 - lr: 3.5938e-04\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00046415888336127773.\n",
      "Epoch 7/400\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.49516 to 0.47221, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::21:30:03.868]\n",
      "Saving backup of the training history epoch=6 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::21:30:03.919]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::21:30:03.940]\n",
      "train: argmin=6 --> min=0.112\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::21:30:03.941]\n",
      "val: argmin=6 --> min=0.472\n",
      "\n",
      "10/10 - 1911s - loss: 0.1118 - val_loss: 0.4722 - lr: 4.6416e-04\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0005994842503189409.\n",
      "Epoch 8/400\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47221 to 0.40171, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::22:01:53.082]\n",
      "Saving backup of the training history epoch=7 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::22:01:53.140]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::22:01:53.161]\n",
      "train: argmin=7 --> min=0.0663\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::22:01:53.163]\n",
      "val: argmin=7 --> min=0.402\n",
      "\n",
      "10/10 - 1909s - loss: 0.0663 - val_loss: 0.4017 - lr: 5.9948e-04\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.000774263682681127.\n",
      "Epoch 9/400\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.40171 to 0.36864, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::22:33:41.729]\n",
      "Saving backup of the training history epoch=8 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::22:33:41.789]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::22:33:41.810]\n",
      "train: argmin=8 --> min=0.0547\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::22:33:41.811]\n",
      "val: argmin=8 --> min=0.369\n",
      "\n",
      "10/10 - 1909s - loss: 0.0547 - val_loss: 0.3686 - lr: 7.7426e-04\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/400\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36864 to 0.33865, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::23:05:31.476]\n",
      "Saving backup of the training history epoch=9 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::23:05:31.544]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::23:05:31.567]\n",
      "train: argmin=9 --> min=0.0382\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::23:05:31.569]\n",
      "val: argmin=9 --> min=0.339\n",
      "\n",
      "10/10 - 1910s - loss: 0.0382 - val_loss: 0.3387 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/400\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.33865 to 0.28844, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-09::23:37:21.671]\n",
      "Saving backup of the training history epoch=10 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-09::23:37:21.729]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::23:37:21.749]\n",
      "train: argmin=10 --> min=0.0371\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-09::23:37:21.751]\n",
      "val: argmin=10 --> min=0.288\n",
      "\n",
      "10/10 - 1910s - loss: 0.0371 - val_loss: 0.2884 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/400\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.28844 to 0.27707, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::00:09:10.863]\n",
      "Saving backup of the training history epoch=11 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::00:09:10.918]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::00:09:10.939]\n",
      "train: argmin=11 --> min=0.0357\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::00:09:10.941]\n",
      "val: argmin=11 --> min=0.277\n",
      "\n",
      "10/10 - 1909s - loss: 0.0357 - val_loss: 0.2771 - lr: 0.0010\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/400\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.27707 to 0.26246, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::00:40:59.211]\n",
      "Saving backup of the training history epoch=12 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::00:40:59.262]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::00:40:59.282]\n",
      "train: argmin=12 --> min=0.0282\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::00:40:59.284]\n",
      "val: argmin=12 --> min=0.262\n",
      "\n",
      "10/10 - 1908s - loss: 0.0282 - val_loss: 0.2625 - lr: 0.0010\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/400\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.26246\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::01:12:45.162]\n",
      "Saving backup of the training history epoch=13 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::01:12:45.230]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::01:12:45.251]\n",
      "train: argmin=12 --> min=0.0282\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::01:12:45.252]\n",
      "val: argmin=12 --> min=0.262\n",
      "\n",
      "10/10 - 1906s - loss: 0.0282 - val_loss: 0.2645 - lr: 0.0010\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/400\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.26246 to 0.26048, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::01:44:34.684]\n",
      "Saving backup of the training history epoch=14 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::01:44:34.743]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::01:44:34.764]\n",
      "train: argmin=14 --> min=0.0279\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::01:44:34.766]\n",
      "val: argmin=14 --> min=0.26\n",
      "\n",
      "10/10 - 1909s - loss: 0.0279 - val_loss: 0.2605 - lr: 0.0010\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 16/400\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.26048 to 0.25793, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::02:16:26.459]\n",
      "Saving backup of the training history epoch=15 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::02:16:26.522]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::02:16:26.543]\n",
      "train: argmin=15 --> min=0.0266\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::02:16:26.544]\n",
      "val: argmin=15 --> min=0.258\n",
      "\n",
      "10/10 - 1912s - loss: 0.0266 - val_loss: 0.2579 - lr: 0.0010\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 17/400\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.25793\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::02:48:12.493]\n",
      "Saving backup of the training history epoch=16 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::02:48:12.550]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::02:48:12.571]\n",
      "train: argmin=15 --> min=0.0266\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::02:48:12.573]\n",
      "val: argmin=15 --> min=0.258\n",
      "\n",
      "10/10 - 1906s - loss: 0.0269 - val_loss: 0.2591 - lr: 0.0010\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 18/400\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.25793\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::03:19:58.018]\n",
      "Saving backup of the training history epoch=17 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::03:19:58.075]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::03:19:58.096]\n",
      "train: argmin=17 --> min=0.0238\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::03:19:58.098]\n",
      "val: argmin=15 --> min=0.258\n",
      "\n",
      "10/10 - 1905s - loss: 0.0238 - val_loss: 0.2627 - lr: 0.0010\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 19/400\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.25793\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::03:51:45.367]\n",
      "Saving backup of the training history epoch=18 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::03:51:45.412]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::03:51:45.433]\n",
      "train: argmin=18 --> min=0.0237\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::03:51:45.435]\n",
      "val: argmin=15 --> min=0.258\n",
      "\n",
      "10/10 - 1907s - loss: 0.0237 - val_loss: 0.2588 - lr: 0.0010\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 20/400\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.25793 to 0.25397, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::04:23:37.252]\n",
      "Saving backup of the training history epoch=19 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::04:23:37.304]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::04:23:37.325]\n",
      "train: argmin=19 --> min=0.0205\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::04:23:37.326]\n",
      "val: argmin=19 --> min=0.254\n",
      "\n",
      "10/10 - 1912s - loss: 0.0205 - val_loss: 0.2540 - lr: 0.0010\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 21/400\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.25397 to 0.24067, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::04:55:29.294]\n",
      "Saving backup of the training history epoch=20 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::04:55:29.510]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::04:55:29.530]\n",
      "train: argmin=19 --> min=0.0205\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::04:55:29.532]\n",
      "val: argmin=20 --> min=0.241\n",
      "\n",
      "10/10 - 1912s - loss: 0.0240 - val_loss: 0.2407 - lr: 0.0010\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 22/400\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.24067 to 0.23275, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::05:27:21.643]\n",
      "Saving backup of the training history epoch=21 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::05:27:21.698]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::05:27:21.719]\n",
      "train: argmin=19 --> min=0.0205\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::05:27:21.720]\n",
      "val: argmin=21 --> min=0.233\n",
      "\n",
      "10/10 - 1912s - loss: 0.0223 - val_loss: 0.2328 - lr: 0.0010\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 23/400\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.23275 to 0.21766, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::05:59:15.709]\n",
      "Saving backup of the training history epoch=22 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::05:59:15.767]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::05:59:15.788]\n",
      "train: argmin=19 --> min=0.0205\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::05:59:15.789]\n",
      "val: argmin=22 --> min=0.218\n",
      "\n",
      "10/10 - 1914s - loss: 0.0227 - val_loss: 0.2177 - lr: 0.0010\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 24/400\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.21766 to 0.19829, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::06:31:07.934]\n",
      "Saving backup of the training history epoch=23 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::06:31:07.993]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::06:31:08.013]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::06:31:08.015]\n",
      "val: argmin=23 --> min=0.198\n",
      "\n",
      "10/10 - 1912s - loss: 0.0197 - val_loss: 0.1983 - lr: 0.0010\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 25/400\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.19829 to 0.18209, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::07:03:00.763]\n",
      "Saving backup of the training history epoch=24 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::07:03:00.940]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::07:03:00.961]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::07:03:00.963]\n",
      "val: argmin=24 --> min=0.182\n",
      "\n",
      "10/10 - 1913s - loss: 0.0216 - val_loss: 0.1821 - lr: 0.0010\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 26/400\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.18209 to 0.16198, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::07:34:53.249]\n",
      "Saving backup of the training history epoch=25 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::07:34:53.309]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::07:34:53.330]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::07:34:53.332]\n",
      "val: argmin=25 --> min=0.162\n",
      "\n",
      "10/10 - 1912s - loss: 0.0214 - val_loss: 0.1620 - lr: 0.0010\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 27/400\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.16198 to 0.14707, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::08:06:46.942]\n",
      "Saving backup of the training history epoch=26 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::08:06:50.529]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::08:06:50.550]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::08:06:50.552]\n",
      "val: argmin=26 --> min=0.147\n",
      "\n",
      "10/10 - 1917s - loss: 0.0204 - val_loss: 0.1471 - lr: 0.0010\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 28/400\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.14707\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::08:38:38.798]\n",
      "Saving backup of the training history epoch=27 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::08:38:38.851]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::08:38:38.871]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::08:38:38.873]\n",
      "val: argmin=26 --> min=0.147\n",
      "\n",
      "10/10 - 1908s - loss: 0.0222 - val_loss: 0.1551 - lr: 0.0010\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 29/400\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.14707\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::09:10:27.358]\n",
      "Saving backup of the training history epoch=28 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::09:10:27.425]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::09:10:27.445]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::09:10:27.447]\n",
      "val: argmin=26 --> min=0.147\n",
      "\n",
      "10/10 - 1908s - loss: 0.0201 - val_loss: 0.1480 - lr: 0.0010\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 30/400\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.14707\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::09:42:16.377]\n",
      "Saving backup of the training history epoch=29 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::09:42:16.437]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::09:42:16.458]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::09:42:16.460]\n",
      "val: argmin=26 --> min=0.147\n",
      "\n",
      "10/10 - 1909s - loss: 0.0201 - val_loss: 0.1538 - lr: 0.0010\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 31/400\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.14707 to 0.12164, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::10:14:17.952]\n",
      "Saving backup of the training history epoch=30 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::10:14:18.016]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::10:14:18.037]\n",
      "train: argmin=23 --> min=0.0197\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::10:14:18.038]\n",
      "val: argmin=30 --> min=0.122\n",
      "\n",
      "10/10 - 1921s - loss: 0.0201 - val_loss: 0.1216 - lr: 0.0010\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000774263682681127.\n",
      "Epoch 32/400\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.12164\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::10:46:08.111]\n",
      "Saving backup of the training history epoch=31 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::10:46:08.175]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::10:46:08.196]\n",
      "train: argmin=31 --> min=0.0195\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::10:46:08.198]\n",
      "val: argmin=30 --> min=0.122\n",
      "\n",
      "10/10 - 1910s - loss: 0.0195 - val_loss: 0.1329 - lr: 7.7426e-04\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005994842503189409.\n",
      "Epoch 33/400\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.12164 to 0.11523, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::11:18:02.346]\n",
      "Saving backup of the training history epoch=32 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::11:18:02.417]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::11:18:02.438]\n",
      "train: argmin=31 --> min=0.0195\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::11:18:02.440]\n",
      "val: argmin=32 --> min=0.115\n",
      "\n",
      "10/10 - 1914s - loss: 0.0208 - val_loss: 0.1152 - lr: 5.9948e-04\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00046415888336127773.\n",
      "Epoch 34/400\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11523\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::11:49:53.308]\n",
      "Saving backup of the training history epoch=33 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::11:49:53.371]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::11:49:53.393]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::11:49:53.394]\n",
      "val: argmin=32 --> min=0.115\n",
      "\n",
      "10/10 - 1911s - loss: 0.0172 - val_loss: 0.1219 - lr: 4.6416e-04\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00035938136638046257.\n",
      "Epoch 35/400\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11523\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::12:21:45.266]\n",
      "Saving backup of the training history epoch=34 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::12:21:45.339]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::12:21:45.360]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::12:21:45.361]\n",
      "val: argmin=32 --> min=0.115\n",
      "\n",
      "10/10 - 1912s - loss: 0.0178 - val_loss: 0.1216 - lr: 3.5938e-04\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0002782559402207126.\n",
      "Epoch 36/400\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.11523 to 0.10642, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::12:53:41.479]\n",
      "Saving backup of the training history epoch=35 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::12:53:41.541]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::12:53:41.562]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::12:53:41.563]\n",
      "val: argmin=35 --> min=0.106\n",
      "\n",
      "10/10 - 1916s - loss: 0.0241 - val_loss: 0.1064 - lr: 2.7826e-04\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.00021544346900318845.\n",
      "Epoch 37/400\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.10642\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-12-10::13:25:33.665]\n",
      "Saving backup of the training history epoch=36 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-10::13:25:33.717]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::13:25:33.738]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::13:25:33.739]\n",
      "val: argmin=35 --> min=0.106\n",
      "\n",
      "10/10 - 1912s - loss: 0.0191 - val_loss: 0.1076 - lr: 2.1544e-04\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001668100537200059.\n",
      "Epoch 38/400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5be902b67630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;31m# data sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrop_seq_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1250x1000 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 400\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # epochs\n",
    "        initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "#         initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "#         epochs=history_cb.last_epoch + 1 + n_epochs,  \n",
    "    #     initial_epoch=113,\n",
    "    #     epochs=126,\n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,  \n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,   \n",
    "    );\n",
    "\n",
    "except Exception as ex:\n",
    "    slack.notify_error()\n",
    "    raise ex\n",
    "    \n",
    "else:\n",
    "    slack.notify_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ! i changed the validation generator because it was to slow\n",
    "\n",
    "continuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0001668100537200059.\n",
      "Epoch 38/400\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.10642 to 0.08438, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:02:42.969]\n",
      "Saving backup of the training history epoch=37 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:02:43.063]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:02:43.065]\n",
      "val: argmin=37 --> min=0.0844\n",
      "\n",
      "10/10 - 571s - loss: 0.0224 - val_loss: 0.0844 - lr: 1.6681e-04\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0001291549665014884.\n",
      "Epoch 39/400\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.08438 to 0.07521, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:12:15.068]\n",
      "Saving backup of the training history epoch=38 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:12:15.160]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:12:15.162]\n",
      "val: argmin=38 --> min=0.0752\n",
      "\n",
      "10/10 - 572s - loss: 0.0202 - val_loss: 0.0752 - lr: 1.2915e-04\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 40/400\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.07521 to 0.07308, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:21:47.905]\n",
      "Saving backup of the training history epoch=39 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:21:47.977]\n",
      "train: argmin=33 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:21:47.979]\n",
      "val: argmin=39 --> min=0.0731\n",
      "\n",
      "10/10 - 573s - loss: 0.0212 - val_loss: 0.0731 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 41/400\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07308\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:31:15.631]\n",
      "Saving backup of the training history epoch=40 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:31:15.714]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:31:15.716]\n",
      "val: argmin=39 --> min=0.0731\n",
      "\n",
      "10/10 - 568s - loss: 0.0172 - val_loss: 0.0746 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00011288378916846895.\n",
      "Epoch 42/400\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.07308 to 0.06998, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:40:48.082]\n",
      "Saving backup of the training history epoch=41 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:40:48.161]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:40:48.162]\n",
      "val: argmin=41 --> min=0.07\n",
      "\n",
      "10/10 - 572s - loss: 0.0180 - val_loss: 0.0700 - lr: 1.1288e-04\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00012742749857031334.\n",
      "Epoch 43/400\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.06998 to 0.06366, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:50:21.173]\n",
      "Saving backup of the training history epoch=42 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:50:21.246]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:50:21.248]\n",
      "val: argmin=42 --> min=0.0637\n",
      "\n",
      "10/10 - 573s - loss: 0.0178 - val_loss: 0.0637 - lr: 1.2743e-04\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0001438449888287663.\n",
      "Epoch 44/400\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.06366 to 0.05718, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::14:59:53.381]\n",
      "Saving backup of the training history epoch=43 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:59:53.441]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::14:59:53.443]\n",
      "val: argmin=43 --> min=0.0572\n",
      "\n",
      "10/10 - 572s - loss: 0.0188 - val_loss: 0.0572 - lr: 1.4384e-04\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0001623776739188721.\n",
      "Epoch 45/400\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.05718\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:09:21.263]\n",
      "Saving backup of the training history epoch=44 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:09:21.340]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:09:21.342]\n",
      "val: argmin=43 --> min=0.0572\n",
      "\n",
      "10/10 - 568s - loss: 0.0192 - val_loss: 0.0602 - lr: 1.6238e-04\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00018329807108324357.\n",
      "Epoch 46/400\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.05718 to 0.05323, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:18:53.304]\n",
      "Saving backup of the training history epoch=45 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:18:53.400]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:18:53.402]\n",
      "val: argmin=45 --> min=0.0532\n",
      "\n",
      "10/10 - 572s - loss: 0.0195 - val_loss: 0.0532 - lr: 1.8330e-04\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00020691380811147902.\n",
      "Epoch 47/400\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.05323 to 0.04957, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:28:25.443]\n",
      "Saving backup of the training history epoch=46 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:28:25.523]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:28:25.524]\n",
      "val: argmin=46 --> min=0.0496\n",
      "\n",
      "10/10 - 572s - loss: 0.0178 - val_loss: 0.0496 - lr: 2.0691e-04\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.00023357214690901214.\n",
      "Epoch 48/400\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.04957\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:37:53.308]\n",
      "Saving backup of the training history epoch=47 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:37:53.396]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:37:53.398]\n",
      "val: argmin=46 --> min=0.0496\n",
      "\n",
      "10/10 - 568s - loss: 0.0178 - val_loss: 0.0519 - lr: 2.3357e-04\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.00026366508987303583.\n",
      "Epoch 49/400\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.04957 to 0.04104, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:47:25.298]\n",
      "Saving backup of the training history epoch=48 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:47:25.390]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:47:25.391]\n",
      "val: argmin=48 --> min=0.041\n",
      "\n",
      "10/10 - 572s - loss: 0.0194 - val_loss: 0.0410 - lr: 2.6367e-04\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.00029763514416313193.\n",
      "Epoch 50/400\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.04104\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::15:56:54.255]\n",
      "Saving backup of the training history epoch=49 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:56:54.367]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::15:56:54.369]\n",
      "val: argmin=48 --> min=0.041\n",
      "\n",
      "10/10 - 569s - loss: 0.0185 - val_loss: 0.0496 - lr: 2.9764e-04\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0003359818286283781.\n",
      "Epoch 51/400\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.04104 to 0.03996, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:06:27.411]\n",
      "Saving backup of the training history epoch=50 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:06:27.486]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:06:27.487]\n",
      "val: argmin=50 --> min=0.04\n",
      "\n",
      "10/10 - 573s - loss: 0.0197 - val_loss: 0.0400 - lr: 3.3598e-04\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.000379269019073225.\n",
      "Epoch 52/400\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.03996\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:15:56.142]\n",
      "Saving backup of the training history epoch=51 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:15:56.229]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:15:56.230]\n",
      "val: argmin=50 --> min=0.04\n",
      "\n",
      "10/10 - 569s - loss: 0.0201 - val_loss: 0.0481 - lr: 3.7927e-04\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.00042813323987193956.\n",
      "Epoch 53/400\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03996 to 0.03344, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:25:28.672]\n",
      "Saving backup of the training history epoch=52 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:25:28.764]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:25:28.765]\n",
      "val: argmin=52 --> min=0.0334\n",
      "\n",
      "10/10 - 572s - loss: 0.0184 - val_loss: 0.0334 - lr: 4.2813e-04\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0004832930238571752.\n",
      "Epoch 54/400\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03344\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:34:56.821]\n",
      "Saving backup of the training history epoch=53 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:34:56.905]\n",
      "train: argmin=40 --> min=0.0172\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:34:56.907]\n",
      "val: argmin=52 --> min=0.0334\n",
      "\n",
      "10/10 - 568s - loss: 0.0178 - val_loss: 0.0343 - lr: 4.8329e-04\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.000545559478116852.\n",
      "Epoch 55/400\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.03344 to 0.03257, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:44:29.130]\n",
      "Saving backup of the training history epoch=54 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:44:29.213]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:44:29.215]\n",
      "val: argmin=54 --> min=0.0326\n",
      "\n",
      "10/10 - 572s - loss: 0.0167 - val_loss: 0.0326 - lr: 5.4556e-04\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0006158482110660267.\n",
      "Epoch 56/400\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.03257 to 0.02749, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::16:54:01.795]\n",
      "Saving backup of the training history epoch=55 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:54:01.878]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::16:54:01.880]\n",
      "val: argmin=55 --> min=0.0275\n",
      "\n",
      "10/10 - 573s - loss: 0.0185 - val_loss: 0.0275 - lr: 6.1585e-04\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0006951927961775605.\n",
      "Epoch 57/400\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.02749\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:03:31.131]\n",
      "Saving backup of the training history epoch=56 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:03:31.221]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:03:31.223]\n",
      "val: argmin=55 --> min=0.0275\n",
      "\n",
      "10/10 - 569s - loss: 0.0215 - val_loss: 0.0328 - lr: 6.9519e-04\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0007847599703514606.\n",
      "Epoch 58/400\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.02749 to 0.02742, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:13:03.757]\n",
      "Saving backup of the training history epoch=57 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:13:03.839]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:13:03.840]\n",
      "val: argmin=57 --> min=0.0274\n",
      "\n",
      "10/10 - 573s - loss: 0.0191 - val_loss: 0.0274 - lr: 7.8476e-04\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0008858667904100823.\n",
      "Epoch 59/400\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.02742\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:22:32.404]\n",
      "Saving backup of the training history epoch=58 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:22:32.479]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:22:32.481]\n",
      "val: argmin=57 --> min=0.0274\n",
      "\n",
      "10/10 - 568s - loss: 0.0180 - val_loss: 0.0356 - lr: 8.8587e-04\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 60/400\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.02742 to 0.02562, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:32:05.178]\n",
      "Saving backup of the training history epoch=59 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:32:05.253]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:32:05.255]\n",
      "val: argmin=59 --> min=0.0256\n",
      "\n",
      "10/10 - 573s - loss: 0.0177 - val_loss: 0.0256 - lr: 0.0010\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 61/400\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.02562 to 0.02207, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:41:37.961]\n",
      "Saving backup of the training history epoch=60 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:41:38.051]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:41:38.053]\n",
      "val: argmin=60 --> min=0.0221\n",
      "\n",
      "10/10 - 573s - loss: 0.0179 - val_loss: 0.0221 - lr: 0.0010\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 62/400\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.02207\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::17:51:07.417]\n",
      "Saving backup of the training history epoch=61 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:51:07.493]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::17:51:07.495]\n",
      "val: argmin=60 --> min=0.0221\n",
      "\n",
      "10/10 - 569s - loss: 0.0221 - val_loss: 0.0231 - lr: 0.0010\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 63/400\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.02207\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:00:36.022]\n",
      "Saving backup of the training history epoch=62 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:00:36.111]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:00:36.113]\n",
      "val: argmin=60 --> min=0.0221\n",
      "\n",
      "10/10 - 569s - loss: 0.0204 - val_loss: 0.0224 - lr: 0.0010\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 64/400\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.02207 to 0.02206, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:10:10.304]\n",
      "Saving backup of the training history epoch=63 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:10:10.378]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:10:10.380]\n",
      "val: argmin=63 --> min=0.0221\n",
      "\n",
      "10/10 - 574s - loss: 0.0184 - val_loss: 0.0221 - lr: 0.0010\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 65/400\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.02206\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:19:39.050]\n",
      "Saving backup of the training history epoch=64 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:19:39.138]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:19:39.140]\n",
      "val: argmin=63 --> min=0.0221\n",
      "\n",
      "10/10 - 569s - loss: 0.0203 - val_loss: 0.0233 - lr: 0.0010\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 66/400\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.02206 to 0.01923, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:29:11.784]\n",
      "Saving backup of the training history epoch=65 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:29:12.023]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:29:12.025]\n",
      "val: argmin=65 --> min=0.0192\n",
      "\n",
      "10/10 - 573s - loss: 0.0205 - val_loss: 0.0192 - lr: 0.0010\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 67/400\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.01923\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:38:40.622]\n",
      "Saving backup of the training history epoch=66 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:38:40.688]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:38:40.691]\n",
      "val: argmin=65 --> min=0.0192\n",
      "\n",
      "10/10 - 568s - loss: 0.0207 - val_loss: 0.0195 - lr: 0.0010\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 68/400\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.01923 to 0.01921, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:48:13.525]\n",
      "Saving backup of the training history epoch=67 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:48:13.799]\n",
      "train: argmin=54 --> min=0.0167\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:48:13.801]\n",
      "val: argmin=67 --> min=0.0192\n",
      "\n",
      "10/10 - 573s - loss: 0.0198 - val_loss: 0.0192 - lr: 0.0010\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 69/400\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.01921 to 0.01783, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::18:57:47.947]\n",
      "Saving backup of the training history epoch=68 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:57:48.028]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::18:57:48.030]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 574s - loss: 0.0166 - val_loss: 0.0178 - lr: 0.0010\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 70/400\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:07:16.788]\n",
      "Saving backup of the training history epoch=69 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:07:16.854]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:07:16.856]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0173 - val_loss: 0.0188 - lr: 0.0010\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 71/400\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:16:45.596]\n",
      "Saving backup of the training history epoch=70 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:16:45.682]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:16:45.683]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0173 - val_loss: 0.0180 - lr: 0.0010\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 72/400\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:26:14.424]\n",
      "Saving backup of the training history epoch=71 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:26:14.511]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:26:14.513]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0177 - val_loss: 0.0194 - lr: 0.0010\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 73/400\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:35:43.083]\n",
      "Saving backup of the training history epoch=72 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:35:43.160]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:35:43.162]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0190 - val_loss: 0.0186 - lr: 0.0010\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 74/400\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:45:12.240]\n",
      "Saving backup of the training history epoch=73 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:45:12.313]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:45:12.315]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0189 - val_loss: 0.0225 - lr: 0.0010\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 75/400\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::19:54:41.673]\n",
      "Saving backup of the training history epoch=74 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:54:41.762]\n",
      "train: argmin=68 --> min=0.0166\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::19:54:41.764]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 571s - loss: 0.0174 - val_loss: 0.0186 - lr: 0.0010\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 76/400\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:04:12.425]\n",
      "Saving backup of the training history epoch=75 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:04:12.865]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:04:12.867]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 570s - loss: 0.0158 - val_loss: 0.0184 - lr: 0.0010\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 77/400\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:13:42.671]\n",
      "Saving backup of the training history epoch=76 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:13:42.912]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:13:42.914]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 570s - loss: 0.0171 - val_loss: 0.0182 - lr: 0.0010\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 78/400\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:23:11.812]\n",
      "Saving backup of the training history epoch=77 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:23:11.885]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:23:11.887]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0179 - val_loss: 0.0196 - lr: 0.0010\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 79/400\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:32:41.256]\n",
      "Saving backup of the training history epoch=78 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:32:41.346]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:32:41.349]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0233 - val_loss: 0.0179 - lr: 0.0010\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 80/400\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:42:11.411]\n",
      "Saving backup of the training history epoch=79 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:42:11.500]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:42:11.502]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 570s - loss: 0.0186 - val_loss: 0.0182 - lr: 0.0010\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 81/400\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.01783\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::20:51:40.318]\n",
      "Saving backup of the training history epoch=80 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:51:40.395]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::20:51:40.397]\n",
      "val: argmin=68 --> min=0.0178\n",
      "\n",
      "10/10 - 569s - loss: 0.0191 - val_loss: 0.0193 - lr: 0.0010\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0008858667904100823.\n",
      "Epoch 82/400\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.01783 to 0.01722, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:01:13.972]\n",
      "Saving backup of the training history epoch=81 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:01:14.247]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:01:14.249]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 574s - loss: 0.0181 - val_loss: 0.0172 - lr: 8.8587e-04\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0007847599703514615.\n",
      "Epoch 83/400\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:10:44.338]\n",
      "Saving backup of the training history epoch=82 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:10:44.428]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:10:44.430]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 570s - loss: 0.0158 - val_loss: 0.0176 - lr: 7.8476e-04\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0006951927961775605.\n",
      "Epoch 84/400\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:20:13.403]\n",
      "Saving backup of the training history epoch=83 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:20:13.588]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:20:13.589]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 569s - loss: 0.0182 - val_loss: 0.0182 - lr: 6.9519e-04\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0006158482110660267.\n",
      "Epoch 85/400\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:29:42.450]\n",
      "Saving backup of the training history epoch=84 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:29:42.535]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:29:42.537]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 569s - loss: 0.0187 - val_loss: 0.0184 - lr: 6.1585e-04\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.000545559478116852.\n",
      "Epoch 86/400\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:39:11.348]\n",
      "Saving backup of the training history epoch=85 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:39:11.570]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:39:11.571]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 569s - loss: 0.0167 - val_loss: 0.0177 - lr: 5.4556e-04\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0004832930238571752.\n",
      "Epoch 87/400\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:48:41.064]\n",
      "Saving backup of the training history epoch=86 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:48:41.154]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:48:41.155]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 569s - loss: 0.0163 - val_loss: 0.0174 - lr: 4.8329e-04\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.00042813323987193956.\n",
      "Epoch 88/400\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::21:58:11.374]\n",
      "Saving backup of the training history epoch=87 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:58:11.461]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::21:58:11.463]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 570s - loss: 0.0167 - val_loss: 0.0175 - lr: 4.2813e-04\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.000379269019073225.\n",
      "Epoch 89/400\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:07:41.075]\n",
      "Saving backup of the training history epoch=88 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:07:41.165]\n",
      "train: argmin=75 --> min=0.0158\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:07:41.167]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 570s - loss: 0.0168 - val_loss: 0.0174 - lr: 3.7927e-04\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0003359818286283781.\n",
      "Epoch 90/400\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.01722\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:17:11.717]\n",
      "Saving backup of the training history epoch=89 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:17:11.806]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:17:11.808]\n",
      "val: argmin=81 --> min=0.0172\n",
      "\n",
      "10/10 - 571s - loss: 0.0138 - val_loss: 0.0173 - lr: 3.3598e-04\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.00029763514416313193.\n",
      "Epoch 91/400\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.01722 to 0.01700, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:26:45.946]\n",
      "Saving backup of the training history epoch=90 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:26:46.035]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:26:46.037]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 574s - loss: 0.0171 - val_loss: 0.0170 - lr: 2.9764e-04\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.00026366508987303583.\n",
      "Epoch 92/400\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:36:18.095]\n",
      "Saving backup of the training history epoch=91 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:36:18.187]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:36:18.189]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 572s - loss: 0.0163 - val_loss: 0.0170 - lr: 2.6367e-04\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.00023357214690901214.\n",
      "Epoch 93/400\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:45:47.956]\n",
      "Saving backup of the training history epoch=92 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:45:48.047]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:45:48.049]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0160 - val_loss: 0.0172 - lr: 2.3357e-04\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.00020691380811147902.\n",
      "Epoch 94/400\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::22:55:17.786]\n",
      "Saving backup of the training history epoch=93 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:55:17.872]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::22:55:17.874]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0182 - val_loss: 0.0173 - lr: 2.0691e-04\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.00018329807108324357.\n",
      "Epoch 95/400\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:04:47.414]\n",
      "Saving backup of the training history epoch=94 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:04:47.506]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:04:47.508]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0172 - val_loss: 0.0175 - lr: 1.8330e-04\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0001623776739188721.\n",
      "Epoch 96/400\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:14:17.477]\n",
      "Saving backup of the training history epoch=95 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:14:17.550]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:14:17.551]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0168 - val_loss: 0.0175 - lr: 1.6238e-04\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0001438449888287663.\n",
      "Epoch 97/400\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:23:47.872]\n",
      "Saving backup of the training history epoch=96 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:23:47.959]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:23:47.961]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0161 - val_loss: 0.0175 - lr: 1.4384e-04\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.00012742749857031348.\n",
      "Epoch 98/400\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.01700\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:33:17.621]\n",
      "Saving backup of the training history epoch=97 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:33:17.705]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:33:17.707]\n",
      "val: argmin=90 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0164 - val_loss: 0.0171 - lr: 1.2743e-04\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.00011288378916846895.\n",
      "Epoch 99/400\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.01700 to 0.01699, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:42:53.344]\n",
      "Saving backup of the training history epoch=98 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:42:53.425]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:42:53.427]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 576s - loss: 0.0164 - val_loss: 0.0170 - lr: 1.1288e-04\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 100/400\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-10::23:52:22.604]\n",
      "Saving backup of the training history epoch=99 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:52:22.692]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-10::23:52:22.694]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 569s - loss: 0.0153 - val_loss: 0.0172 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 101/400\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:01:52.148]\n",
      "Saving backup of the training history epoch=100 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:01:52.236]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:01:52.238]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 569s - loss: 0.0190 - val_loss: 0.0172 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.00011288378916846895.\n",
      "Epoch 102/400\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:11:22.200]\n",
      "Saving backup of the training history epoch=101 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:11:22.283]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:11:22.285]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0177 - val_loss: 0.0170 - lr: 1.1288e-04\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.00012742749857031334.\n",
      "Epoch 103/400\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:20:52.199]\n",
      "Saving backup of the training history epoch=102 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:20:52.395]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:20:52.397]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0157 - val_loss: 0.0171 - lr: 1.2743e-04\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.0001438449888287663.\n",
      "Epoch 104/400\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:30:22.546]\n",
      "Saving backup of the training history epoch=103 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:30:22.618]\n",
      "train: argmin=89 --> min=0.0138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:30:22.620]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0173 - val_loss: 0.0172 - lr: 1.4384e-04\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.0001623776739188721.\n",
      "Epoch 105/400\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:39:52.654]\n",
      "Saving backup of the training history epoch=104 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:39:52.739]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:39:52.740]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0136 - val_loss: 0.0174 - lr: 1.6238e-04\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.00018329807108324357.\n",
      "Epoch 106/400\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:49:22.382]\n",
      "Saving backup of the training history epoch=105 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:49:22.467]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:49:22.469]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0176 - val_loss: 0.0171 - lr: 1.8330e-04\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.00020691380811147902.\n",
      "Epoch 107/400\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::00:58:52.447]\n",
      "Saving backup of the training history epoch=106 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:58:52.532]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::00:58:52.534]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0165 - val_loss: 0.0171 - lr: 2.0691e-04\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.00023357214690901214.\n",
      "Epoch 108/400\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:08:22.462]\n",
      "Saving backup of the training history epoch=107 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:08:22.544]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:08:22.546]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0162 - val_loss: 0.0170 - lr: 2.3357e-04\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.00026366508987303583.\n",
      "Epoch 109/400\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:17:51.908]\n",
      "Saving backup of the training history epoch=108 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:17:52.004]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:17:52.006]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 569s - loss: 0.0158 - val_loss: 0.0176 - lr: 2.6367e-04\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.00029763514416313193.\n",
      "Epoch 110/400\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.01699\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:27:22.043]\n",
      "Saving backup of the training history epoch=109 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:27:22.126]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:27:22.128]\n",
      "val: argmin=98 --> min=0.017\n",
      "\n",
      "10/10 - 570s - loss: 0.0184 - val_loss: 0.0178 - lr: 2.9764e-04\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.0003359818286283781.\n",
      "Epoch 111/400\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.01699 to 0.01684, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:36:56.562]\n",
      "Saving backup of the training history epoch=110 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:36:56.642]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:36:56.644]\n",
      "val: argmin=110 --> min=0.0168\n",
      "\n",
      "10/10 - 574s - loss: 0.0162 - val_loss: 0.0168 - lr: 3.3598e-04\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.000379269019073225.\n",
      "Epoch 112/400\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.01684 to 0.01666, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:46:31.154]\n",
      "Saving backup of the training history epoch=111 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:46:31.408]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:46:31.410]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 575s - loss: 0.0180 - val_loss: 0.0167 - lr: 3.7927e-04\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.00042813323987193956.\n",
      "Epoch 113/400\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::01:56:01.902]\n",
      "Saving backup of the training history epoch=112 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:56:01.994]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::01:56:01.996]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0174 - val_loss: 0.0170 - lr: 4.2813e-04\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.0004832930238571752.\n",
      "Epoch 114/400\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:05:32.307]\n",
      "Saving backup of the training history epoch=113 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:05:32.381]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:05:32.382]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0172 - val_loss: 0.0176 - lr: 4.8329e-04\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.000545559478116852.\n",
      "Epoch 115/400\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:15:01.698]\n",
      "Saving backup of the training history epoch=114 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:15:01.787]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:15:01.789]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0163 - val_loss: 0.0175 - lr: 5.4556e-04\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.0006158482110660267.\n",
      "Epoch 116/400\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:24:31.077]\n",
      "Saving backup of the training history epoch=115 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:24:31.210]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:24:31.212]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0191 - val_loss: 0.0179 - lr: 6.1585e-04\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.0006951927961775605.\n",
      "Epoch 117/400\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:34:01.219]\n",
      "Saving backup of the training history epoch=116 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:34:01.297]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:34:01.299]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0171 - val_loss: 0.0171 - lr: 6.9519e-04\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.0007847599703514606.\n",
      "Epoch 118/400\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:43:30.744]\n",
      "Saving backup of the training history epoch=117 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:43:30.833]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:43:30.835]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0184 - val_loss: 0.0169 - lr: 7.8476e-04\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.0008858667904100823.\n",
      "Epoch 119/400\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::02:53:00.328]\n",
      "Saving backup of the training history epoch=118 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:53:00.423]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::02:53:00.425]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0193 - val_loss: 0.0173 - lr: 8.8587e-04\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 120/400\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:02:29.783]\n",
      "Saving backup of the training history epoch=119 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:02:29.865]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:02:29.867]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0185 - val_loss: 0.0181 - lr: 0.0010\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 121/400\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:11:59.856]\n",
      "Saving backup of the training history epoch=120 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:11:59.937]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:11:59.939]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0171 - val_loss: 0.0185 - lr: 0.0010\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 122/400\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:21:29.714]\n",
      "Saving backup of the training history epoch=121 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:21:29.788]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:21:29.790]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0162 - val_loss: 0.0183 - lr: 0.0010\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 123/400\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:30:59.166]\n",
      "Saving backup of the training history epoch=122 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:30:59.256]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:30:59.258]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0159 - val_loss: 0.0193 - lr: 0.0010\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 124/400\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:40:30.571]\n",
      "Saving backup of the training history epoch=123 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:40:30.649]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:40:30.650]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 571s - loss: 0.0167 - val_loss: 0.0186 - lr: 0.0010\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 125/400\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:50:00.612]\n",
      "Saving backup of the training history epoch=124 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:50:00.695]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:50:00.697]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0146 - val_loss: 0.0172 - lr: 0.0010\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 126/400\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::03:59:30.098]\n",
      "Saving backup of the training history epoch=125 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:59:30.184]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::03:59:30.186]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0179 - val_loss: 0.0171 - lr: 0.0010\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 127/400\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:08:59.527]\n",
      "Saving backup of the training history epoch=126 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:08:59.603]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:08:59.605]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0195 - val_loss: 0.0167 - lr: 0.0010\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 128/400\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:18:28.867]\n",
      "Saving backup of the training history epoch=127 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:18:28.958]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:18:28.959]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0174 - val_loss: 0.0176 - lr: 0.0010\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 129/400\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:27:58.570]\n",
      "Saving backup of the training history epoch=128 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:27:58.660]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:27:58.661]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0163 - val_loss: 0.0177 - lr: 0.0010\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 130/400\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:37:28.143]\n",
      "Saving backup of the training history epoch=129 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:37:28.228]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:37:28.230]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0205 - val_loss: 0.0192 - lr: 0.0010\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 131/400\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:46:57.704]\n",
      "Saving backup of the training history epoch=130 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:46:57.796]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:46:57.798]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0186 - val_loss: 0.0178 - lr: 0.0010\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 132/400\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::04:56:27.422]\n",
      "Saving backup of the training history epoch=131 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:56:27.495]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::04:56:27.496]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0149 - val_loss: 0.0223 - lr: 0.0010\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 133/400\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:05:57.079]\n",
      "Saving backup of the training history epoch=132 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:05:57.162]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:05:57.164]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0154 - val_loss: 0.0187 - lr: 0.0010\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 134/400\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:15:26.533]\n",
      "Saving backup of the training history epoch=133 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:15:26.607]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:15:26.609]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0171 - val_loss: 0.0182 - lr: 0.0010\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 135/400\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:24:55.974]\n",
      "Saving backup of the training history epoch=134 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:24:56.053]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:24:56.055]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0143 - val_loss: 0.0181 - lr: 0.0010\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 136/400\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:34:25.622]\n",
      "Saving backup of the training history epoch=135 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:34:25.702]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:34:25.704]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0151 - val_loss: 0.0179 - lr: 0.0010\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 137/400\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:43:55.364]\n",
      "Saving backup of the training history epoch=136 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:43:55.447]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:43:55.449]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0170 - val_loss: 0.0177 - lr: 0.0010\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 138/400\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::05:53:24.829]\n",
      "Saving backup of the training history epoch=137 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:53:24.902]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::05:53:24.904]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0171 - val_loss: 0.0179 - lr: 0.0010\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 139/400\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:02:55.824]\n",
      "Saving backup of the training history epoch=138 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:02:55.928]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:02:55.929]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 571s - loss: 0.0178 - val_loss: 0.0245 - lr: 0.0010\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 140/400\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:12:25.497]\n",
      "Saving backup of the training history epoch=139 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:12:25.577]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:12:25.579]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0175 - val_loss: 0.0180 - lr: 0.0010\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 141/400\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:21:55.339]\n",
      "Saving backup of the training history epoch=140 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:21:55.426]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:21:55.428]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0170 - val_loss: 0.0173 - lr: 0.0010\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.0008858667904100823.\n",
      "Epoch 142/400\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:31:23.557]\n",
      "Saving backup of the training history epoch=141 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:31:23.643]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:31:23.645]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 568s - loss: 0.0198 - val_loss: 0.0176 - lr: 8.8587e-04\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.0007847599703514615.\n",
      "Epoch 143/400\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:40:54.584]\n",
      "Saving backup of the training history epoch=142 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:40:54.655]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:40:54.657]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 571s - loss: 0.0197 - val_loss: 0.0220 - lr: 7.8476e-04\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.0006951927961775605.\n",
      "Epoch 144/400\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:50:24.287]\n",
      "Saving backup of the training history epoch=143 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:50:24.371]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:50:24.372]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0166 - val_loss: 0.0173 - lr: 6.9519e-04\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.0006158482110660267.\n",
      "Epoch 145/400\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::06:59:53.907]\n",
      "Saving backup of the training history epoch=144 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:59:53.988]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::06:59:53.990]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0191 - val_loss: 0.0178 - lr: 6.1585e-04\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.000545559478116852.\n",
      "Epoch 146/400\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:09:23.503]\n",
      "Saving backup of the training history epoch=145 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:09:23.587]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:09:23.589]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0160 - val_loss: 0.0171 - lr: 5.4556e-04\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.0004832930238571752.\n",
      "Epoch 147/400\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:18:52.959]\n",
      "Saving backup of the training history epoch=146 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:18:53.035]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:18:53.037]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0169 - val_loss: 0.0174 - lr: 4.8329e-04\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.00042813323987193956.\n",
      "Epoch 148/400\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:28:22.863]\n",
      "Saving backup of the training history epoch=147 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:28:22.951]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:28:22.952]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0158 - val_loss: 0.0173 - lr: 4.2813e-04\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.000379269019073225.\n",
      "Epoch 149/400\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:37:52.397]\n",
      "Saving backup of the training history epoch=148 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:37:52.477]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:37:52.479]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0166 - val_loss: 0.0168 - lr: 3.7927e-04\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0003359818286283781.\n",
      "Epoch 150/400\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:47:21.769]\n",
      "Saving backup of the training history epoch=149 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:47:21.843]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:47:21.845]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0163 - val_loss: 0.0173 - lr: 3.3598e-04\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.00029763514416313193.\n",
      "Epoch 151/400\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::07:56:51.333]\n",
      "Saving backup of the training history epoch=150 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:56:51.419]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::07:56:51.420]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0156 - val_loss: 0.0178 - lr: 2.9764e-04\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.00026366508987303583.\n",
      "Epoch 152/400\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:06:20.872]\n",
      "Saving backup of the training history epoch=151 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:06:20.951]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:06:20.953]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0151 - val_loss: 0.0169 - lr: 2.6367e-04\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.00023357214690901214.\n",
      "Epoch 153/400\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:15:50.380]\n",
      "Saving backup of the training history epoch=152 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:15:50.458]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:15:50.460]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0156 - val_loss: 0.0172 - lr: 2.3357e-04\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.00020691380811147902.\n",
      "Epoch 154/400\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:25:19.795]\n",
      "Saving backup of the training history epoch=153 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:25:19.885]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:25:19.887]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 569s - loss: 0.0161 - val_loss: 0.0171 - lr: 2.0691e-04\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.00018329807108324357.\n",
      "Epoch 155/400\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:34:49.458]\n",
      "Saving backup of the training history epoch=154 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:34:49.533]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:34:49.535]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0148 - val_loss: 0.0172 - lr: 1.8330e-04\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001623776739188721.\n",
      "Epoch 156/400\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:44:18.907]\n",
      "Saving backup of the training history epoch=155 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:44:19.117]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:44:19.119]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0178 - val_loss: 0.0174 - lr: 1.6238e-04\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001438449888287663.\n",
      "Epoch 157/400\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::08:53:48.533]\n",
      "Saving backup of the training history epoch=156 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:53:48.848]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::08:53:48.850]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0171 - val_loss: 0.0176 - lr: 1.4384e-04\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.00012742749857031348.\n",
      "Epoch 158/400\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:03:18.307]\n",
      "Saving backup of the training history epoch=157 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:03:18.638]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:03:18.640]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 570s - loss: 0.0155 - val_loss: 0.0170 - lr: 1.2743e-04\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.00011288378916846895.\n",
      "Epoch 159/400\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.01666\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:12:50.128]\n",
      "Saving backup of the training history epoch=158 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:12:50.230]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:12:50.231]\n",
      "val: argmin=111 --> min=0.0167\n",
      "\n",
      "10/10 - 571s - loss: 0.0148 - val_loss: 0.0168 - lr: 1.1288e-04\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 160/400\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.01666 to 0.01661, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:22:24.247]\n",
      "Saving backup of the training history epoch=159 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:22:24.329]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:22:24.331]\n",
      "val: argmin=159 --> min=0.0166\n",
      "\n",
      "10/10 - 574s - loss: 0.0145 - val_loss: 0.0166 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 161/400\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.01661 to 0.01649, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:31:58.039]\n",
      "Saving backup of the training history epoch=160 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:31:58.123]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:31:58.125]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 574s - loss: 0.0150 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 162/400\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:41:27.459]\n",
      "Saving backup of the training history epoch=161 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:41:27.547]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:41:27.548]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 569s - loss: 0.0166 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 163/400\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::09:50:57.035]\n",
      "Saving backup of the training history epoch=162 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:50:57.113]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::09:50:57.115]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 569s - loss: 0.0161 - val_loss: 0.0170 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 164/400\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:00:24.720]\n",
      "Saving backup of the training history epoch=163 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:00:24.818]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:00:24.820]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 568s - loss: 0.0154 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 165/400\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:09:57.930]\n",
      "Saving backup of the training history epoch=164 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:09:58.018]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:09:58.020]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 573s - loss: 0.0154 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 166/400\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:19:28.065]\n",
      "Saving backup of the training history epoch=165 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:19:28.145]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:19:28.147]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 570s - loss: 0.0171 - val_loss: 0.0167 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 167/400\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.01649\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:28:57.753]\n",
      "Saving backup of the training history epoch=166 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:28:57.841]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:28:57.843]\n",
      "val: argmin=160 --> min=0.0165\n",
      "\n",
      "10/10 - 570s - loss: 0.0173 - val_loss: 0.0166 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 168/400\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.01649 to 0.01646, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:38:32.115]\n",
      "Saving backup of the training history epoch=167 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:38:32.197]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:38:32.198]\n",
      "val: argmin=167 --> min=0.0165\n",
      "\n",
      "10/10 - 574s - loss: 0.0164 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 169/400\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.01646\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:47:59.920]\n",
      "Saving backup of the training history epoch=168 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:47:59.994]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:47:59.995]\n",
      "val: argmin=167 --> min=0.0165\n",
      "\n",
      "10/10 - 568s - loss: 0.0150 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 170/400\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.01646 to 0.01642, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::10:57:35.279]\n",
      "Saving backup of the training history epoch=169 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:57:35.361]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::10:57:35.363]\n",
      "val: argmin=169 --> min=0.0164\n",
      "\n",
      "10/10 - 575s - loss: 0.0149 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 171/400\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.01642\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:07:05.303]\n",
      "Saving backup of the training history epoch=170 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:07:05.375]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:07:05.377]\n",
      "val: argmin=169 --> min=0.0164\n",
      "\n",
      "10/10 - 570s - loss: 0.0157 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 172/400\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.01642\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:16:35.040]\n",
      "Saving backup of the training history epoch=171 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:16:35.115]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:16:35.117]\n",
      "val: argmin=169 --> min=0.0164\n",
      "\n",
      "10/10 - 570s - loss: 0.0166 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 173/400\n",
      "\n",
      "Epoch 00173: val_loss improved from 0.01642 to 0.01635, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:26:09.809]\n",
      "Saving backup of the training history epoch=172 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:26:09.906]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:26:09.908]\n",
      "val: argmin=172 --> min=0.0164\n",
      "\n",
      "10/10 - 575s - loss: 0.0137 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 174/400\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.01635\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:35:40.566]\n",
      "Saving backup of the training history epoch=173 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:35:40.654]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:35:40.656]\n",
      "val: argmin=172 --> min=0.0164\n",
      "\n",
      "10/10 - 571s - loss: 0.0155 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 175/400\n",
      "\n",
      "Epoch 00175: val_loss improved from 0.01635 to 0.01634, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:45:13.784]\n",
      "Saving backup of the training history epoch=174 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:45:13.876]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:45:13.878]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 573s - loss: 0.0156 - val_loss: 0.0163 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 176/400\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.01634\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::11:54:45.638]\n",
      "Saving backup of the training history epoch=175 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:54:45.746]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::11:54:45.748]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 572s - loss: 0.0182 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 177/400\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.01634\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:04:15.911]\n",
      "Saving backup of the training history epoch=176 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:04:16.001]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:04:16.003]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 570s - loss: 0.0150 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 178/400\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.01634\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:13:46.927]\n",
      "Saving backup of the training history epoch=177 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:13:47.065]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:13:47.067]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 571s - loss: 0.0160 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 179/400\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.01634\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:23:17.118]\n",
      "Saving backup of the training history epoch=178 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:23:17.213]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:23:17.214]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 570s - loss: 0.0163 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 180/400\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.01634\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:32:45.504]\n",
      "Saving backup of the training history epoch=179 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:32:45.577]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:32:45.579]\n",
      "val: argmin=174 --> min=0.0163\n",
      "\n",
      "10/10 - 568s - loss: 0.0180 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 181/400\n",
      "\n",
      "Epoch 00181: val_loss improved from 0.01634 to 0.01633, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:42:21.139]\n",
      "Saving backup of the training history epoch=180 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:42:21.222]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:42:21.223]\n",
      "val: argmin=180 --> min=0.0163\n",
      "\n",
      "10/10 - 576s - loss: 0.0158 - val_loss: 0.0163 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 182/400\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.01633\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::12:51:51.108]\n",
      "Saving backup of the training history epoch=181 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:51:51.223]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::12:51:51.224]\n",
      "val: argmin=180 --> min=0.0163\n",
      "\n",
      "10/10 - 570s - loss: 0.0172 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 183/400\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.01633\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:01:21.118]\n",
      "Saving backup of the training history epoch=182 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:01:21.189]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:01:21.191]\n",
      "val: argmin=180 --> min=0.0163\n",
      "\n",
      "10/10 - 570s - loss: 0.0178 - val_loss: 0.0169 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 184/400\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.01633\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:10:50.464]\n",
      "Saving backup of the training history epoch=183 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:10:50.547]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:10:50.549]\n",
      "val: argmin=180 --> min=0.0163\n",
      "\n",
      "10/10 - 569s - loss: 0.0165 - val_loss: 0.0171 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 185/400\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.01633\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:20:22.499]\n",
      "Saving backup of the training history epoch=184 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:20:22.614]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:20:22.616]\n",
      "val: argmin=180 --> min=0.0163\n",
      "\n",
      "10/10 - 572s - loss: 0.0171 - val_loss: 0.0164 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 186/400\n",
      "\n",
      "Epoch 00186: val_loss improved from 0.01633 to 0.01619, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:29:57.499]\n",
      "Saving backup of the training history epoch=185 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:29:57.588]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:29:57.590]\n",
      "val: argmin=185 --> min=0.0162\n",
      "\n",
      "10/10 - 575s - loss: 0.0164 - val_loss: 0.0162 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 187/400\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.01619\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:39:26.042]\n",
      "Saving backup of the training history epoch=186 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:39:26.113]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:39:26.115]\n",
      "val: argmin=185 --> min=0.0162\n",
      "\n",
      "10/10 - 568s - loss: 0.0197 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 188/400\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.01619\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:48:57.667]\n",
      "Saving backup of the training history epoch=187 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:48:57.763]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:48:57.765]\n",
      "val: argmin=185 --> min=0.0162\n",
      "\n",
      "10/10 - 572s - loss: 0.0149 - val_loss: 0.0163 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 189/400\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.01619\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::13:58:28.011]\n",
      "Saving backup of the training history epoch=188 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:58:28.088]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::13:58:28.090]\n",
      "val: argmin=185 --> min=0.0162\n",
      "\n",
      "10/10 - 570s - loss: 0.0182 - val_loss: 0.0166 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 190/400\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.01619\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-11::14:07:58.261]\n",
      "Saving backup of the training history epoch=189 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d/unet2d.crop48-f16.fold000.1607-530-580/history.csv')\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::14:07:58.345]\n",
      "train: argmin=104 --> min=0.0136\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-11::14:07:58.346]\n",
      "val: argmin=185 --> min=0.0162\n",
      "\n",
      "10/10 - 570s - loss: 0.0182 - val_loss: 0.0165 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 191/400\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 400\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # epochs\n",
    "#         initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "        initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "#         epochs=history_cb.last_epoch + 1 + n_epochs,  \n",
    "    #     initial_epoch=113,\n",
    "    #     epochs=126,\n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,  \n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,   \n",
    "    );\n",
    "\n",
    "except Exception as ex:\n",
    "    slack.notify_error()\n",
    "    raise ex\n",
    "    \n",
    "else:\n",
    "    slack.notify_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows := 2, ncols := 1, figsize=(2.5 * (sz := 5), nrows * sz), dpi=100)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "hist_display = viz.TrainingHistoryDisplay(\n",
    "    history_cb.history, \n",
    "    model_name=tomo2seg_model.name,\n",
    "    loss_name=model.loss.__name__,\n",
    "    x_axis_mode=(\n",
    "        \"epoch\", \"batch\", \"crop\", \"voxel\", \"time\",\n",
    "    ),\n",
    ").plot(\n",
    "    axs, \n",
    "    with_lr=True,\n",
    "    metrics=(\n",
    "        \"loss\", \n",
    "    ),\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[-1].set_yscale(\"log\")\n",
    "\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"loss\"][0])\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"val_loss\"][0], txt_kwargs=dict(rotation=0))\n",
    "\n",
    "hist_display.fig_.savefig(\n",
    "    tomo2seg_model.model_path / (hist_display.title + \".png\"),\n",
    "    format='png',\n",
    ")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "aborted",
     "timestamp": 1602255923919,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "d-EnhRhrrEGQ"
   },
   "outputs": [],
   "source": [
    "history_cb.dataframe.to_csv(history_cb.csv_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "aborted",
     "timestamp": 1602255923920,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "LQz6HBJss1o4"
   },
   "outputs": [],
   "source": [
    "model.save(tomo2seg_model.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_name = \"train-04-hathi91.ipynb\"\n",
    "import os\n",
    "this_dir = os.getcwd()\n",
    "logger.warning(f\"{this_nb_name=} {this_dir=}\")\n",
    "\n",
    "os.system(f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {str(tomo2seg_model.model_path)} --to html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2FW3h3DkQ4XcY6OgH7u/r",
   "collapsed_sections": [
    "EnVqPFS9BNCg",
    "j8e5FhmUaKND",
    "nJtppItnKn5G"
   ],
   "mount_file_id": "1LuEITv9j0lLf8Z418J3a94SjEZ8GvKvI",
   "name": "dryrun-02.ipynb",
   "provenance": [
    {
     "file_id": "1NiX28EcC_FVOYCJL4usp7n5iQ2x3aXIm",
     "timestamp": 1602152789440
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
