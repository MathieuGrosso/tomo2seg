{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JEHjvuBBIab"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1602255916978,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "KTMgQv07JkgY",
    "outputId": "69cd78fc-f0f1-46f6-f1d8-63b99d55eaae"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from enum import Enum\n",
    "import functools\n",
    "import operator\n",
    "from functools import partial\n",
    "import logging\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from typing import *\n",
    "import time\n",
    "import yaml\n",
    "from yaml import YAMLObject\n",
    "import socket\n",
    "\n",
    "import humanize\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymicro.file import file_utils\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks as keras_callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics as keras_metrics\n",
    "\n",
    "from tomo2seg import slack\n",
    "from tomo2seg import modular_unet\n",
    "from tomo2seg.logger import logger, dict2str, add_file_handler\n",
    "from tomo2seg import data as tomo2seg_data\n",
    "from tomo2seg import viz\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.metadata import Metadata\n",
    "from tomo2seg.volume_sequence import (\n",
    "    MetaCrop3DGenerator, VolumeCropSequence,\n",
    "    UniformGridPosition, SequentialGridPosition,\n",
    "    ET3DUniformCuboidAlmostEverywhere, ET3DConstantEverywhere, \n",
    "    GTUniformEverywhere, GTConstantEverywhere, \n",
    "    VSConstantEverywhere, VSUniformEverywhere\n",
    ")\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import callbacks as tomo2seg_callbacks\n",
    "from tomo2seg import losses as tomo2seg_losses\n",
    "from tomo2seg import schedule as tomo2seg_schedule\n",
    "from tomo2seg import utils as tomo2seg_utils\n",
    "from tomo2seg import slackme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-6-78d1635f7d9a>:<module>:014}::[2020-12-14::10:51:32.448]\n",
      "MAX_INTERNAL_NVOXELS=133632000 (133,632,000)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these are estimates based on things i've seen fit in the GPU\n",
    "MAX_INTERNAL_NVOXELS = max(\n",
    "    # seen cases\n",
    "    # batch_size * internal_multiplier_factor * (crop_nvoxels) * gpu_factor=1\n",
    "    4 * (8 * 6) * (96**3),\n",
    "    8 * (16 * 6) * (320**2),  \n",
    "    3 * (16 * 6) * (800 * 928),\n",
    "    15 * 23 * (208**2 * 5) * (8 / 5),\n",
    ")\n",
    "\n",
    "MAX_INTERNAL_NVOXELS *= 5/8  # a smaller gpu on other pcs...\n",
    "MAX_INTERNAL_NVOXELS = int(MAX_INTERNAL_NVOXELS)\n",
    "\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-7-806f1f39c55a>:<module>:091}::[2020-12-14::10:51:33.460]\n",
      "args={   'batch_size_mode': <BatchSizeMode.try_max_and_reduce: 1>,\n",
      "    'batch_size_per_gpu': None,\n",
      "    'early_stop_mode': <EarlyStopMode.no_early_stop: 0>,\n",
      "    'labels_version': 'refined3',\n",
      "    'random_state_seed': 42,\n",
      "    'runid': 1607939493,\n",
      "    'train_mode': <TrainMode.from_scratch: 0>,\n",
      "    'volume_name': 'PA66GF30',\n",
      "    'volume_version': 'v1'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [manual-input]\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "\n",
    "    class EarlyStopMode(Enum):\n",
    "        no_early_stop = 0\n",
    "        \n",
    "    class BatchSizeMode(Enum):\n",
    "        try_max_and_fail = 0\n",
    "        try_max_and_reduce = 1\n",
    "        \n",
    "    # None: continue from the latest model\n",
    "    # 1: continue from model.autosaved_model_path\n",
    "    # 2: continue from model.autosaved2_model_path\n",
    "    # continue_from_autosave: Optional[int] = None \n",
    "    class TrainMode(Enum):\n",
    "        from_scratch = 0\n",
    "        continuation_from_autosaved_model = 1\n",
    "        continuation_from_autosaved2_best_model = 2\n",
    "        continuation_from_latest_model = 3\n",
    "\n",
    "        @property\n",
    "        def is_continuation(self) -> bool:\n",
    "            return self in (\n",
    "                Args.TrainMode.continuation_from_autosaved_model,\n",
    "                Args.TrainMode.continuation_from_autosaved2_best_model,\n",
    "                Args.TrainMode.continuation_from_latest_model,\n",
    "            )\n",
    "\n",
    "    early_stop_mode: EarlyStopMode\n",
    "    batch_size_mode: BatchSizeMode\n",
    "    train_mode: TrainMode\n",
    "        \n",
    "    volume_name: str\n",
    "    volume_version: str\n",
    "    labels_version: str\n",
    "    \n",
    "    # override the auto-sized value \n",
    "    # this allows to reproduce reproduce the same conditions across experiments\n",
    "    batch_size_per_gpu: Optional[int] = None  \n",
    "    \n",
    "    random_state_seed: int = 42\n",
    "        \n",
    "    runid: int = None\n",
    "        \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        if self.train_mode.is_continuation:\n",
    "            assert self.runid is not None, f\"Incompatible args {self.runid=} {self.self.train_mode=}\"\n",
    "        \n",
    "        if self.runid is None:\n",
    "            self.runid = int(time.time())\n",
    "            \n",
    "        if self.batch_size_per_gpu is not None:\n",
    "            assert self.batch_size_per_gpu > 0, f\"{self.batch_size_per_gpu=}\"\n",
    "\n",
    "            ngpus = len(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "            if ngpus > 0: \n",
    "                assert self.batch_size_per_gpu % ngpus == 0, f\"{self.batch_size_per_gpu=} {ngpus=}\"\n",
    "            \n",
    "            try:\n",
    "                MAX_INTERNAL_NVOXELS\n",
    "            \n",
    "            except NameError as ex:\n",
    "                ValueError(f\"Please define the variable `{ex.args[0]}`\")\n",
    "\n",
    "                \n",
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_V1_REDUCED as VOLUME_NAME_VERSION,\n",
    "    VOLUME_COMPOSITE_V1_LABELS_REFINED3 as LABELS_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00_LABELS_REFINED3 as LABELS_VERSION,\n",
    ")\n",
    "\n",
    "args = Args(\n",
    "    early_stop_mode = Args.EarlyStopMode.no_early_stop,\n",
    "    batch_size_mode = Args.BatchSizeMode.try_max_and_reduce,\n",
    "    train_mode=Args.TrainMode.from_scratch,\n",
    "    \n",
    "    volume_name=VOLUME_NAME_VERSION[0],\n",
    "    volume_version=VOLUME_NAME_VERSION[1],\n",
    "    labels_version=LABELS_VERSION,\n",
    "    \n",
    "#     random_state_seed=30,  # I'll change it so we don't repeat the same crops from the begining\n",
    "#     runid = 1607698009,\n",
    ")\n",
    "\n",
    "logger.info(f\"args={dict2str(asdict(args))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnVqPFS9BNCg"
   },
   "source": [
    "\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-8-050df0cd2614>:<module>:007}::[2020-12-14::10:51:37.820]\n",
      "tf_version='2.2.0'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-8-050df0cd2614>:<module>:010}::[2020-12-14::10:51:37.821]\n",
      "Hostname: akela.materiaux.ensmp.fr\n",
      "Num GPUs Available: 1\n",
      "This should be:\n",
      "\t2 on R790-TOMO\n",
      "\t1 on akela\n",
      "\t1 on hathi\n",
      "\t1 on krilin\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-8-050df0cd2614>:<module>:014}::[2020-12-14::10:51:37.900]\n",
      "physical GPU devices:\n",
      "\tPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "logical GPU devices:\n",
      "\tLogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "DEBUG::tomo2seg::{<ipython-input-8-050df0cd2614>:<module>:025}::[2020-12-14::10:51:37.904]\n",
      "gpu_strategy=<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f2ac8558490>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "random_state = np.random.RandomState(args.random_state_seed)\n",
    "\n",
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "tf_version = tf.__version__\n",
    "logger.info(f\"{tf_version=}\")\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "logger.info(\n",
    "    f\"Hostname: {hostname}\\nNum GPUs Available: {n_gpus}\\nThis should be:\\n\\t\" + '\\n\\t'.join(['2 on R790-TOMO', '1 on akela', '1 on hathi', '1 on krilin'])\n",
    ")\n",
    "\n",
    "logger.debug(\n",
    "    \"physical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_physical_devices('GPU'))) + \"\\n\" +\n",
    "    \"logical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_logical_devices('GPU'))) \n",
    ")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "gpu_strategy = tf.distribute.MirroredStrategy()  \n",
    "logger.debug(f\"{gpu_strategy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already deleted (:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "except NameError:\n",
    "    print(\"already deleted (:\")\n",
    "else:\n",
    "    del tomo2seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::tomo2seg::{<ipython-input-12-084be105897a>:<module>:043}::[2020-12-14::10:53:06.711]\n",
      "The model is already defined. To create a new one: `del tomo2seg_model`\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-12-084be105897a>:<module>:046}::[2020-12-14::10:53:06.714]\n",
      "tomo2seg_model\n",
      "{   'factory_function': 'tomo2seg.modular_unet.u_net2halfd_IIenc',\n",
      "    'factory_kwargs': {   'convlayer': <ConvLayer.conv2d: 0>,\n",
      "                          'depth': 4,\n",
      "                          'input_shape': (208, 208, 5),\n",
      "                          'nb_filters_0': 8,\n",
      "                          'output_channels': 3,\n",
      "                          'sigma_noise': 0,\n",
      "                          'unet_block_kwargs': {   'batch_norm': True,\n",
      "                                                   'dropout': 0,\n",
      "                                                   'kernel_size': 3,\n",
      "                                                   'res': True},\n",
      "                          'unet_down_kwargs': {'batchnorm': True},\n",
      "                          'unet_up_kwargs': {'batchnorm': True},\n",
      "                          'updown_conv_sampling': True},\n",
      "    'fold': 0,\n",
      "    'master_name': 'unet2halfd',\n",
      "    'runid': 1607939493,\n",
      "    'version': 'II-enc-c208-f08'}\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-12-084be105897a>:<module>:047}::[2020-12-14::10:53:06.715]\n",
      "tomo2seg_model.name='unet2halfd.II-enc-c208-f08.fold000.1607-939-493'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [manual-input]\n",
    "crop_shape = (208, 208, 5)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "model_nclasses = 3\n",
    "\n",
    "model_master_name = \"unet2halfd\"\n",
    "model_version = \"II-enc-c208-f08\"\n",
    "\n",
    "model_is_2halfd = True\n",
    "model_is_2d = False\n",
    "\n",
    "model_factory_function = modular_unet.u_net2halfd_IIenc\n",
    "model_factory_kwargs = {\n",
    "    **modular_unet.kwargs_IIenc03,\n",
    "    **dict(\n",
    "        convlayer=modular_unet.ConvLayer.conv2d,\n",
    "        input_shape = crop_shape,\n",
    "        output_channels = model_nclasses,\n",
    "#         nb_filters_0 = 2,\n",
    "#         nb_filters_0 = 4,\n",
    "        nb_filters_0 = 8,\n",
    "#         nb_filters_0 = 12,\n",
    "#         nb_filters_0 = 16,common_random_state\n",
    "#         nb_filters_0 = 32,\n",
    "    ),\n",
    "}\n",
    "\n",
    "try:\n",
    "    tomo2seg_model\n",
    "    \n",
    "except NameError:\n",
    "    \n",
    "    logger.info(\"Creating a Tomo2SegModel.\")\n",
    "    \n",
    "    tomo2seg_model = Tomo2SegModel(\n",
    "        model_master_name, \n",
    "        model_version, \n",
    "        runid=args.runid,\n",
    "        factory_function=model_factory_function,\n",
    "        factory_kwargs=model_factory_kwargs,\n",
    "    )\n",
    "                    \n",
    "else:\n",
    "    logger.warning(\"The model is already defined. To create a new one: `del tomo2seg_model`\")\n",
    "\n",
    "finally:\n",
    "    logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")    \n",
    "    logger.info(f\"{tomo2seg_model.name=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY",
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:001}::[2020-12-14::10:51:55.711]\n",
      "Creating the Keras model.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:031}::[2020-12-14::10:51:55.712]\n",
      "A new model will be instantiated!\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:051}::[2020-12-14::10:51:55.714]\n",
      "Instantiating a new model with model_factory_function=u_net2halfd_IIenc.\n",
      "\n",
      "DEBUG::tomo2seg::{modular_unet.py:u_net2halfd_IIenc:488}::[2020-12-14::10:51:55.714]\n",
      "dict2str(unet_block_kwargs)=\"{   'batch_norm': True,\\n    'convlayer': <ConvLayer.conv2d: 0>,\\n    'dropout': 0,\\n    'kernel_size': 3,\\n    'res': True,\\n    'return_layers': True}\"\n",
      "\n",
      "DEBUG::tomo2seg::{modular_unet.py:u_net2halfd_IIenc:504}::[2020-12-14::10:51:55.715]\n",
      "dict2str(unet_down_kwargs)=\"{   'batchnorm': True,\\n    'conv_sampling': True,\\n    'convlayer': <ConvLayer.conv2d: 0>,\\n    'return_layers': True}\"\n",
      "\n",
      "DEBUG::tomo2seg::{modular_unet.py:u_net2halfd_IIenc:519}::[2020-12-14::10:51:55.715]\n",
      "dict2str(unet_up_kwargs)=\"{   'batchnorm': True,\\n    'conv_sampling': True,\\n    'convlayer': <ConvLayer.conv2d: 0>,\\n    'return_layers': True}\"\n",
      "\n",
      "DEBUG::tomo2seg::{modular_unet.py:u_net2halfd_IIenc:527}::[2020-12-14::10:51:55.716]\n",
      "nlayers=5\n",
      "\n",
      "DEBUG::tomo2seg::{modular_unet.py:u_net2halfd_IIenc:531}::[2020-12-14::10:51:55.716]\n",
      "predicted_layer=2\n",
      "\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:058}::[2020-12-14::10:51:59.801]\n",
      "Compiling the model.\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:068}::[2020-12-14::10:51:59.802]\n",
      "loss=<function jaccard2_flat at 0x7f2a431e0c10>\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:069}::[2020-12-14::10:51:59.803]\n",
      "optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f2a4306ffa0>\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-11-841396a8de5d>:<module>:070}::[2020-12-14::10:51:59.803]\n",
      "metrics=[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Creating the Keras model.\")\n",
    "\n",
    "if args.train_mode.is_continuation:\n",
    "    logger.warning(\"Training continuation: a model will be loaded.\")\n",
    "\n",
    "    if args.train_mode == Args.TrainMode.continuation_from_latest_model:\n",
    "        logger.info(\"Using the LATEST model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved_model:\n",
    "        logger.info(\"Using the AUTOSAVED model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved_model_path\n",
    "\n",
    "    elif args.train_mode == Args.TrainMode.continuation_from_autosaved2_best_model:\n",
    "        logger.info(\"Using the (best) AUTOSAVED2 model to continue the training.\")\n",
    "        load_model_path = tomo2seg_model.autosaved2_best_model_path\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"{args.train_mode=}\")\n",
    "\n",
    "elif (\n",
    "    tomo2seg_model.model_path.exists() or\n",
    "    tomo2seg_model.autosaved_model_path.exists()\n",
    "    # todo uncomment me when implemented\n",
    "#             or tomo2seg_model.autosaved2_best_model_path.exists()\n",
    "):\n",
    "    logger.error(f\"The model seems to already exist but this is not a continuation. Please, make sure the arguments are correct.\")\n",
    "    raise ValueError(f\"{args.train_mode=} ==> {args.train_mode.is_continuation=} {tomo2seg_model.name=}\")\n",
    "\n",
    "elif args.train_mode == Args.TrainMode.from_scratch:\n",
    "    logger.info(f\"A new model will be instantiated!\")\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.train_mode=}\")\n",
    "\n",
    "    \n",
    "with gpu_strategy.scope():\n",
    "    \n",
    "    if args.train_mode.is_continuation:\n",
    "        \n",
    "        assert load_model_path.exists(), f\"Inconsistent arguments {args.train_mode.is_continuation=} {load_model_path=} {load_model_path.exists()=}.\"\n",
    "        \n",
    "        logger.info(f\"Loading model {load_model_path.name}\")\n",
    "        \n",
    "        model = keras.models.load_model(str(load_model_path), compile=False)\n",
    "\n",
    "        assert model.name == tomo2seg_model.name, f\"{model.name=} {tomo2seg_model.name=}\"\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        logger.info(f\"Instantiating a new model with model_factory_function={model_factory_function.__name__}.\")\n",
    "      \n",
    "        model = model_factory_function(\n",
    "            name=tomo2seg_model.name,\n",
    "            **model_factory_kwargs\n",
    "        )\n",
    "\n",
    "    logger.info(\"Compiling the model.\")\n",
    "\n",
    "    # [manual-input]\n",
    "    # using the avg jaccard is dangerous if one of the classes is too\n",
    "    # underrepresented because it's jaccard will be unstable\n",
    "    # to be verified!\n",
    "    loss = tomo2seg_losses.jaccard2_flat\n",
    "    optimizer = optimizers.Adam(lr=.003)\n",
    "    metrics = []\n",
    "    \n",
    "    logger.debug(f\"{loss=}\")\n",
    "    logger.debug(f\"{optimizer=}\")\n",
    "    logger.debug(f\"{metrics=}\")\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:003}::[2020-12-14::10:53:40.690]\n",
      "Saving the model at tomo2seg_model.model_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493').\n",
      "\n",
      "WARNING:tensorflow:From /home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/assets\n",
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:007}::[2020-12-14::10:53:56.813]\n",
      "Writing the model summary at tomo2seg_model.summary_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/summary.txt').\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:014}::[2020-12-14::10:53:57.252]\n",
      "Printing an image of the architecture at tomo2seg_model.architecture_plot_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/architecture.png').\n",
      "\n",
      "INFO::tomo2seg::{logger.py:add_file_handler:021}::[2020-12-14::10:53:59.917]\n",
      "Added a new file handler to the logger. logspath='/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.train.log'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:021}::[2020-12-14::10:53:59.921]\n",
      "args\n",
      "{   'batch_size_mode': <BatchSizeMode.try_max_and_reduce: 1>,\n",
      "    'batch_size_per_gpu': None,\n",
      "    'early_stop_mode': <EarlyStopMode.no_early_stop: 0>,\n",
      "    'labels_version': 'refined3',\n",
      "    'random_state_seed': 42,\n",
      "    'runid': 1607939493,\n",
      "    'train_mode': <TrainMode.from_scratch: 0>,\n",
      "    'volume_name': 'PA66GF30',\n",
      "    'volume_version': 'v1'}\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:022}::[2020-12-14::10:53:59.923]\n",
      "tomo2seg_model.name='unet2halfd.II-enc-c208-f08.fold000.1607-939-493'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-e7ea4213221d>:<module>:023}::[2020-12-14::10:53:59.927]\n",
      "tomo2seg_model\n",
      "{   'factory_function': 'tomo2seg.modular_unet.u_net2halfd_IIenc',\n",
      "    'factory_kwargs': {   'convlayer': <ConvLayer.conv2d: 0>,\n",
      "                          'depth': 4,\n",
      "                          'input_shape': (208, 208, 5),\n",
      "                          'nb_filters_0': 8,\n",
      "                          'output_channels': 3,\n",
      "                          'sigma_noise': 0,\n",
      "                          'unet_block_kwargs': {   'batch_norm': True,\n",
      "                                                   'dropout': 0,\n",
      "                                                   'kernel_size': 3,\n",
      "                                                   'res': True},\n",
      "                          'unet_down_kwargs': {'batchnorm': True},\n",
      "                          'unet_up_kwargs': {'batchnorm': True},\n",
      "                          'updown_conv_sampling': True},\n",
      "    'fold': 0,\n",
      "    'master_name': 'unet2halfd',\n",
      "    'runid': 1607939493,\n",
      "    'version': 'II-enc-c208-f08'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not args.train_mode.is_continuation:\n",
    "    \n",
    "    logger.info(f\"Saving the model at {tomo2seg_model.model_path=}.\")\\\n",
    "    \n",
    "    model.save(tomo2seg_model.model_path)\n",
    "\n",
    "    logger.info(f\"Writing the model summary at {tomo2seg_model.summary_path=}.\")\n",
    "    \n",
    "    with tomo2seg_model.summary_path.open(\"w\") as f:\n",
    "        def print_to_txt(line):\n",
    "            f.writelines([line + \"\\n\"])\n",
    "        model.summary(print_fn=print_to_txt, line_length=140)\n",
    "\n",
    "    logger.info(f\"Printing an image of the architecture at {tomo2seg_model.architecture_plot_path=}.\")\n",
    "    \n",
    "    utils.plot_model(model, show_shapes=True, to_file=tomo2seg_model.architecture_plot_path);\n",
    "    \n",
    "add_file_handler(logger, tomo2seg_model.train_log_path)\n",
    "\n",
    "# repeat it so that the log file saves this\n",
    "logger.info(f\"args\\n{dict2str(asdict(args))}\")    \n",
    "logger.info(f\"{tomo2seg_model.name=}\")\n",
    "logger.info(f\"tomo2seg_model\\n{dict2str(asdict(tomo2seg_model))}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8e5FhmUaKND"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1602255917946,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "4CfP7usu2VKr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{data.py:with_check:264}::[2020-12-14::10:54:48.187]\n",
      "vol=Volume(name='PA66GF30', version='v1', _metadata=None)\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:metadata:201}::[2020-12-14::10:54:48.189]\n",
      "Loading metadata from `/home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.metadata.yml`.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:008}::[2020-12-14::10:54:48.193]\n",
      "volume\n",
      "{   '_metadata': {   'dimensions': [1300, 1040, 1900],\n",
      "                     'dtype': 'uint8',\n",
      "                     'labels': [0, 1, 2],\n",
      "                     'labels_names': {0: 'matrix', 1: 'fiber', 2: 'porosity'},\n",
      "                     'set_partitions': {   'test': {   'alias': 'test',\n",
      "                                                       'x_range': [0, 1300],\n",
      "                                                       'y_range': [0, 1040],\n",
      "                                                       'z_range': [1300, 1600]},\n",
      "                                           'train': {   'alias': 'train',\n",
      "                                                        'x_range': [0, 1300],\n",
      "                                                        'y_range': [0, 1040],\n",
      "                                                        'z_range': [0, 1300]},\n",
      "                                           'val': {   'alias': 'val',\n",
      "                                                      'x_range': [0, 1300],\n",
      "                                                      'y_range': [0, 1040],\n",
      "                                                      'z_range': [   1600,\n",
      "                                                                     1900]}}},\n",
      "    'name': 'PA66GF30',\n",
      "    'version': 'v1'}\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:012}::[2020-12-14::10:54:48.194]\n",
      "Loading data from disk.\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:025}::[2020-12-14::10:54:59.284]\n",
      "voldata.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:030}::[2020-12-14::10:54:59.285]\n",
      "voldata_train.shape=(1300, 1040, 1300)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:031}::[2020-12-14::10:54:59.286]\n",
      "voldata_val.shape=(1300, 1040, 300)\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:046}::[2020-12-14::10:55:03.053]\n",
      "vollabels.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:051}::[2020-12-14::10:55:03.054]\n",
      "vollabels_train.shape=(1300, 1040, 1300)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-16-3be1b813cb29>:<module>:052}::[2020-12-14::10:55:03.055]\n",
      "vollabels_val.shape=(1300, 1040, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metadata/paths objects\n",
    "\n",
    "## Volume\n",
    "volume = Volume.with_check(\n",
    "    name=args.volume_name, version=args.volume_version\n",
    ")\n",
    "\n",
    "logger.info(f\"volume\\n{dict2str(asdict(volume))}\")\n",
    "\n",
    "assert volume.nclasses\n",
    "\n",
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "## Data\n",
    "voldata = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "    \n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    "    \n",
    ") / volume.normalization_factor\n",
    "\n",
    "logger.debug(f\"{voldata.shape=}\")\n",
    "\n",
    "voldata_train = volume.train_partition.get_volume_partition(voldata)\n",
    "voldata_val = volume.val_partition.get_volume_partition(voldata)\n",
    "\n",
    "logger.debug(f\"{voldata_train.shape=}\")\n",
    "logger.debug(f\"{voldata_val.shape=}\")\n",
    "\n",
    "del voldata\n",
    "\n",
    "## Labels\n",
    "\n",
    "vollabels = file_utils.HST_read(\n",
    "    str(volume.versioned_labels_path(args.labels_version)),\n",
    "    \n",
    "    autoparse_filename=False,\n",
    "    data_type=\"uint8\",\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vollabels.shape=}\")\n",
    "\n",
    "vollabels_train = volume.train_partition.get_volume_partition(vollabels)\n",
    "vollabels_val = volume.val_partition.get_volume_partition(vollabels)\n",
    "\n",
    "logger.debug(f\"{vollabels_train.shape=}\")\n",
    "logger.debug(f\"{vollabels_val.shape=}\")\n",
    "\n",
    "del vollabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnsQ7lX0bVRh"
   },
   "source": [
    "# Data crop sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:023}::[2020-12-14::10:56:15.224]\n",
      "input_layer=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f2a4306fd60>\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:029}::[2020-12-14::10:56:15.226]\n",
      "input_nvoxels=216320\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:041}::[2020-12-14::10:56:15.232]\n",
      "max_internal_nvoxels=4845568 (4,845,568)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:003}::[2020-12-14::10:56:15.233]\n",
      "model_internal_nvoxel_factor=23\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:007}::[2020-12-14::10:56:15.235]\n",
      "max_batch_nvoxels=5810086 (5,810,086)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:011}::[2020-12-14::10:56:15.236]\n",
      "crop_shape=(208, 208, 5) ==> crop_nvoxels=216320\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:015}::[2020-12-14::10:56:15.237]\n",
      "batch_size_per_gpu=26\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:021}::[2020-12-14::10:56:15.240]\n",
      "n_gpus=1\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-17-ce5d5f99df49>:<module>:025}::[2020-12-14::10:56:15.241]\n",
      "batch_size=26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_internal_nvoxel_factor = tomo2seg_utils.get_model_internal_nvoxel_factor(model)\n",
    "\n",
    "logger.debug(f\"{model_internal_nvoxel_factor=}\")\n",
    "\n",
    "max_batch_nvoxels = int(np.floor(MAX_INTERNAL_NVOXELS / model_internal_nvoxel_factor))\n",
    "\n",
    "logger.debug(f\"{max_batch_nvoxels=} ({humanize.intcomma(max_batch_nvoxels)})\")\n",
    "\n",
    "crop_nvoxels = functools.reduce(operator.mul, crop_shape)\n",
    "\n",
    "logger.debug(f\"{crop_shape=} ==> {crop_nvoxels=}\")\n",
    "\n",
    "max_batch_size_per_gpu = batch_size_per_gpu = max(1, int(np.floor(max_batch_nvoxels / crop_nvoxels)))\n",
    "\n",
    "logger.info(f\"{batch_size_per_gpu=}\")\n",
    "\n",
    "if args.batch_size_per_gpu is not None:\n",
    "    logger.warning(f\"{args.batch_size_per_gpu=} given ==> replacing {batch_size_per_gpu=}\")\n",
    "    batch_size_per_gpu = args.batch_size_per_gpu\n",
    "\n",
    "logger.info(f\"{n_gpus=}\")\n",
    "\n",
    "batch_size = batch_size_per_gpu * max(1, n_gpus)\n",
    "\n",
    "logger.info(f\"{batch_size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-18-d49b358ca222>:<module>:008}::[2020-12-14::10:56:16.471]\n",
      "metacrop_gen_common_kwargs={'crop_shape': (208, 208, 5), 'common_random_state_seed': 42, 'is_2halfd': True, 'gt_type': <enum 'GT2D'>}\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-18-d49b358ca222>:<module>:019}::[2020-12-14::10:56:16.472]\n",
      "vol_crop_seq_common_kwargs={'output_as_2d': False, 'output_as_2halfd': True, 'labels': [0, 1, 2], 'debug__no_data_check': True}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metacrop_gen_common_kwargs = dict(\n",
    "    crop_shape=crop_shape,\n",
    "    common_random_state_seed=args.random_state_seed,\n",
    "    is_2halfd=model_is_2halfd,\n",
    "    gt_type=volume_sequence.GT2D if model_is_2d or model_is_2halfd else volume_sequence.GT3D,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{metacrop_gen_common_kwargs=}\")\n",
    "\n",
    "vol_crop_seq_common_kwargs = dict(\n",
    "    output_as_2d = model_is_2d,\n",
    "    output_as_2halfd = model_is_2halfd,\n",
    "    labels = volume.metadata.labels,\n",
    "\n",
    "    # [manual-input]\n",
    "    debug__no_data_check=True,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{vol_crop_seq_common_kwargs=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:450}::[2020-12-14::10:56:18.378]\n",
      "Built UniformGridPosition from volume_shape=(1300, 1040, 1300) and crop_shape=(208, 208, 5) ==> {'x_range': (0, 1093), 'y_range': (0, 833), 'z_range': (0, 1296)}\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:412}::[2020-12-14::10:56:18.380]\n",
      "UniformGridPosition ==> npositions=1179967824 (1,179,967,824)\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:18.381]\n",
      "Initializing ET3DConstantEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:18.381]\n",
      "Initializing GTUniformEverywhere2 with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:807}::[2020-12-14::10:56:18.382]\n",
      "len(self.gt_name_list)=8\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:18.383]\n",
      "Initializing VSUniformEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1465}::[2020-12-14::10:56:18.384]\n",
      "self.output_as_2halfd=True only xy layers is available for 2.5d now!\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:1476}::[2020-12-14::10:56:18.384]\n",
      "Initializing VolumeCropSequence.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1508}::[2020-12-14::10:56:18.385]\n",
      "No meta crops history file path given. The randomly generated crops will not be saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = voldata_train\n",
    "labels = vollabels_train\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "crop_seq_train = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=batch_size,\n",
    "    \n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_train01(\n",
    "        volume_shape=volume_shape,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "        data_original_dtype=volume.metadata.dtype,\n",
    "        \n",
    "        # [manual-input]\n",
    "        gt_no_transpose_rot = False,\n",
    "    ),\n",
    "    \n",
    "    # this volume cropper only returns random crops, \n",
    "    # so the number of crops per epoch/batch is w/e i want\n",
    "    epoch_size=10,\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-20-4565690e19ff>:<module>:010}::[2020-12-14::10:56:19.606]\n",
      "val_batch_size=26\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:build_min_overlap:518}::[2020-12-14::10:56:19.607]\n",
      "Building SequentialGridPosition with minimal overlap (smallest n_steps in each directions) n_steps={'n_steps_x': 7, 'n_steps_y': 5, 'n_steps_z': 60}.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:build_min_overlap:521}::[2020-12-14::10:56:19.608]\n",
      "n_steps_kwargs={'n_steps_z': 30} was given --> effective n_steps={'n_steps_x': 7, 'n_steps_y': 5, 'n_steps_z': 30}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:450}::[2020-12-14::10:56:19.609]\n",
      "Built SequentialGridPosition from volume_shape=(1300, 1040, 300) and crop_shape=(208, 208, 5) ==> {'x_range': (0, 1093), 'y_range': (0, 833), 'z_range': (0, 296)}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:__post_init__:498}::[2020-12-14::10:56:19.612]\n",
      "The SequentialGridPosition has len(self.positions)=1050 different positions (therefore crops).\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:19.613]\n",
      "Initializing ET3DConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:19.614]\n",
      "Initializing GTConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:707}::[2020-12-14::10:56:19.614]\n",
      "Initializing VSConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1465}::[2020-12-14::10:56:19.615]\n",
      "self.output_as_2halfd=True only xy layers is available for 2.5d now!\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:1476}::[2020-12-14::10:56:19.615]\n",
      "Initializing VolumeCropSequence.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:1508}::[2020-12-14::10:56:19.616]\n",
      "No meta crops history file path given. The randomly generated crops will not be saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = voldata_val\n",
    "labels = vollabels_val\n",
    "\n",
    "volume_shape = data.shape\n",
    "\n",
    "# the validation has no reproducibility issues\n",
    "# so let's push the GPUs (:\n",
    "val_batch_size = max_batch_size_per_gpu * n_gpus\n",
    "\n",
    "logger.debug(f\"{val_batch_size=}\")\n",
    "\n",
    "grid_pos_gen = SequentialGridPosition.build_min_overlap(\n",
    "    volume_shape=volume_shape, \n",
    "    crop_shape=crop_shape,\n",
    "    # [manual-input]\n",
    "    # reduce the total number of crops\n",
    "#         n_steps_x=11,\n",
    "#         n_steps_y=11,\n",
    "        n_steps_z=30,\n",
    ")\n",
    "\n",
    "crop_seq_val = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    \n",
    "    batch_size=val_batch_size,\n",
    "    \n",
    "    # go through all the crops in validation\n",
    "    epoch_size=len(grid_pos_gen),      \n",
    "    \n",
    "    # data augmentation\n",
    "    meta_crop_generator=MetaCrop3DGenerator.build_setup_val00(\n",
    "        volume_shape=volume_shape,\n",
    "        grid_pos_gen=grid_pos_gen,\n",
    "        **metacrop_gen_common_kwargs,\n",
    "    ),\n",
    "#     debug__no_data_check = True,\n",
    "    \n",
    "    **vol_crop_seq_common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsccmAxOX7v"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-21-be8d862a0547>:<module>:009}::[2020-12-14::10:56:21.227]\n",
      "autosave_cb=<tensorflow.python.keras.callbacks.ModelCheckpoint object at 0x7f2909aadfd0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autosave_cb = keras_callbacks.ModelCheckpoint(\n",
    "    tomo2seg_model.autosaved2_model_path_str, \n",
    "    monitor=\"val_loss\", \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "logger.debug(f\"{autosave_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-22-9e74ae8e55e2>:<module>:007}::[2020-12-14::10:56:21.691]\n",
      "Creating a new history callback.\n",
      "\n",
      "INFO::tomo2seg::{callbacks.py:__init__:051}::[2020-12-14::10:56:21.706]\n",
      "Loading history from csv self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv').\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:__init__:071}::[2020-12-14::10:56:21.708]\n",
      "History hasn't been saved yet.\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-22-9e74ae8e55e2>:<module>:040}::[2020-12-14::10:56:21.709]\n",
      "history_cb=<tomo2seg.callbacks.History object at 0x7f29208c9df0>\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-22-9e74ae8e55e2>:<module>:041}::[2020-12-14::10:56:21.710]\n",
      "history_cb.dataframe.index.size=0\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-22-9e74ae8e55e2>:<module>:042}::[2020-12-14::10:56:21.711]\n",
      "history_cb.last_epoch=0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is important because sometimes i update things in the notebook\n",
    "# so i need to make sure that the objects in the history cb are updated\n",
    "try:\n",
    "    history_cb\n",
    "    \n",
    "except NameError:\n",
    "    logger.info(\"Creating a new history callback.\")\n",
    "    \n",
    "    history_cb = tomo2seg_callbacks.History(\n",
    "        optimizer=model.optimizer,\n",
    "        crop_seq_train=crop_seq_train,\n",
    "        crop_seq_val=crop_seq_val,\n",
    "        backup=1,\n",
    "        csv_path=tomo2seg_model.history_path,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"The history callback already exists!\")\n",
    "    \n",
    "    history_df = history_cb.dataframe\n",
    "\n",
    "    try:\n",
    "        history_df_temp = pd.read_csv(tomo2seg_model.history_path)\n",
    "        # keep the longest one\n",
    "        history_df = history_df if history_df.shape[0] >= history_df_temp.shape[0] else history_df_temp\n",
    "        del history_df_temp\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "finally:\n",
    "    # make sure the correct objects are linked \n",
    "    history_cb.optimizer = model.optimizer\n",
    "    history_cb.crop_seq_train = crop_seq_train\n",
    "    history_cb.crop_seq_val = crop_seq_val\n",
    "\n",
    "logger.debug(f\"{history_cb=}\")\n",
    "logger.debug(f\"{history_cb.dataframe.index.size=}\")\n",
    "logger.debug(f\"{history_cb.last_epoch=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-23-6edf00a82883>:<module>:005}::[2020-12-14::10:56:21.893]\n",
      "history_plot_cb=HistoryPlot(history_callback=<tomo2seg.callbacks.History object at 0x7f29208c9df0>, save_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/train-hist-plot-wip.png'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_plot_cb = tomo2seg_callbacks.HistoryPlot(\n",
    "    history_callback=history_cb,\n",
    "    save_path=tomo2seg_model.train_history_plot_wip_path\n",
    ")\n",
    "logger.debug(f\"{history_plot_cb=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-24-1b2efaf987e7>:<module>:001}::[2020-12-14::10:56:22.370]\n",
      "Setting up early stop with args.early_stop_mode=<EarlyStopMode.no_early_stop: 0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Setting up early stop with {args.early_stop_mode=}\")\n",
    "\n",
    "if args.early_stop_mode == Args.EarlyStopMode.no_early_stop:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(f\"{args.early_stop_mode=}\")\n",
    "#     # todo modify the early stopping to take more conditions (don't stop too early before it doesnt break the jaccard2=.32)\n",
    "#     early_stop_cb = keras_callbacks.EarlyStopping(  \n",
    "#         monitor='val_loss', \n",
    "#         min_delta=.1 / 100, \n",
    "#         patience=50,\n",
    "#         verbose=2, \n",
    "#         mode='auto',\n",
    "#         baseline=.71,  # 0th-order classifier\n",
    "#         restore_best_weights=False,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYnLzlFdDeY"
   },
   "source": [
    "# Summary before training\n",
    "\n",
    "stuff that i use after the training but i want it to appear in the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode## Metadata\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Volume slices\n",
    "\n",
    "todo do this in a notebook\n",
    "\n",
    "## Generator samples\n",
    "\n",
    "todo do this in a notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuEmT2AZODXi"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teeth log lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.519]\n",
      "LogSpaceSchedule ==> self.n=10\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.520]\n",
      "LogSpaceSchedule ==> self.n=30\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.521]\n",
      "LogSpaceSchedule ==> self.n=20\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.522]\n",
      "LogSpaceSchedule ==> self.n=40\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.523]\n",
      "LogSpaceSchedule ==> self.n=20\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.524]\n",
      "LogSpaceSchedule ==> self.n=40\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-12-14::10:56:25.525]\n",
      "LogSpaceSchedule ==> self.n=100\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:107}::[2020-12-14::10:56:25.526]\n",
      "ComposedSchedule ==> self.n=260\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-25-4abf876edaa4>:<module>:012}::[2020-12-14::10:56:25.527]\n",
      "lr_schedule_cb.schedule.range=(0, 260)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [manual-input]\n",
    "lr_schedule_cb = keras_callbacks.LearningRateScheduler(\n",
    "    schedule=(\n",
    "        schedule := tomo2seg_schedule.get_schedule00()\n",
    "#         schedule := tomo2seg_schedule.LogSpaceSchedule(\n",
    "#             offset_epoch=0, wait=0, start=-3, stop=-5, n_between_scales=100\n",
    "#         )\n",
    "    ),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"{lr_schedule_cb.schedule.range=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-26-c16af9d5fdc3>:<module>:019}::[2020-12-14::10:56:28.565]\n",
      "using callback TerminateOnNaN\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-26-c16af9d5fdc3>:<module>:019}::[2020-12-14::10:56:28.566]\n",
      "using callback ModelCheckpoint\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-26-c16af9d5fdc3>:<module>:019}::[2020-12-14::10:56:28.567]\n",
      "using callback History\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-26-c16af9d5fdc3>:<module>:019}::[2020-12-14::10:56:28.567]\n",
      "using callback HistoryPlot\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-26-c16af9d5fdc3>:<module>:019}::[2020-12-14::10:56:28.568]\n",
      "using callback LearningRateScheduler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras_callbacks.TerminateOnNaN(),\n",
    "    autosave_cb,\n",
    "    history_cb,\n",
    "    history_plot_cb,\n",
    "    lr_schedule_cb,\n",
    "]\n",
    "\n",
    "try:\n",
    "    early_stop_cb\n",
    "\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "else:\n",
    "    callbacks.append(early_stop_cb)\n",
    "\n",
    "for cb in callbacks:\n",
    "    logger.debug(f\"using callback {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::10:59:00.993]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[26,224,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-1/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[26,224,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-1/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 644, in _call\n",
      "    return self._stateless_fn(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[26,224,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-1/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[26,224,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-1/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::10:59:01.001]\n",
      "reduced batch_size=25\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:01:33.711]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[25,32,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-1-conv-skip-bn/FusedBatchNormV3 (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[25,32,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-1-conv-skip-bn/FusedBatchNormV3 (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[25,32,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-1-conv-skip-bn/FusedBatchNormV3 (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[25,32,104,104] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-1-conv-skip-bn/FusedBatchNormV3 (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:01:33.715]\n",
      "reduced batch_size=24\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:04:01.395]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[24,32,209,209] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-UP/conv2d_transpose (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[24,32,209,209] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-UP/conv2d_transpose (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[24,32,209,209] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-UP/conv2d_transpose (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[24,32,209,209] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-UP/conv2d_transpose (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:04:01.401]\n",
      "reduced batch_size=23\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:06:34.503]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[23,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[23,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[23,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[23,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:06:34.508]\n",
      "reduced batch_size=22\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:09:10.172]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[22,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[22,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[22,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[22,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:09:10.177]\n",
      "reduced batch_size=21\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:11:45.024]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[21,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[21,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[21,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[21,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/concat-0/concat (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:11:45.030]\n",
      "reduced batch_size=20\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:14:27.609]\n",
      "2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[20,16,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv2/Conv2D (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[20,16,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv2/Conv2D (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.\n",
      "  (0) Resource exhausted:  OOM when allocating tensor with shape[20,16,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv2/Conv2D (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "\t [[div_no_nan/ReadVariableOp_1/_468]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "  (1) Resource exhausted:  OOM when allocating tensor with shape[20,16,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv2/Conv2D (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function -> train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:14:27.614]\n",
      "reduced batch_size=19\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:17:42.136]\n",
      " OOM when allocating tensor with shape[19,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[19,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:17:42.140]\n",
      "reduced batch_size=18\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:21:03.492]\n",
      " OOM when allocating tensor with shape[18,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[18,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:21:03.497]\n",
      "reduced batch_size=17\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:24:21.349]\n",
      " OOM when allocating tensor with shape[17,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[17,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv-skip/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:24:21.354]\n",
      "reduced batch_size=16\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "ERROR::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:048}::[2020-12-14::11:27:41.158]\n",
      " OOM when allocating tensor with shape[16,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv1/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 41, in <module>\n",
      "    fit()\n",
      "  File \"<ipython-input-27-1e0a9057069d>\", line 14, in fit\n",
      "    model.fit(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 66, in _method_wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 848, in fit\n",
      "    tmp_logs = train_function(iterator)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1661, in _filtered_call\n",
      "    return self._call_flat(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1745, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function.call(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 593, in call\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[16,112,208,208] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[node gradient_tape/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/dec-block-0-conv1/Conv2DBackpropInput (defined at /threading.py:932) ]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      " [Op:__inference_train_function_66974]\n",
      "\n",
      "Function call stack:\n",
      "train_function\n",
      "\n",
      "WARNING::tomo2seg::{<ipython-input-27-1e0a9057069d>:<module>:054}::[2020-12-14::11:27:41.163]\n",
      "reduced batch_size=15\n",
      "\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/400\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.66842, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.001-0.668424.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::11:40:19.936]\n",
      "Saving backup of the training history epoch=0 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:128}::[2020-12-14::11:40:20.026]\n",
      "epoch=0 is too early to plot something.\n",
      "\n",
      "ERROR::tomo2seg::{callbacks.py:on_epoch_end:169}::[2020-12-14::11:40:20.090]\n",
      "AssertionError occurred while trying to plot the history.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/callbacks.py\", line 140, in on_epoch_end\n",
      "    hist_display = viz.TrainingHistoryDisplay(\n",
      "  File \"<string>\", line 9, in __init__\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/viz.py\", line 254, in __post_init__\n",
      "AssertionError: You don't have enough epochs to plot. Go to the gym and call me later.\n",
      "10/10 - 295s - loss: 0.3230 - val_loss: 0.6684 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001291549665014884.\n",
      "Epoch 2/400\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.66842 to 0.61294, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.002-0.612935.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::11:45:04.912]\n",
      "Saving backup of the training history epoch=1 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:128}::[2020-12-14::11:45:04.943]\n",
      "epoch=1 is too early to plot something.\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::11:45:04.983]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:45:05.008]\n",
      "train: argmin=1 --> min=0.205\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:45:05.012]\n",
      "val: argmin=1 --> min=0.613\n",
      "\n",
      "10/10 - 284s - loss: 0.2047 - val_loss: 0.6129 - lr: 1.2915e-04\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001668100537200059.\n",
      "Epoch 3/400\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61294 to 0.54192, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.003-0.541921.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::11:49:50.409]\n",
      "Saving backup of the training history epoch=2 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::11:49:50.468]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:49:50.490]\n",
      "train: argmin=2 --> min=0.171\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:49:50.492]\n",
      "val: argmin=2 --> min=0.542\n",
      "\n",
      "10/10 - 284s - loss: 0.1708 - val_loss: 0.5419 - lr: 1.6681e-04\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00021544346900318845.\n",
      "Epoch 4/400\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54192 to 0.45954, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.004-0.459538.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::11:54:35.249]\n",
      "Saving backup of the training history epoch=3 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::11:54:35.317]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:54:35.339]\n",
      "train: argmin=3 --> min=0.16\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:54:35.341]\n",
      "val: argmin=3 --> min=0.46\n",
      "\n",
      "10/10 - 284s - loss: 0.1596 - val_loss: 0.4595 - lr: 2.1544e-04\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002782559402207126.\n",
      "Epoch 5/400\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45954 to 0.37045, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.005-0.370447.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::11:59:17.490]\n",
      "Saving backup of the training history epoch=4 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::11:59:17.670]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:59:17.692]\n",
      "train: argmin=4 --> min=0.149\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::11:59:17.694]\n",
      "val: argmin=4 --> min=0.37\n",
      "\n",
      "10/10 - 281s - loss: 0.1492 - val_loss: 0.3704 - lr: 2.7826e-04\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00035938136638046257.\n",
      "Epoch 6/400\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.37045 to 0.29500, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.006-0.295002.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:04:02.002]\n",
      "Saving backup of the training history epoch=5 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:04:02.064]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:04:02.086]\n",
      "train: argmin=5 --> min=0.141\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:04:02.088]\n",
      "val: argmin=5 --> min=0.295\n",
      "\n",
      "10/10 - 283s - loss: 0.1409 - val_loss: 0.2950 - lr: 3.5938e-04\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00046415888336127773.\n",
      "Epoch 7/400\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.29500 to 0.23984, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.007-0.239843.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:08:47.757]\n",
      "Saving backup of the training history epoch=6 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:08:47.954]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:08:47.977]\n",
      "train: argmin=6 --> min=0.138\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:08:47.979]\n",
      "val: argmin=6 --> min=0.24\n",
      "\n",
      "10/10 - 285s - loss: 0.1380 - val_loss: 0.2398 - lr: 4.6416e-04\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0005994842503189409.\n",
      "Epoch 8/400\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23984 to 0.20814, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.008-0.208136.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:13:39.551]\n",
      "Saving backup of the training history epoch=7 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:13:39.617]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:13:39.638]\n",
      "train: argmin=7 --> min=0.137\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:13:39.641]\n",
      "val: argmin=7 --> min=0.208\n",
      "\n",
      "10/10 - 291s - loss: 0.1369 - val_loss: 0.2081 - lr: 5.9948e-04\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.000774263682681127.\n",
      "Epoch 9/400\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20814 to 0.19013, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.009-0.190129.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:18:26.750]\n",
      "Saving backup of the training history epoch=8 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:18:26.840]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:18:26.862]\n",
      "train: argmin=8 --> min=0.134\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:18:26.864]\n",
      "val: argmin=8 --> min=0.19\n",
      "\n",
      "10/10 - 286s - loss: 0.1344 - val_loss: 0.1901 - lr: 7.7426e-04\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/400\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19013 to 0.17707, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.010-0.177071.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:23:11.674]\n",
      "Saving backup of the training history epoch=9 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:23:11.748]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:23:11.769]\n",
      "train: argmin=9 --> min=0.131\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:23:11.771]\n",
      "val: argmin=9 --> min=0.177\n",
      "\n",
      "10/10 - 284s - loss: 0.1307 - val_loss: 0.1771 - lr: 0.0010\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/400\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.17707 to 0.16610, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.011-0.166098.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:27:55.690]\n",
      "Saving backup of the training history epoch=10 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:27:55.749]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:27:55.772]\n",
      "train: argmin=10 --> min=0.129\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:27:55.775]\n",
      "val: argmin=10 --> min=0.166\n",
      "\n",
      "10/10 - 283s - loss: 0.1286 - val_loss: 0.1661 - lr: 0.0010\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/400\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.16610 to 0.15882, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.012-0.158825.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:32:40.760]\n",
      "Saving backup of the training history epoch=11 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:32:40.830]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:32:40.851]\n",
      "train: argmin=11 --> min=0.125\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:32:40.853]\n",
      "val: argmin=11 --> min=0.159\n",
      "\n",
      "10/10 - 284s - loss: 0.1248 - val_loss: 0.1588 - lr: 0.0010\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/400\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.15882\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:37:22.945]\n",
      "Saving backup of the training history epoch=12 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:37:23.002]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:37:23.027]\n",
      "train: argmin=12 --> min=0.12\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:37:23.029]\n",
      "val: argmin=11 --> min=0.159\n",
      "\n",
      "10/10 - 281s - loss: 0.1200 - val_loss: 0.1745 - lr: 0.0010\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/400\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.15882\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:42:04.652]\n",
      "Saving backup of the training history epoch=13 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:42:04.719]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:42:04.740]\n",
      "train: argmin=12 --> min=0.12\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:42:04.742]\n",
      "val: argmin=11 --> min=0.159\n",
      "\n",
      "10/10 - 281s - loss: 0.1214 - val_loss: 0.1684 - lr: 0.0010\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/400\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15882 to 0.15528, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.015-0.155284.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:46:49.617]\n",
      "Saving backup of the training history epoch=14 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:46:49.679]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:46:49.701]\n",
      "train: argmin=14 --> min=0.116\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:46:49.703]\n",
      "val: argmin=14 --> min=0.155\n",
      "\n",
      "10/10 - 284s - loss: 0.1160 - val_loss: 0.1553 - lr: 0.0010\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 16/400\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15528\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:51:32.017]\n",
      "Saving backup of the training history epoch=15 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:51:32.076]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:51:32.098]\n",
      "train: argmin=15 --> min=0.114\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:51:32.101]\n",
      "val: argmin=14 --> min=0.155\n",
      "\n",
      "10/10 - 281s - loss: 0.1135 - val_loss: 0.1579 - lr: 0.0010\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 17/400\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.15528 to 0.13170, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.017-0.131700.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::12:56:10.767]\n",
      "Saving backup of the training history epoch=16 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::12:56:10.829]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:56:10.851]\n",
      "train: argmin=16 --> min=0.109\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::12:56:10.853]\n",
      "val: argmin=16 --> min=0.132\n",
      "\n",
      "10/10 - 278s - loss: 0.1095 - val_loss: 0.1317 - lr: 0.0010\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 18/400\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.13170 to 0.11727, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.018-0.117272.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:00:43.525]\n",
      "Saving backup of the training history epoch=17 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:00:43.598]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:00:43.620]\n",
      "train: argmin=16 --> min=0.109\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:00:43.624]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 272s - loss: 0.1125 - val_loss: 0.1173 - lr: 0.0010\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 19/400\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:05:14.853]\n",
      "Saving backup of the training history epoch=18 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:05:14.926]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:05:14.949]\n",
      "train: argmin=16 --> min=0.109\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:05:14.951]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 271s - loss: 0.1110 - val_loss: 0.1482 - lr: 0.0010\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 20/400\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:09:46.473]\n",
      "Saving backup of the training history epoch=19 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:09:46.539]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:09:46.563]\n",
      "train: argmin=19 --> min=0.107\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:09:46.567]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 270s - loss: 0.1073 - val_loss: 0.1448 - lr: 0.0010\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 21/400\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:14:18.081]\n",
      "Saving backup of the training history epoch=20 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:14:18.152]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:14:18.175]\n",
      "train: argmin=19 --> min=0.107\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:14:18.177]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 271s - loss: 0.1083 - val_loss: 0.1455 - lr: 0.0010\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 22/400\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:18:49.685]\n",
      "Saving backup of the training history epoch=21 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:18:49.754]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:18:49.778]\n",
      "train: argmin=21 --> min=0.106\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:18:49.780]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 271s - loss: 0.1060 - val_loss: 0.1622 - lr: 0.0010\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 23/400\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:23:21.128]\n",
      "Saving backup of the training history epoch=22 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:23:21.185]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:23:21.209]\n",
      "train: argmin=22 --> min=0.105\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:23:21.211]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 270s - loss: 0.1051 - val_loss: 0.1533 - lr: 0.0010\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 24/400\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:27:52.704]\n",
      "Saving backup of the training history epoch=23 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:27:52.771]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:27:52.795]\n",
      "train: argmin=23 --> min=0.101\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:27:52.797]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 271s - loss: 0.1012 - val_loss: 0.1380 - lr: 0.0010\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 25/400\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:32:24.098]\n",
      "Saving backup of the training history epoch=24 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:32:24.165]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:32:24.189]\n",
      "train: argmin=23 --> min=0.101\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:32:24.192]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 270s - loss: 0.1038 - val_loss: 0.1251 - lr: 0.0010\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 26/400\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:36:55.659]\n",
      "Saving backup of the training history epoch=25 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:36:55.733]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:36:55.756]\n",
      "train: argmin=25 --> min=0.101\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:36:55.758]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 271s - loss: 0.1010 - val_loss: 0.1277 - lr: 0.0010\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 27/400\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11727\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:41:33.469]\n",
      "Saving backup of the training history epoch=26 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:41:33.536]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:41:33.558]\n",
      "train: argmin=26 --> min=0.1\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:41:33.560]\n",
      "val: argmin=17 --> min=0.117\n",
      "\n",
      "10/10 - 277s - loss: 0.1005 - val_loss: 0.1176 - lr: 0.0010\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 28/400\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.11727 to 0.11534, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.028-0.115343.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:46:23.857]\n",
      "Saving backup of the training history epoch=27 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:46:23.917]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:46:23.939]\n",
      "train: argmin=27 --> min=0.098\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:46:23.941]\n",
      "val: argmin=27 --> min=0.115\n",
      "\n",
      "10/10 - 289s - loss: 0.0980 - val_loss: 0.1153 - lr: 0.0010\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 29/400\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11534 to 0.11006, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.029-0.110063.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:51:08.962]\n",
      "Saving backup of the training history epoch=28 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:51:09.017]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:51:09.038]\n",
      "train: argmin=27 --> min=0.098\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:51:09.040]\n",
      "val: argmin=28 --> min=0.11\n",
      "\n",
      "10/10 - 284s - loss: 0.0998 - val_loss: 0.1101 - lr: 0.0010\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 30/400\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.11006 to 0.10968, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.030-0.109682.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::13:55:55.314]\n",
      "Saving backup of the training history epoch=29 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::13:55:55.371]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:55:55.392]\n",
      "train: argmin=29 --> min=0.0942\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::13:55:55.394]\n",
      "val: argmin=29 --> min=0.11\n",
      "\n",
      "10/10 - 285s - loss: 0.0942 - val_loss: 0.1097 - lr: 0.0010\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 31/400\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.10968\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::14:00:38.415]\n",
      "Saving backup of the training history epoch=30 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::14:00:38.483]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::14:00:38.508]\n",
      "train: argmin=29 --> min=0.0942\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::14:00:38.510]\n",
      "val: argmin=29 --> min=0.11\n",
      "\n",
      "10/10 - 282s - loss: 0.0984 - val_loss: 0.1145 - lr: 0.0010\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000774263682681127.\n",
      "Epoch 32/400\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.10968 to 0.10689, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/unet2halfd.II-enc-c208-f08.fold000.1607-939-493.autosaved.032-0.106894.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:110}::[2020-12-14::14:05:21.223]\n",
      "Saving backup of the training history epoch=31 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2halfd/unet2halfd.II-enc-c208-f08.fold000.1607-939-493/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:297}::[2020-12-14::14:05:21.289]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::14:05:21.310]\n",
      "train: argmin=31 --> min=0.0934\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:392}::[2020-12-14::14:05:21.312]\n",
      "val: argmin=31 --> min=0.107\n",
      "\n",
      "10/10 - 282s - loss: 0.0934 - val_loss: 0.1069 - lr: 7.7426e-04\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005994842503189409.\n",
      "Epoch 33/400\n"
     ]
    }
   ],
   "source": [
    "# [manual-input]\n",
    "n_epochs = 400\n",
    "\n",
    "\n",
    "class TrainingFinished(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class FailedToFindBatchSize(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def fit():\n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # [manual-input]\n",
    "        # epochs\n",
    "        initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "    #     initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "    #         epochs=history_cb.last_epoch + 1 + n_epochs,  \n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,  \n",
    "        verbose=2,\n",
    "\n",
    "        # todo change the volume sequence to dinamically load the volume\n",
    "        # because it would allow me to pass just a path string therefore\n",
    "        # making it serializible ==> i will be able to multithread (:\n",
    "        use_multiprocessing=False,   \n",
    "    );\n",
    "    raise TrainingFinished()\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    try:\n",
    "        fit()\n",
    "        \n",
    "    except TrainingFinished:\n",
    "        slack.notify_finished()\n",
    "        \n",
    "    except Exception as ex:\n",
    "                \n",
    "        logger.exception(ex)\n",
    "        \n",
    "        if args.batch_size_mode == Args.BatchSizeMode.try_max_and_fail:\n",
    "            raise ex\n",
    "        \n",
    "        batch_size -= n_gpus\n",
    "        logger.warning(f\"reduced {batch_size=}\")\n",
    "        \n",
    "        if batch_size < n_gpus:\n",
    "            raise FailedToFindBatchSize\n",
    "        \n",
    "        crop_seq_train.batch_size = batch_size\n",
    "        crop_seq_val.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows := 2, ncols := 1, figsize=(2.5 * (sz := 5), nrows * sz), dpi=100)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "hist_display = viz.TrainingHistoryDisplay(\n",
    "    history_cb.history, \n",
    "    model_name=tomo2seg_model.name,\n",
    "    loss_name=model.loss.__name__,\n",
    "    x_axis_mode=(\n",
    "        \"epoch\", \"batch\", \"crop\", \"voxel\", \"time\",\n",
    "    ),\n",
    ").plot(\n",
    "    axs, \n",
    "    with_lr=True,\n",
    "    metrics=(\n",
    "        \"loss\", \n",
    "    ),\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[-1].set_yscale(\"log\")\n",
    "\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"loss\"][0])\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"val_loss\"][0], txt_kwargs=dict(rotation=0))\n",
    "\n",
    "hist_display.fig_.savefig(\n",
    "    tomo2seg_model.model_path / (hist_display.title + \".png\"),\n",
    "    format='png',\n",
    ")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "aborted",
     "timestamp": 1602255923919,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "d-EnhRhrrEGQ"
   },
   "outputs": [],
   "source": [
    "history_cb.dataframe.to_csv(history_cb.csv_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "aborted",
     "timestamp": 1602255923920,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "LQz6HBJss1o4"
   },
   "outputs": [],
   "source": [
    "model.save(tomo2seg_model.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_name = \"train-06-akela90.ipynb\"\n",
    "import os\n",
    "this_dir = os.getcwd()\n",
    "logger.warning(f\"{this_nb_name=} {this_dir=}\")\n",
    "\n",
    "os.system(f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {str(tomo2seg_model.model_path)} --to html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2FW3h3DkQ4XcY6OgH7u/r",
   "collapsed_sections": [
    "EnVqPFS9BNCg",
    "j8e5FhmUaKND",
    "nJtppItnKn5G"
   ],
   "mount_file_id": "1LuEITv9j0lLf8Z418J3a94SjEZ8GvKvI",
   "name": "dryrun-02.ipynb",
   "provenance": [
    {
     "file_id": "1NiX28EcC_FVOYCJL4usp7n5iQ2x3aXIm",
     "timestamp": 1602152789440
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
