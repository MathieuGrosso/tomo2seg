{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnVqPFS9BNCg"
   },
   "source": [
    "\n",
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JEHjvuBBIab"
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "executionInfo": {
     "elapsed": 1970,
     "status": "ok",
     "timestamp": 1602255916978,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "KTMgQv07JkgY",
    "outputId": "69cd78fc-f0f1-46f6-f1d8-63b99d55eaae"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1967,
     "status": "ok",
     "timestamp": 1602255916979,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "m7qeyEdDT3Hl"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "import logging\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import sys\n",
    "from typing import *\n",
    "import time\n",
    "import yaml\n",
    "from yaml import YAMLObject\n",
    "\n",
    "import humanize\n",
    "from matplotlib import pyplot as plt, cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymicro.file import file_utils\n",
    "import tensorflow as tf\n",
    "from numpy.random import RandomState\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks as keras_callbacks\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics as keras_metrics\n",
    "\n",
    "from tomo2seg import modular_unet\n",
    "from tomo2seg.logger import logger\n",
    "from tomo2seg import data, viz\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.metadata import Metadata\n",
    "from tomo2seg.volume_sequence import (\n",
    "    MetaCrop3DGenerator, VolumeCropSequence,\n",
    "    UniformGridPosition, SequentialGridPosition,\n",
    "    ET3DUniformCuboidAlmostEverywhere, ET3DConstantEverywhere, \n",
    "    GTUniformEverywhere, GTConstantEverywhere, \n",
    "    VSConstantEverywhere, VSUniformEverywhere\n",
    ")\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import callbacks as tomo2seg_callbacks\n",
    "from tomo2seg import losses as tomo2seg_losses\n",
    "from tomo2seg.schedule import ComposedSchedule, LogSpaceSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-4-f5dd942bd525>:<module>:005}::[2020-11-26::12:50:47.459]\n",
      "runid=1606391447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "random_state = np.random.RandomState(random_state)\n",
    "runid = int(time.time())\n",
    "# runid = 1606238421\n",
    "logger.info(f\"{runid=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-5-7a7728cf1275>:<module>:001}::[2020-11-26::12:50:47.514]\n",
      "tf.__version__='2.2.0'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-5-7a7728cf1275>:<module>:002}::[2020-11-26::12:50:47.592]\n",
      "Num GPUs Available: 2\n",
      "This should be 2 on R790-TOMO.\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-5-7a7728cf1275>:<module>:003}::[2020-11-26::12:50:47.957]\n",
      "Both here should return 2 devices...\n",
      "tf.config.list_physical_devices('GPU')=[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "tf.config.list_logical_devices('GPU')=[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n",
      "\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "logger.debug(f\"{tf.__version__=}\")\n",
    "logger.info(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\\nThis should be 2 on R790-TOMO.\")\n",
    "logger.debug(f\"Both here should return 2 devices...\\n{tf.config.list_physical_devices('GPU')=}\\n{tf.config.list_logical_devices('GPU')=}\")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "strategy = tf.distribute.MirroredStrategy()  \n",
    "# strategy = tf.distribute.MirroredStrategy(devices=[\"\"])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8e5FhmUaKND"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-6-f863a788f16a>:<module>:010}::[2020-11-26::12:50:48.046]\n",
      "volume_name='PA66GF30' volume_version='v1' labels_version='refined3'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_V1_REDUCED as VOLUME_NAME_VERSION,\n",
    "    VOLUME_COMPOSITE_V1_LABELS_REFINED3 as LABELS_VERSION\n",
    ")\n",
    "\n",
    "volume_name, volume_version = VOLUME_NAME_VERSION\n",
    "labels_version = LABELS_VERSION\n",
    "\n",
    "logger.info(f\"{volume_name=} {volume_version=} {labels_version=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1602255917946,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "4CfP7usu2VKr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{data.py:with_check:237}::[2020-11-26::12:50:48.102]\n",
      "vol=Volume(name='PA66GF30', version='v1', _metadata=None)\n",
      "\n",
      "ERROR::tomo2seg::{data.py:with_check:255}::[2020-11-26::12:50:48.104]\n",
      "Missing file: /home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.labels.raw\n",
      "\n",
      "WARNING::tomo2seg::{data.py:with_check:259}::[2020-11-26::12:50:48.105]\n",
      "Missing file: /home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.weights.raw\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:metadata:194}::[2020-11-26::12:50:48.106]\n",
      "Loading metadata from `/home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.metadata.yml`.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:007}::[2020-11-26::12:50:48.111]\n",
      "volume=Volume(name='PA66GF30', version='v1', _metadata=Volume.Metadata(dimensions=[1300, 1040, 1900], dtype='uint8', labels=[0, 1, 2], labels_names={0: 'matrix', 1: 'fiber', 2: 'porosity'}, set_partitions={'train': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [0, 1300], 'alias': 'train'}, 'val': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [1600, 1900], 'alias': 'val'}, 'test': {'x_range': [0, 1300], 'y_range': [0, 1040], 'z_range': [1300, 1600], 'alias': 'test'}}))\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:024}::[2020-11-26::12:50:48.112]\n",
      "Loading data from disk.\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:028}::[2020-11-26::12:50:58.148]\n",
      "voldata.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:032}::[2020-11-26::12:50:58.150]\n",
      "voldata_train.shape=(1300, 1040, 1300) voldata_val.shape=(1300, 1040, 300)\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:038}::[2020-11-26::12:51:01.170]\n",
      "vollabels.shape=(1300, 1040, 1900)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-7-81bf4074b174>:<module>:042}::[2020-11-26::12:51:01.172]\n",
      "vollabels_train.shape=(1300, 1040, 1300) vollabels_val.shape=(1300, 1040, 300)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metadata/paths objects\n",
    "\n",
    "## Volume\n",
    "volume = Volume.with_check(\n",
    "    name=volume_name, version=volume_version\n",
    ")\n",
    "logger.info(f\"{volume=}\")\n",
    "\n",
    "n_classes = len(volume.metadata.labels)\n",
    "\n",
    "def _read_raw(path_: Path, volume_: Volume): \n",
    "    # from pymicro\n",
    "    return file_utils.HST_read(\n",
    "        str(path_),  # it doesn't accept paths...\n",
    "        # pre-loaded kwargs\n",
    "        autoparse_filename=False,  # the file names are not properly formatted\n",
    "        data_type=volume.metadata.dtype,\n",
    "        dims=volume.metadata.dimensions,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "read_raw = partial(_read_raw, volume_=volume)\n",
    "\n",
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "## Data\n",
    "voldata = read_raw(volume.data_path) / 255  # normalize\n",
    "logger.debug(f\"{voldata.shape=}\")\n",
    "\n",
    "voldata_train = volume.train_partition.get_volume_partition(voldata)\n",
    "voldata_val = volume.val_partition.get_volume_partition(voldata)\n",
    "logger.debug(f\"{voldata_train.shape=} {voldata_val.shape=}\")\n",
    "\n",
    "del voldata\n",
    "\n",
    "## Labels\n",
    "vollabels = read_raw(volume.versioned_labels_path(labels_version))\n",
    "logger.debug(f\"{vollabels.shape=}\")\n",
    "\n",
    "vollabels_train = volume.train_partition.get_volume_partition(vollabels)\n",
    "vollabels_val = volume.val_partition.get_volume_partition(vollabels)\n",
    "logger.debug(f\"{vollabels_train.shape=} {vollabels_val.shape=}\")\n",
    "\n",
    "del vollabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already deleted (:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "except NameError:\n",
    "    print(\"already deleted (:\")\n",
    "else:\n",
    "    del tomo2seg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [],
   "source": [
    "# crop_shape = (256, 256, 1)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "# bigger crops will have less border effects (?)\n",
    "crop_shape = (320, 320, 1)  # multiple of 16 (requirement of a 4-level u-net)\n",
    "\n",
    "model_master_name = \"unet2d\"\n",
    "model_version = \"vanilla01-f16\"\n",
    "model_factory_function = modular_unet.u_net\n",
    "model_factory_kwargs = {\n",
    "    **modular_unet.kwargs_vanilla01,\n",
    "    **dict(\n",
    "        convlayer=modular_unet.ConvLayer.conv2d,\n",
    "        input_shape = crop_shape,\n",
    "        output_channels=n_classes,\n",
    "        nb_filters_0 = 16,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 590,
     "status": "ok",
     "timestamp": 1602255973613,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "lnPHivbmBhpY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:005}::[2020-11-26::12:51:01.402]\n",
      "Creating a Tomo2SegModel.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:020}::[2020-11-26::12:51:01.404]\n",
      "tomo2seg_model=Model(master_name='unet2d', version='vanilla01-f16', fold=0, runid=1606391447, factory_function='tomo2seg.modular_unet.u_net', factory_kwargs={'depth': 4, 'sigma_noise': 0, 'updown_conv_sampling': False, 'unet_block_kwargs': {'kernel_size': 3, 'res': True, 'batch_norm': False, 'dropout': 0}, 'unet_down_kwargs': {}, 'unet_up_kwargs': {}, 'convlayer': <ConvLayer.conv2d: 0>, 'input_shape': (320, 320, 1), 'output_channels': 3, 'nb_filters_0': 16})\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:022}::[2020-11-26::12:51:01.405]\n",
      "Creating the Keras model.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:026}::[2020-11-26::12:51:01.407]\n",
      "Instantiating a new model with model_factory_function=u_net\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:036}::[2020-11-26::12:51:02.819]\n",
      "Compiling the model.\n",
      "\n",
      "WARNING:tensorflow:From /home/users/jcasagrande/projects/tomo2seg/condaenv/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/assets\n",
      "INFO::tomo2seg::{<ipython-input-10-2291d74903ed>:<module>:067}::[2020-11-26::12:51:08.865]\n",
      "Check the summary and the figure of the model in the following locations:\n",
      "/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/summary.txt\n",
      "/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/architecture.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tomo2seg_model\n",
    "    \n",
    "except NameError:\n",
    "    logger.info(\"Creating a Tomo2SegModel.\")\n",
    "    \n",
    "    tomo2seg_model = Tomo2SegModel(\n",
    "        model_master_name, \n",
    "        model_version, \n",
    "        runid=runid,\n",
    "        factory_function=model_factory_function,\n",
    "        factory_kwargs=model_factory_kwargs,\n",
    "    )\n",
    "                \n",
    "else:\n",
    "    logger.warning(\"The model is already defined. To create a new one: `del tomo2seg_model`\")\n",
    "\n",
    "finally:\n",
    "    \n",
    "    logger.info(f\"{tomo2seg_model=}\")\n",
    "    \n",
    "logger.info(\"Creating the Keras model.\")\n",
    "\n",
    "with strategy.scope():\n",
    "    if not tomo2seg_model.autosaved_model_path.exists():\n",
    "        logger.info(f\"Instantiating a new model with model_factory_function={model_factory_function.__name__}\")\n",
    "      \n",
    "        model = model_factory_function(\n",
    "            name=tomo2seg_model.name,\n",
    "            **model_factory_kwargs\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(\"An autosaved model already exists, loading it instead of creating a new one!\")\n",
    "        model = keras.models.load_model(tomo2seg_model.autosaved_model_path_str, compile=False)\n",
    "\n",
    "    logger.info(\"Compiling the model.\")\n",
    "\n",
    "    # using the avg jaccard is dangerous if one of the classes is too\n",
    "    # underrepresented because it's jaccard will be unstable\n",
    "    loss = tomo2seg_losses.jaccard2_flat\n",
    "\n",
    "    optimizer = optimizers.Adam(lr=.003)\n",
    "    metrics = [\n",
    "#         tomo2seg_losses.jaccard2_macro_avg,\n",
    "#         keras_metrics.Accuracy(),\n",
    "#     ] + [\n",
    "#         tomo2seg_losses.Jaccard2(class_idx)\n",
    "#         for class_idx in range(n_classes)\n",
    "    ]\n",
    "\n",
    "    model.compile(\n",
    "        loss=loss, \n",
    "        optimizer=optimizer,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    model.save(tomo2seg_model.model_path)\n",
    "\n",
    "    # write the model summary in a file\n",
    "    with tomo2seg_model.summary_path.open(\"w\") as f:\n",
    "        def print_to_txt(line):\n",
    "            f.writelines([line + \"\\n\"])\n",
    "        model.summary(print_fn=print_to_txt, line_length=140)\n",
    "\n",
    "    # same for the architecture\n",
    "    utils.plot_model(model, show_shapes=True, to_file=tomo2seg_model.architecture_plot_path);\n",
    "\n",
    "    logger.info(f\"Check the summary and the figure of the model in the following locations:\\n{tomo2seg_model.summary_path}\\n{tomo2seg_model.architecture_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnsQ7lX0bVRh"
   },
   "source": [
    "# Data crop sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-11-19fcba377971>:<module>:006}::[2020-11-26::12:51:08.985]\n",
      "batch_size_per_replica=8\n",
      "n_replicas=2\n",
      "batch_size=16\n",
      "common_random_state=143\n",
      "crop_shape=(320, 320, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size_per_replica = 8\n",
    "batch_size = batch_size_per_replica * (n_replicas := strategy.num_replicas_in_sync)\n",
    "\n",
    "common_random_state = 143\n",
    "\n",
    "logger.info(f\"{batch_size_per_replica=}\\n{n_replicas=}\\n{batch_size=}\\n{common_random_state=}\\n{crop_shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:145}::[2020-11-26::12:51:09.053]\n",
      "Built UniformGridPosition from volume_shape=(1300, 1040, 1300) and crop_shape=(320, 320, 1) ==> {'x_range': (0, 981), 'y_range': (0, 721), 'z_range': (0, 1300)}\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.055]\n",
      "Initializing ET3DConstantEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.056]\n",
      "Initializing GTUniformEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.057]\n",
      "Initializing VSUniformEverywhere with a UniformGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:818}::[2020-11-26::12:51:09.058]\n",
      "Initializing VolumeCropSequence.\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:__post_init__:846}::[2020-11-26::12:51:09.060]\n",
      "A meta crops history file path was given but it still doesn't exist. Writing csv headers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = voldata_train\n",
    "labels = vollabels_train\n",
    "volume_shape = data.shape\n",
    "labels_list = volume.metadata.labels\n",
    "\n",
    "crop_seq_train = VolumeCropSequence(\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    labels=labels_list,\n",
    "    meta_crop_generator=MetaCrop3DGenerator(\n",
    "        volume_shape=volume_shape,\n",
    "        crop_shape=crop_shape,\n",
    "        x0y0z0_generator=(\n",
    "            grid_pos_gen := UniformGridPosition.build_from_volume_crop_shapes(\n",
    "                volume_shape=volume_shape, \n",
    "                crop_shape=crop_shape,\n",
    "                random_state=RandomState(common_random_state),\n",
    "            )\n",
    "        ),\n",
    "        # it is too slow\n",
    "        et_field=ET3DConstantEverywhere.build_no_displacement(grid_position_generator=grid_pos_gen),\n",
    "#         et_field=ET3DUniformCuboidAlmostEverywhere.build_half_voxel_cuboid(\n",
    "#             crop_shape=crop_shape,\n",
    "#             crop_source_volume_shape=volume_shape,\n",
    "#             spline_order=1,  # linear interpolation\n",
    "#             grid_position_generator=grid_pos_gen,\n",
    "#             random_state=RandomState(common_random_state),\n",
    "#         ),\n",
    "        gt_field=GTUniformEverywhere.build_2d(\n",
    "            random_state=RandomState(common_random_state),\n",
    "            grid_position_generator=grid_pos_gen,\n",
    "        ),\n",
    "        vs_field=VSUniformEverywhere.build_plus_or_mines(\n",
    "            shift=1. / 255 / 2,  # half a value to both sides +/-\n",
    "            grid_position_generator=grid_pos_gen,\n",
    "            random_state=RandomState(common_random_state),\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    # this volume cropper only returns random crops, \n",
    "    #so the number of crops per epoch/batch is w/e i want\n",
    "#     epoch_size=5,\n",
    "    epoch_size=1,\n",
    "    meta_crops_hist_path=tomo2seg_model.train_metacrop_history_path,  # todo add a new path to the model and save this\n",
    "    debug__no_data_check=True,  # remove me!\n",
    "    output_as_2d=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{volume_sequence.py:build_min_overlap:202}::[2020-11-26::12:51:09.132]\n",
      "Building SequentialGridPosition with minimal overlap (smallest n_steps in each directions) n_steps={'n_steps_x': 5, 'n_steps_y': 4, 'n_steps_z': 300}.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:build_min_overlap:205}::[2020-11-26::12:51:09.134]\n",
      "n_steps_kwargs={'n_steps_z': 60} was given --> effective n_steps={'n_steps_x': 5, 'n_steps_y': 4, 'n_steps_z': 60}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:build_from_volume_crop_shapes:145}::[2020-11-26::12:51:09.135]\n",
      "Built SequentialGridPosition from volume_shape=(1300, 1040, 300) and crop_shape=(320, 320, 1) ==> {'x_range': (0, 981), 'y_range': (0, 721), 'z_range': (0, 300)}\n",
      "\n",
      "INFO::tomo2seg::{volume_sequence.py:__post_init__:184}::[2020-11-26::12:51:09.139]\n",
      "The SequentialGridPosition has len(self.positions)=1200 different positions (therefore crops).\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.140]\n",
      "Initializing ET3DConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.140]\n",
      "Initializing GTConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:233}::[2020-11-26::12:51:09.141]\n",
      "Initializing VSConstantEverywhere with a SequentialGridPosition.\n",
      "The {x, y, z}_range values will be overwritten.\n",
      "\n",
      "DEBUG::tomo2seg::{volume_sequence.py:__post_init__:818}::[2020-11-26::12:51:09.142]\n",
      "Initializing VolumeCropSequence.\n",
      "\n",
      "WARNING::tomo2seg::{volume_sequence.py:__post_init__:850}::[2020-11-26::12:51:09.143]\n",
      "No meta crops history file path given. The randomly generated crops will not be saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# val volume\n",
    "\n",
    "data = voldata_val\n",
    "labels = vollabels_val\n",
    "volume_shape = data.shape\n",
    "labels_list = volume.metadata.labels\n",
    "\n",
    "crop_seq_val = VolumeCropSequence(\n",
    "    # data source\n",
    "    data_volume=data,\n",
    "    labels_volume=labels,\n",
    "    labels=labels_list,\n",
    "    \n",
    "    # data augmentation\n",
    "    meta_crop_generator=MetaCrop3DGenerator(\n",
    "        volume_shape=volume_shape,\n",
    "        crop_shape=crop_shape,\n",
    "        x0y0z0_generator=(\n",
    "            grid_pos_gen := SequentialGridPosition.build_min_overlap(\n",
    "                volume_shape=volume_shape, crop_shape=crop_shape,\n",
    "                # reduce the total number of crops\n",
    "                n_steps_z=60,\n",
    "            )\n",
    "#             grid_pos_gen := SequentialGridPosition.build_from_volume_crop_shapes(\n",
    "#                 volume_shape=volume_shape, crop_shape=crop_shape,\n",
    "#                 n_steps_x=2, n_steps_y=2, n_steps_z=200,\n",
    "#             )\n",
    "        ),\n",
    "        et_field=ET3DConstantEverywhere.build_no_displacement(grid_position_generator=grid_pos_gen),\n",
    "        gt_field=GTConstantEverywhere.build_gt2d_identity(grid_position_generator=grid_pos_gen),\n",
    "        vs_field=VSConstantEverywhere.build_no_shift(grid_position_generator=grid_pos_gen),\n",
    "    ),\n",
    "    \n",
    "    # others\n",
    "    batch_size=batch_size,\n",
    "    epoch_size=len(grid_pos_gen),  # go through all the crops in validation    \n",
    "    meta_crops_hist_path=None,  # todo add a new path to the model and save this\n",
    "    debug__no_data_check=True,  # remove me!\n",
    "    output_as_2d=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsccmAxOX7v"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8834,
     "status": "aborted",
     "timestamp": 1602255923910,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "zRp2b17np-48"
   },
   "outputs": [],
   "source": [
    "autosave_cb = keras_callbacks.ModelCheckpoint(\n",
    "    tomo2seg_model.autosaved_model_path_str, \n",
    "    monitor=\"val_loss\", \n",
    "    verbose=2, \n",
    "    save_best_only=True, \n",
    "    mode=\"auto\",\n",
    ")\n",
    "\n",
    "# todo load if it already exists\n",
    "try:\n",
    "    history_cb\n",
    "    \n",
    "except NameError:\n",
    "    history_cb = tomo2seg_callbacks.History(\n",
    "        optimizer=model.optimizer,\n",
    "        crop_seq_train=crop_seq_train,\n",
    "        crop_seq_val=crop_seq_val,\n",
    "        backup=1,\n",
    "        csv_path=tomo2seg_model.history_path,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"The history callback already exists!\")\n",
    "    \n",
    "    history_df = history_cb.dataframe\n",
    "\n",
    "    try:\n",
    "        history_df_temp = pd.read_csv(tomo2seg_model.history_path)\n",
    "        # keep the longest one\n",
    "        history_df = history_df if history_df.shape[0] >= history_df_temp.shape[0] else history_df_temp\n",
    "        del history_df_temp\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.info(\"History hasn't been saved yet.\")\n",
    "        \n",
    "finally:\n",
    "    # make sure the correct objects are linked \n",
    "    history_cb.model = model\n",
    "    history_cb.crop_seq_train = crop_seq_train\n",
    "    history_cb.crop_seq_val = crop_seq_val\n",
    "    # todo do the same with other objs in history_cb\n",
    "    \n",
    "history_plot_cb = tomo2seg_callbacks.HistoryPlot(\n",
    "    history_callback=history_cb,\n",
    "    save_path=tomo2seg_model.train_history_plot_wip_path\n",
    ")\n",
    "\n",
    "early_stop_cb = keras_callbacks.EarlyStopping(  # todo modify the early stopping to take more conditions (don't stop too early before it doesnt break the jaccard2=.32)\n",
    "    monitor='val_loss', \n",
    "    min_delta=.1 / 100, \n",
    "    patience=40, \n",
    "    verbose=2, \n",
    "    mode='auto',\n",
    "    baseline=.71,  # 0th-order classifier\n",
    "    restore_best_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kYnLzlFdDeY"
   },
   "source": [
    "# Summary before training\n",
    "\n",
    "stuff that i use after the training but i want it to appear in the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Volume slices\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "## Generator samples\n",
    "\n",
    "todo put this back to work\n",
    "\n",
    "# Learning rate range test\n",
    "\n",
    "todo put this back to work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuEmT2AZODXi"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triangular log lr schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-11-26::12:51:09.251]\n",
      "LogSpaceSchedule ==> self.n=50\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-11-26::12:51:09.252]\n",
      "LogSpaceSchedule ==> self.n=70\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-11-26::12:51:09.253]\n",
      "LogSpaceSchedule ==> self.n=20\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:071}::[2020-11-26::12:51:09.254]\n",
      "LogSpaceSchedule ==> self.n=60\n",
      "\n",
      "INFO::tomo2seg::{schedule.py:__post_init__:107}::[2020-11-26::12:51:09.255]\n",
      "ComposedSchedule ==> self.n=200\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-15-e59a14ab1e38>:<module>:019}::[2020-11-26::12:51:09.256]\n",
      "(0, 200)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### from tensorflow.keras import backend as K\n",
    "# lr = 0.001\n",
    "# K.set_value(model.optimizer.learning_rate, lr)\n",
    "\n",
    "lr_schedule_cb = keras_callbacks.LearningRateScheduler(\n",
    "    schedule= (schedule := ComposedSchedule(\n",
    "        offset_epoch=0,\n",
    "        sub_schedules=[\n",
    "            LogSpaceSchedule(0, wait=10, start=-5, stop=-4, n_between_scales=38),  # DECREASE THE LENGTH HERE!!!!!!!\n",
    "            LogSpaceSchedule(50, wait=10, start=-4, stop=-5, n_between_scales=58),\n",
    "            LogSpaceSchedule(120, wait=0, start=-5, stop=-4, n_between_scales=18),\n",
    "            LogSpaceSchedule(140, wait=0, start=-4, stop=-5, n_between_scales=58),\n",
    "#             LogSpaceSchedule(40, wait=, start=-3, stop=-4, n_between_scales=18),\n",
    "        ]\n",
    "    )),\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.info(f\"{schedule.range}\")\n",
    "\n",
    "crop_seq_train.epoch_size = 10\n",
    "\n",
    "callbacks = [\n",
    "    keras_callbacks.TerminateOnNaN(),\n",
    "    autosave_cb,\n",
    "    history_cb,\n",
    "    history_plot_cb,\n",
    "    lr_schedule_cb,\n",
    "    early_stop_cb,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomo2seg import slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 1/200\n",
      "INFO:tensorflow:batch_all_reduce: 56 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 56 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57329, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::12:57:17.871]\n",
      "Saving backup of the training history epoch=0 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:103}::[2020-11-26::12:57:17.911]\n",
      "epoch=0 is too early to plot something.\n",
      "\n",
      "ERROR::tomo2seg::{callbacks.py:on_epoch_end:144}::[2020-11-26::12:57:18.003]\n",
      "AssertionError occurred while trying to plot the history.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/callbacks.py\", line 115, in on_epoch_end\n",
      "    hist_display = viz.TrainingHistoryDisplay(\n",
      "  File \"<string>\", line 9, in __init__\n",
      "  File \"/home/users/jcasagrande/projects/tomo2seg/tomo2seg/viz.py\", line 254, in __post_init__\n",
      "    assert len(x_axis) > 1, \"You don't have enough epochs to plot. Go to the gym and call me later.\"\n",
      "AssertionError: You don't have enough epochs to plot. Go to the gym and call me later.\n",
      "10/10 - 165s - loss: 0.6172 - val_loss: 0.5733 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.57329 to 0.43760, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:00:02.389]\n",
      "Saving backup of the training history epoch=1 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{callbacks.py:on_epoch_end:103}::[2020-11-26::13:00:02.422]\n",
      "epoch=1 is too early to plot something.\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:00:02.473]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:00:02.503]\n",
      "train: argmin=1 --> min=0.519\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:00:02.506]\n",
      "val: argmin=1 --> min=0.438\n",
      "\n",
      "10/10 - 165s - loss: 0.5186 - val_loss: 0.4376 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43760 to 0.32591, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:02:47.882]\n",
      "Saving backup of the training history epoch=2 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:02:47.981]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:02:48.015]\n",
      "train: argmin=2 --> min=0.371\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:02:48.018]\n",
      "val: argmin=2 --> min=0.326\n",
      "\n",
      "10/10 - 165s - loss: 0.3711 - val_loss: 0.3259 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32591 to 0.30876, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:05:33.088]\n",
      "Saving backup of the training history epoch=3 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:05:33.175]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:05:33.205]\n",
      "train: argmin=3 --> min=0.31\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:05:33.208]\n",
      "val: argmin=3 --> min=0.309\n",
      "\n",
      "10/10 - 165s - loss: 0.3101 - val_loss: 0.3088 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30876 to 0.30566, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:08:18.276]\n",
      "Saving backup of the training history epoch=4 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:08:18.364]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:08:18.395]\n",
      "train: argmin=4 --> min=0.303\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:08:18.398]\n",
      "val: argmin=4 --> min=0.306\n",
      "\n",
      "10/10 - 165s - loss: 0.3027 - val_loss: 0.3057 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30566 to 0.30453, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:11:03.462]\n",
      "Saving backup of the training history epoch=5 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:11:03.555]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:11:03.587]\n",
      "train: argmin=5 --> min=0.301\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:11:03.590]\n",
      "val: argmin=5 --> min=0.305\n",
      "\n",
      "10/10 - 165s - loss: 0.3012 - val_loss: 0.3045 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.30453 to 0.30394, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:13:48.943]\n",
      "Saving backup of the training history epoch=6 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:13:49.032]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:13:49.062]\n",
      "train: argmin=6 --> min=0.3\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:13:49.066]\n",
      "val: argmin=6 --> min=0.304\n",
      "\n",
      "10/10 - 165s - loss: 0.3002 - val_loss: 0.3039 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.30394 to 0.30354, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:16:34.300]\n",
      "Saving backup of the training history epoch=7 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:16:34.397]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:16:34.430]\n",
      "train: argmin=7 --> min=0.299\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:16:34.433]\n",
      "val: argmin=7 --> min=0.304\n",
      "\n",
      "10/10 - 165s - loss: 0.2989 - val_loss: 0.3035 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.30354 to 0.30325, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:19:20.060]\n",
      "Saving backup of the training history epoch=8 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:19:20.155]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:19:20.189]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:19:20.193]\n",
      "val: argmin=8 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2928 - val_loss: 0.3033 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30325 to 0.30305, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:22:05.571]\n",
      "Saving backup of the training history epoch=9 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:22:05.672]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:22:05.704]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:22:05.707]\n",
      "val: argmin=9 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2998 - val_loss: 0.3031 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.30305 to 0.30292, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:24:50.646]\n",
      "Saving backup of the training history epoch=10 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:24:50.734]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:24:50.763]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:24:50.766]\n",
      "val: argmin=10 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2979 - val_loss: 0.3029 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 1.0608183551394482e-05.\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.30292 to 0.30281, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:27:35.758]\n",
      "Saving backup of the training history epoch=11 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:27:35.830]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:27:35.855]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:27:35.858]\n",
      "val: argmin=11 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.3001 - val_loss: 0.3028 - lr: 1.0608e-05\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 1.1253355826007646e-05.\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.30281 to 0.30274, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:30:21.176]\n",
      "Saving backup of the training history epoch=12 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:30:21.259]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:30:21.290]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:30:21.293]\n",
      "val: argmin=12 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2965 - val_loss: 0.3027 - lr: 1.1253e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 1.1937766417144358e-05.\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30274 to 0.30268, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:33:06.356]\n",
      "Saving backup of the training history epoch=13 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:33:06.517]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:33:06.549]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:33:06.553]\n",
      "val: argmin=13 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2977 - val_loss: 0.3027 - lr: 1.1938e-05\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 1.2663801734674022e-05.\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.30268 to 0.30265, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:35:51.681]\n",
      "Saving backup of the training history epoch=14 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:35:51.785]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:35:51.818]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:35:51.822]\n",
      "val: argmin=14 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2985 - val_loss: 0.3026 - lr: 1.2664e-05\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.3433993325988988e-05.\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.30265 to 0.30262, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:38:37.001]\n",
      "Saving backup of the training history epoch=15 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:38:37.188]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:38:37.221]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:38:37.225]\n",
      "val: argmin=15 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2956 - val_loss: 0.3026 - lr: 1.3434e-05\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.4251026703029993e-05.\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.30262 to 0.30260, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:41:22.829]\n",
      "Saving backup of the training history epoch=16 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:41:22.919]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:41:22.951]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:41:22.954]\n",
      "val: argmin=16 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2973 - val_loss: 0.3026 - lr: 1.4251e-05\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.511775070615663e-05.\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.30260 to 0.30259, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:44:08.311]\n",
      "Saving backup of the training history epoch=17 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:44:08.399]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:44:08.430]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:44:08.433]\n",
      "val: argmin=17 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2945 - val_loss: 0.3026 - lr: 1.5118e-05\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.603718743751331e-05.\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.30259 to 0.30258, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:46:54.063]\n",
      "Saving backup of the training history epoch=18 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:46:54.158]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:46:54.189]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:46:54.192]\n",
      "val: argmin=18 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2998 - val_loss: 0.3026 - lr: 1.6037e-05\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.7012542798525893e-05.\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.30258 to 0.30257, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:49:39.057]\n",
      "Saving backup of the training history epoch=19 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:49:39.163]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:49:39.196]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:49:39.199]\n",
      "val: argmin=19 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2962 - val_loss: 0.3026 - lr: 1.7013e-05\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.80472176682717e-05.\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30257 to 0.30256, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:52:24.877]\n",
      "Saving backup of the training history epoch=20 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:52:24.968]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:52:24.999]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:52:25.002]\n",
      "val: argmin=20 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2955 - val_loss: 0.3026 - lr: 1.8047e-05\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.9144819761699577e-05.\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.30256 to 0.30256, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:55:11.007]\n",
      "Saving backup of the training history epoch=21 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:55:11.101]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:55:11.132]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:55:11.135]\n",
      "val: argmin=21 --> min=0.303\n",
      "\n",
      "10/10 - 165s - loss: 0.2989 - val_loss: 0.3026 - lr: 1.9145e-05\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 2.0309176209047348e-05.\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.30256 to 0.30255, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::13:57:55.900]\n",
      "Saving backup of the training history epoch=22 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::13:57:55.994]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:57:56.025]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::13:57:56.028]\n",
      "val: argmin=22 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2989 - val_loss: 0.3025 - lr: 2.0309e-05\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 2.1544346900318823e-05.\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.30255 to 0.30254, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:00:40.861]\n",
      "Saving backup of the training history epoch=23 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:00:40.958]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:00:40.989]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:00:40.993]\n",
      "val: argmin=23 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2991 - val_loss: 0.3025 - lr: 2.1544e-05\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 2.2854638641349885e-05.\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.30254 to 0.30254, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:03:25.773]\n",
      "Saving backup of the training history epoch=24 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:03:25.868]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:03:25.899]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:03:25.903]\n",
      "val: argmin=24 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2978 - val_loss: 0.3025 - lr: 2.2855e-05\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 2.424462017082331e-05.\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.30254 to 0.30253, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:06:10.636]\n",
      "Saving backup of the training history epoch=25 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:06:10.726]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:06:10.757]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:06:10.760]\n",
      "val: argmin=25 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2938 - val_loss: 0.3025 - lr: 2.4245e-05\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 2.571913809059347e-05.\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.30253 to 0.30253, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:08:55.433]\n",
      "Saving backup of the training history epoch=26 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:08:55.524]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:08:55.555]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:08:55.559]\n",
      "val: argmin=26 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.3007 - val_loss: 0.3025 - lr: 2.5719e-05\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 2.7283333764867696e-05.\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.30253 to 0.30252, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:11:40.485]\n",
      "Saving backup of the training history epoch=27 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:11:40.577]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:11:40.608]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:11:40.611]\n",
      "val: argmin=27 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2952 - val_loss: 0.3025 - lr: 2.7283e-05\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 2.8942661247167517e-05.\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.30252 to 0.30251, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:14:25.136]\n",
      "Saving backup of the training history epoch=28 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:14:25.224]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:14:25.254]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:14:25.257]\n",
      "val: argmin=28 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2980 - val_loss: 0.3025 - lr: 2.8943e-05\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.07029062975785e-05.\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.30251 to 0.30251, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:17:09.622]\n",
      "Saving backup of the training history epoch=29 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:17:09.717]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:17:09.748]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:17:09.751]\n",
      "val: argmin=29 --> min=0.303\n",
      "\n",
      "10/10 - 164s - loss: 0.2971 - val_loss: 0.3025 - lr: 3.0703e-05\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.257020655659783e-05.\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.30251 to 0.30250, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:19:55.003]\n",
      "Saving backup of the training history epoch=30 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:20:11.273]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:20:11.311]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:20:11.314]\n",
      "val: argmin=30 --> min=0.302\n",
      "\n",
      "10/10 - 188s - loss: 0.2951 - val_loss: 0.3025 - lr: 3.2570e-05\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 3.455107294592218e-05.\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.30250 to 0.30248, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:23:02.840]\n",
      "Saving backup of the training history epoch=31 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:23:02.936]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:23:02.967]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:23:02.970]\n",
      "val: argmin=31 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.2955 - val_loss: 0.3025 - lr: 3.4551e-05\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 3.665241237079626e-05.\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.30248 to 0.30247, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:25:47.570]\n",
      "Saving backup of the training history epoch=32 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:25:47.656]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:25:47.684]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:25:47.687]\n",
      "val: argmin=32 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.2950 - val_loss: 0.3025 - lr: 3.6652e-05\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 3.888155180308085e-05.\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.30247 to 0.30245, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:28:32.617]\n",
      "Saving backup of the training history epoch=33 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:28:32.732]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:28:32.764]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:28:32.767]\n",
      "val: argmin=33 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.2963 - val_loss: 0.3024 - lr: 3.8882e-05\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 4.124626382901348e-05.\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.30245 to 0.30239, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:31:17.591]\n",
      "Saving backup of the training history epoch=34 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:31:17.691]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:31:17.723]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:31:17.726]\n",
      "val: argmin=34 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.2989 - val_loss: 0.3024 - lr: 4.1246e-05\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 4.37547937507418e-05.\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.30239 to 0.30230, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:34:02.630]\n",
      "Saving backup of the training history epoch=35 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:34:02.730]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:34:02.760]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:34:02.763]\n",
      "val: argmin=35 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.3009 - val_loss: 0.3023 - lr: 4.3755e-05\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 4.641588833612782e-05.\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.30230 to 0.30213, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:36:47.823]\n",
      "Saving backup of the training history epoch=36 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:36:47.920]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:36:47.951]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:36:47.955]\n",
      "val: argmin=36 --> min=0.302\n",
      "\n",
      "10/10 - 164s - loss: 0.2928 - val_loss: 0.3021 - lr: 4.6416e-05\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 4.9238826317067415e-05.\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.30213 to 0.30069, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:39:32.389]\n",
      "Saving backup of the training history epoch=37 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:39:32.486]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:39:32.517]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:39:32.521]\n",
      "val: argmin=37 --> min=0.301\n",
      "\n",
      "10/10 - 164s - loss: 0.2969 - val_loss: 0.3007 - lr: 4.9239e-05\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 5.2233450742668436e-05.\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.30069 to 0.30069, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:42:17.876]\n",
      "Saving backup of the training history epoch=38 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:42:17.988]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:42:18.022]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:42:18.026]\n",
      "val: argmin=38 --> min=0.301\n",
      "\n",
      "10/10 - 165s - loss: 0.3300 - val_loss: 0.3007 - lr: 5.2233e-05\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 5.541020330009492e-05.\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.30069\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:45:00.056]\n",
      "Saving backup of the training history epoch=39 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:45:00.164]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:45:00.198]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:45:00.202]\n",
      "val: argmin=38 --> min=0.301\n",
      "\n",
      "10/10 - 162s - loss: 0.2938 - val_loss: 0.3019 - lr: 5.5410e-05\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 5.878016072274912e-05.\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.30069 to 0.29231, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:47:45.062]\n",
      "Saving backup of the training history epoch=40 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:47:45.148]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:47:45.179]\n",
      "train: argmin=8 --> min=0.293\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:47:45.182]\n",
      "val: argmin=40 --> min=0.292\n",
      "\n",
      "10/10 - 164s - loss: 0.2984 - val_loss: 0.2923 - lr: 5.8780e-05\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 6.235507341273913e-05.\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.29231 to 0.15957, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:50:30.348]\n",
      "Saving backup of the training history epoch=41 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:50:30.434]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:50:30.463]\n",
      "train: argmin=41 --> min=0.222\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:50:30.466]\n",
      "val: argmin=41 --> min=0.16\n",
      "\n",
      "10/10 - 165s - loss: 0.2218 - val_loss: 0.1596 - lr: 6.2355e-05\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 6.614740641230145e-05.\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.15957 to 0.07322, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:53:15.431]\n",
      "Saving backup of the training history epoch=42 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:53:15.521]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:53:15.550]\n",
      "train: argmin=42 --> min=0.113\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:53:15.553]\n",
      "val: argmin=42 --> min=0.0732\n",
      "\n",
      "10/10 - 164s - loss: 0.1132 - val_loss: 0.0732 - lr: 6.6147e-05\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 7.017038286703822e-05.\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.07322 to 0.05828, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:56:00.887]\n",
      "Saving backup of the training history epoch=43 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:56:01.003]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:56:01.030]\n",
      "train: argmin=43 --> min=0.0713\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:56:01.033]\n",
      "val: argmin=43 --> min=0.0583\n",
      "\n",
      "10/10 - 165s - loss: 0.0713 - val_loss: 0.0583 - lr: 7.0170e-05\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 7.443803013251696e-05.\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.05828 to 0.05305, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::14:58:46.079]\n",
      "Saving backup of the training history epoch=44 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::14:58:46.183]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:58:46.216]\n",
      "train: argmin=44 --> min=0.0557\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::14:58:46.220]\n",
      "val: argmin=44 --> min=0.0531\n",
      "\n",
      "10/10 - 165s - loss: 0.0557 - val_loss: 0.0531 - lr: 7.4438e-05\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 7.896522868499733e-05.\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.05305 to 0.04637, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:01:31.701]\n",
      "Saving backup of the training history epoch=45 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:01:31.813]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:01:31.847]\n",
      "train: argmin=45 --> min=0.0476\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:01:31.851]\n",
      "val: argmin=45 --> min=0.0464\n",
      "\n",
      "10/10 - 165s - loss: 0.0476 - val_loss: 0.0464 - lr: 7.8965e-05\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 8.376776400682924e-05.\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.04637 to 0.03997, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:04:17.517]\n",
      "Saving backup of the training history epoch=46 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:04:17.610]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:04:17.641]\n",
      "train: argmin=46 --> min=0.0413\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:04:17.644]\n",
      "val: argmin=46 --> min=0.04\n",
      "\n",
      "10/10 - 165s - loss: 0.0413 - val_loss: 0.0400 - lr: 8.3768e-05\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 8.886238162743407e-05.\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.03997 to 0.03742, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:07:02.727]\n",
      "Saving backup of the training history epoch=47 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:07:02.823]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:07:02.853]\n",
      "train: argmin=47 --> min=0.0379\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:07:02.856]\n",
      "val: argmin=47 --> min=0.0374\n",
      "\n",
      "10/10 - 165s - loss: 0.0379 - val_loss: 0.0374 - lr: 8.8862e-05\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 9.426684551178853e-05.\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.03742 to 0.03412, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:09:48.122]\n",
      "Saving backup of the training history epoch=48 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:09:48.729]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:09:48.749]\n",
      "train: argmin=48 --> min=0.0349\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:09:48.751]\n",
      "val: argmin=48 --> min=0.0341\n",
      "\n",
      "10/10 - 165s - loss: 0.0349 - val_loss: 0.0341 - lr: 9.4267e-05\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.03412\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:12:31.216]\n",
      "Saving backup of the training history epoch=49 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:12:31.289]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:12:31.315]\n",
      "train: argmin=49 --> min=0.0323\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:12:31.317]\n",
      "val: argmin=48 --> min=0.0341\n",
      "\n",
      "10/10 - 162s - loss: 0.0323 - val_loss: 0.0381 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.03412\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:15:14.108]\n",
      "Saving backup of the training history epoch=50 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:15:14.201]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:15:14.229]\n",
      "train: argmin=49 --> min=0.0323\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:15:14.232]\n",
      "val: argmin=48 --> min=0.0341\n",
      "\n",
      "10/10 - 162s - loss: 0.0340 - val_loss: 0.0353 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.03412 to 0.03171, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:17:59.387]\n",
      "Saving backup of the training history epoch=51 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:17:59.492]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:17:59.527]\n",
      "train: argmin=51 --> min=0.0308\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:17:59.530]\n",
      "val: argmin=51 --> min=0.0317\n",
      "\n",
      "10/10 - 165s - loss: 0.0308 - val_loss: 0.0317 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.03171 to 0.03050, saving model to /home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447.autosaved.hdf5\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:20:45.194]\n",
      "Saving backup of the training history epoch=52 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:20:45.293]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:20:45.326]\n",
      "train: argmin=52 --> min=0.0291\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:20:45.329]\n",
      "val: argmin=52 --> min=0.0305\n",
      "\n",
      "10/10 - 165s - loss: 0.0291 - val_loss: 0.0305 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.03050\n",
      "INFO::tomo2seg::{callbacks.py:on_epoch_end:085}::[2020-11-26::15:23:27.860]\n",
      "Saving backup of the training history epoch=53 self.csv_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/models/unet2d.vanilla01-f16.fold000.1606-391-447/history.csv')\n",
      "\n",
      "DEBUG::tomo2seg::{viz.py:plot:294}::[2020-11-26::15:23:27.964]\n",
      "TrainingHistoryDisplay.plot plotting loss\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:23:27.995]\n",
      "train: argmin=53 --> min=0.029\n",
      "\n",
      "INFO::tomo2seg::{viz.py:mark_min_values:389}::[2020-11-26::15:23:27.999]\n",
      "val: argmin=52 --> min=0.0305\n",
      "\n",
      "10/10 - 162s - loss: 0.0290 - val_loss: 0.0308 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 55/200\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "try:\n",
    "    model.fit(\n",
    "        # data sequences\n",
    "        x=crop_seq_train,\n",
    "        validation_data=crop_seq_val,\n",
    "\n",
    "        # epochs\n",
    "        initial_epoch=0,\n",
    "        epochs=n_epochs,\n",
    "#         initial_epoch=history_cb.last_epoch + 1,  # for some reason it is 0-starting and others 1-starting...\n",
    "#         epochs=history_cb.last_epoch + 1 + n_epochs,  \n",
    "    #     initial_epoch=113,\n",
    "    #     epochs=126,\n",
    "\n",
    "        # others\n",
    "        callbacks=callbacks,  \n",
    "        verbose=2,\n",
    "        use_multiprocessing=False,   \n",
    "    );\n",
    "\n",
    "except Exception as ex:\n",
    "    slack.notify_error()\n",
    "    raise ex\n",
    "    \n",
    "else:\n",
    "    slack.notify_finished()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows := 2, ncols := 1, figsize=(2.5 * (sz := 5), nrows * sz), dpi=100)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "hist_display = viz.TrainingHistoryDisplay(\n",
    "    history_cb.history, \n",
    "    model_name=tomo2seg_model.name,\n",
    "    loss_name=model.loss.__name__,\n",
    "    x_axis_mode=(\n",
    "        \"epoch\", \n",
    "        \"batch\",\n",
    "        \"crop\", \n",
    "        \"voxel\",\n",
    "        \"time\",\n",
    "    ),\n",
    ").plot(\n",
    "    axs, \n",
    "    with_lr=True,\n",
    "    metrics=(\n",
    "        \"loss\", \n",
    "#         \"jaccard2.class_idx=0\",\n",
    "#         \"jaccard2.class_idx=1\",\n",
    "#         \"jaccard2.class_idx=2\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[-1].set_yscale(\"log\")\n",
    "\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"loss\"][0])\n",
    "viz.mark_min_values(hist_display.axs_metrics_[0], hist_display.plots_[\"val_loss\"][0], txt_kwargs=dict(rotation=0))\n",
    "\n",
    "hist_display.fig_.savefig(\n",
    "    tomo2seg_model.model_path / (hist_display.title + \".png\"),\n",
    "    format='png',\n",
    ")\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8793,
     "status": "aborted",
     "timestamp": 1602255923919,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "d-EnhRhrrEGQ"
   },
   "outputs": [],
   "source": [
    "history_cb.dataframe.to_csv(history_cb.csv_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8791,
     "status": "aborted",
     "timestamp": 1602255923920,
     "user": {
      "displayName": "João Paulo Casagrande Bertoldo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioL6iQ5PNE9hm2XaOtZd36rClxQBdpy33-9-QsTUs=s64",
      "userId": "13620585220725745309"
     },
     "user_tz": -120
    },
    "id": "LQz6HBJss1o4"
   },
   "outputs": [],
   "source": [
    "model.save(tomo2seg_model.model_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this_nb_name = \"train-03.ipynb\"\n",
    "import os\n",
    "this_dir = os.getcwd()\n",
    "logger.warning(f\"{this_nb_name=} {this_dir=}\")\n",
    "\n",
    "os.system(f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {str(tomo2seg_model.model_path)} --to html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP2FW3h3DkQ4XcY6OgH7u/r",
   "collapsed_sections": [
    "EnVqPFS9BNCg",
    "j8e5FhmUaKND",
    "nJtppItnKn5G"
   ],
   "mount_file_id": "1LuEITv9j0lLf8Z418J3a94SjEZ8GvKvI",
   "name": "dryrun-02.ipynb",
   "provenance": [
    {
     "file_id": "1NiX28EcC_FVOYCJL4usp7n5iQ2x3aXIm",
     "timestamp": 1602152789440
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
